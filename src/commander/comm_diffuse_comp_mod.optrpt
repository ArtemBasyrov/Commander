Intel(R) Advisor can now assist with vectorization and show optimization
  report messages with your source code.
See "https://software.intel.com/en-us/intel-advisor-xe" for details.

Intel(R) Fortran Intel(R) 64 Compiler for applications running on Intel(R) 64, Version 18.0.3.222 Build 20180410

Compiler options: -I/mn/stornext/u3/hke/owl/local/src/dagsshealpix/include -I/mn/stornext/u3/hke/owl/local/include -I/astro/local/opt/Intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/include -I/astro/local/opt/Intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/include -O3 -traceback -qopenmp -qopt-report=5 -parallel -c

    Report from: Interprocedural optimizations [ipo]

  WHOLE PROGRAM (SAFE) [EITHER METHOD]: false
  WHOLE PROGRAM (SEEN) [TABLE METHOD]: false
  WHOLE PROGRAM (READ) [OBJECT READER METHOD]: false

INLINING OPTION VALUES:
  -inline-factor: 100
  -inline-min-size: 30
  -inline-max-size: 230
  -inline-max-total-size: 2000
  -inline-max-per-routine: 10000
  -inline-max-per-compile: 500000

In the inlining report below:
   "sz" refers to the "size" of the routine. The smaller a routine's size,
      the more likely it is to be inlined.
   "isz" refers to the "inlined size" of the routine. This is the amount
      the calling routine will grow if the called routine is inlined into it.
      The compiler generally limits the amount a routine can grow by having
      routines inlined into it.

Begin optimization report for: comm_diffuse_comp_mod._

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (comm_diffuse_comp_mod._) [1/20=5.0%] comm_diffuse_comp_mod.f90(1,8)


    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(1,8):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod._] comm_diffuse_comp_mod.f90:1

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :    0[ reg_null]
        
    Routine temporaries
        Total         :       6
            Global    :       0
            Local     :       6
        Regenerable   :       0
        Spilled       :       0
        
    Routine stack
        Variables     :       0 bytes*
            Reads     :       0 [0.00e+00 ~ 0.0%]
            Writes    :       0 [0.00e+00 ~ 0.0%]
        Spills        :       0 bytes*
            Reads     :       0 [0.00e+00 ~ 0.0%]
            Writes    :       0 [0.00e+00 ~ 0.0%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::UPDATEDIFFPRECOND

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::UPDATEDIFFPRECOND) [2/20=10.0%] comm_diffuse_comp_mod.f90(324,14)
  -> EXTERN: (329,18) memmove
  -> EXTERN: (329,18) _alloca
  -> EXTERN: (329,18) for_trim
  -> EXTERN: (330,11) for_cpstr
  -> (331,13) COMM_DIFFUSE_COMP_MOD::UPDATEDIFFPRECOND_DIAGONAL (isz = 3541) (sz = 3548)
     [[ Inlining would exceed -inline-max-size value (3548>230) <1>]]
  -> EXTERN: (332,11) for_cpstr
  -> (333,13) COMM_DIFFUSE_COMP_MOD::UPDATEDIFFPRECOND_PSEUDOINV (isz = 816) (sz = 823)
     [[ Inlining would exceed -inline-max-size value (823>253) <1>]]
  -> EXTERN: (335,13) __resetsp_inlined
  -> EXTERN: (335,13) REPORT_ERROR
  -> EXTERN: (335,13) __getsp_inlined


    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(329,18):remark #34014: optimization advice for memmove: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(329,18):remark #34026: call to memmove implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(324,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_updatediffprecond_] comm_diffuse_comp_mod.f90:324

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   11[ rax rdx rcx rbx rsi rdi r8 r12-r15]
        
    Routine temporaries
        Total         :      52
            Global    :      17
            Local     :      35
        Regenerable   :      18
        Spilled       :       5
        
    Routine stack
        Variables     :     512 bytes*
            Reads     :       0 [0.00e+00 ~ 0.0%]
            Writes    :       0 [0.00e+00 ~ 0.0%]
        Spills        :       0 bytes*
            Reads     :       0 [0.00e+00 ~ 0.0%]
            Writes    :       0 [0.00e+00 ~ 0.0%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::UPDATEDIFFPRECOND_DIAGONAL

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::UPDATEDIFFPRECOND_DIAGONAL) [3/20=15.0%] comm_diffuse_comp_mod.f90(340,14)
  -> EXTERN: (360,42) for_realloc_lhs
  -> EXTERN: (360,42) for_dealloc_allocatable
  -> EXTERN: (378,10) wall_time_
  -> EXTERN: (380,12) for_trim
  -> EXTERN: (380,41) for_cpstr
  -> EXTERN: (382,8) for_alloc_allocatable
  -> EXTERN: (382,8) for_check_mult_overflow64
  -> INDIRECT-: (398,16)  (*((P64*) *((P64*) (&((COMM_DIFFUSE_COMP_MOD$.btDIFF_PTR$P$50_V$10be *)comm_diffuse_comp_mod_mp_diffcomps_$56_V$39b4)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_updatediffprecond_diagonal_$K1_V$491d.0.7)), l:comm_diffuse_comp_mod_mp_diffcomps_$56_V$39bd[0(SI32), l:0(SI64)]]->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$CL$47_V$f84 + 56(SI64)))))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (412,8) for_dealloc_allocatable
  -> EXTERN: (418,12) for_trim
  -> EXTERN: (418,41) for_cpstr
  -> EXTERN: (420,8) for_alloc_allocatable
  -> EXTERN: (420,8) for_check_mult_overflow64
  -> INDIRECT-: (436,16)  (*((P64*) *((P64*) (&((COMM_DIFFUSE_COMP_MOD$.btDIFF_PTR$P$50_V$10be *)comm_diffuse_comp_mod_mp_diffcomps_$56_V$39b4)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_updatediffprecond_diagonal_$K1_V$491d.0.7)), l:comm_diffuse_comp_mod_mp_diffcomps_$56_V$39bd[0(SI32), l:0(SI64)]]->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$CL$47_V$f84 + 56(SI64)))))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (448,8) for_dealloc_allocatable
  -> EXTERN: (478,12) for_trim
  -> EXTERN: (478,41) for_cpstr
  -> INDIRECT-: (482,16)  (*((P64*) comm_diffuse_comp_mod_mp_info_pre_$55_V$398a))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (510,10) wall_time_
  -> EXTERN: (512,8) for_check_mult_overflow64
  -> EXTERN: (512,8) for_alloc_allocatable
  -> EXTERN: (512,8) for_check_mult_overflow64
  -> EXTERN: (512,8) for_alloc_allocatable
  -> INDIRECT-: (517,19)  (*((P64*) comm_diffuse_comp_mod_mp_info_pre_$55_V$398a))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (520,17) for_alloc_allocatable
  -> EXTERN: (520,17) for_check_mult_overflow64
  -> EXTERN: (520,17) for_alloc_allocatable
  -> EXTERN: (520,17) for_check_mult_overflow64
  -> EXTERN: (521,22) GET_EIGENVALUES
  -> EXTERN: (525,17) for_dealloc_allocatable
  -> EXTERN: (525,17) for_dealloc_allocatable
  -> EXTERN: (529,13) mpi_allreduce_
  -> EXTERN: (530,13) mpi_allreduce_
  -> EXTERN: (532,11) for_write_seq_lis_xmit
  -> EXTERN: (532,11) for_write_seq_lis
  -> EXTERN: (533,11) for_write_seq_lis_xmit
  -> EXTERN: (533,11) for_write_seq_lis
  -> EXTERN: (534,18) GETLUN
  -> EXTERN: (535,11) __resetsp_inlined
  -> EXTERN: (535,11) for_open
  -> EXTERN: (535,11) __getsp_inlined
  -> EXTERN: (535,27) for_trim
  -> EXTERN: (535,39) for_concat
  -> EXTERN: (535,39) _alloca
  -> EXTERN: (537,14) for_write_seq_fmt_xmit
  -> EXTERN: (537,14) for_write_seq_fmt_xmit
  -> EXTERN: (537,14) for_write_seq_fmt
  -> EXTERN: (539,17) for_close
  -> EXTERN: (543,8) for_dealloc_allocatable
  -> EXTERN: (543,8) for_dealloc_allocatable
  -> EXTERN: (545,10) wall_time_
  -> EXTERN: (565,10) wall_time_
  -> EXTERN: (574,54) INVERT_MATRIX_WITH_MASK_DP
  -> EXTERN: (578,10) wall_time_
  -> EXTERN: (584,3) for_dealloc_allocatable
  -> EXTERN: (584,3) for_dealloc_allocatable
  -> EXTERN: (584,3) for_dealloc_allocatable
  -> EXTERN: (584,3) for_dealloc_allocatable
  -> EXTERN: (584,3) for_dealloc_allocatable


    Report from: OpenMP optimizations [openmp]

OpenMP Construct at comm_diffuse_comp_mod.f90(381,14)
remark #16201: OpenMP DEFINED REGION WAS PARALLELIZED
OpenMP Construct at comm_diffuse_comp_mod.f90(419,14)
remark #16201: OpenMP DEFINED REGION WAS PARALLELIZED

    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(358,5)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between i (359:8) and i (361:8)
   remark #17106: parallel dependence: assumed OUTPUT dependence between i (361:8) and i (359:8)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(360,15)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(360,42)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed ANTI dependence between at (360:42) and at (360:42)
         remark #17106: parallel dependence: assumed FLOW dependence between at (360:42) and at (360:42)
         remark #17106: parallel dependence: assumed FLOW dependence between at (360:42) and at (360:42)
         remark #17106: parallel dependence: assumed ANTI dependence between at (360:42) and at (360:42)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(360,42)
            remark #25399: memcopy generated
            remark #17104: loop was not parallelized: existence of parallel dependence
            remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,:,:) (360:42) and p_cr(i,j,:,:) (360:42)
            remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,:,:) (360:42) and p_cr(i,j,:,:) (360:42)
            remark #15542: loop was not vectorized: inner loop was already vectorized

            LOOP BEGIN at comm_diffuse_comp_mod.f90(360,42)
            <Peeled loop for vectorization>
               remark #25015: Estimate of max trip count of loop=1
            LOOP END

            LOOP BEGIN at comm_diffuse_comp_mod.f90(360,42)
               remark #17108: loop was not parallelized: insufficient computational work
               remark #15388: vectorization support: reference p_cr(i,j,:,:) has aligned access
               remark #15389: vectorization support: reference p_cr(i,j,:,:) has unaligned access
               remark #15381: vectorization support: unaligned access used inside loop body
               remark #15305: vectorization support: vector length 2
               remark #15309: vectorization support: normalized vectorization overhead 3.667
               remark #15300: LOOP WAS VECTORIZED
               remark #15442: entire loop may be executed in remainder
               remark #15449: unmasked aligned unit stride stores: 1 
               remark #15450: unmasked unaligned unit stride loads: 1 
               remark #15475: --- begin vector cost summary ---
               remark #15476: scalar cost: 4 
               remark #15477: vector cost: 1.500 
               remark #15478: estimated potential speedup: 1.230 
               remark #15488: --- end vector cost summary ---
               remark #25015: Estimate of max trip count of loop=6
            LOOP END

            LOOP BEGIN at comm_diffuse_comp_mod.f90(360,42)
            <Remainder loop for vectorization>
               remark #25015: Estimate of max trip count of loop=12
            LOOP END
         LOOP END
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(379,5)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(417,5)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(456,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,1,:,:) (458:11) and p_cr(i,1,:,:) (458:11)
   remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,1,:,:) (458:11) and p_cr(i,1,:,:) (458:11)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(458,11)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,1,:,:) (458:11) and p_cr(i,1,:,:) (458:11)
      remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,1,:,:) (458:11) and p_cr(i,1,:,:) (458:11)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(458,11)
         remark #25408: memset generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,1,:,:) (458:11) and p_cr(i,1,:,:) (458:11)
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,1,:,:) (458:11) and p_cr(i,1,:,:) (458:11)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(458,11)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15389: vectorization support: reference p_cr(i,1,:,:) has unaligned access
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 2
            remark #15309: vectorization support: normalized vectorization overhead 0.300
            remark #15300: LOOP WAS VECTORIZED
            remark #15451: unmasked unaligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 2.500 
            remark #15478: estimated potential speedup: 1.450 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=3
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(458,11)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=12
         LOOP END
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(477,5)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification

   LOOP BEGIN at comm_diffuse_comp_mod.f90(481,8)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification

      LOOP BEGIN at comm_diffuse_comp_mod.f90(487,14)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed FLOW dependence between p_cr(i,j,p,p) (490:28) and p_cr(i,j,p,p) (490:28)
         remark #17106: parallel dependence: assumed ANTI dependence between p_cr(i,j,p,p) (490:28) and p_cr(i,j,p,p) (490:28)
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,p,p) (490:28) and p_cr(i,j,p,p) (490:28)
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,p,p) (490:28) and p_cr(i,j,p,p) (490:28)
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed FLOW dependence between p_cr(i,j,p,p) (490:28) and p_cr(i,j,p,p) (490:28)
         remark #15346: vector dependence: assumed ANTI dependence between p_cr(i,j,p,p) (490:28) and p_cr(i,j,p,p) (490:28)
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(513,8)
   remark #25408: memset generated
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between COND(:) (513:8) and COND(:) (513:8)
   remark #17106: parallel dependence: assumed OUTPUT dependence between COND(:) (513:8) and COND(:) (513:8)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(513,8)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15388: vectorization support: reference COND(:) has aligned access
      remark #15305: vectorization support: vector length 2
      remark #15399: vectorization support: unroll factor set to 2
      remark #15300: LOOP WAS VECTORIZED
      remark #15449: unmasked aligned unit stride stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 4 
      remark #15477: vector cost: 1.500 
      remark #15478: estimated potential speedup: 2.660 
      remark #15488: --- end vector cost summary ---
      remark #25015: Estimate of max trip count of loop=3
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(513,8)
   <Remainder loop for vectorization>
      remark #25015: Estimate of max trip count of loop=12
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(514,8)
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15388: vectorization support: reference W_MIN(:) has aligned access
   remark #15305: vectorization support: vector length 2
   remark #15399: vectorization support: unroll factor set to 4
   remark #15300: LOOP WAS VECTORIZED
   remark #15449: unmasked aligned unit stride stores: 1 
   remark #15475: --- begin vector cost summary ---
   remark #15476: scalar cost: 4 
   remark #15477: vector cost: 1.500 
   remark #15478: estimated potential speedup: 2.600 
   remark #15488: --- end vector cost summary ---
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(514,8)
<Remainder loop for vectorization>
   remark #15388: vectorization support: reference W_MIN(:) has aligned access
   remark #15335: remainder loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
   remark #15305: vectorization support: vector length 2
   remark #15309: vectorization support: normalized vectorization overhead 2.000
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(515,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between info_pre (516:11) and info_pre_ (517:19)
   remark #17106: parallel dependence: assumed FLOW dependence between info_pre_ (517:19) and info_pre (516:11)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(516,11)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(522,28)
         remark #25084: Preprocess Loopnests: Moving Out Store    [ comm_diffuse_comp_mod.f90(522,28) ]
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15388: vectorization support: reference W(:) has aligned access
         remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 4
         remark #15309: vectorization support: normalized vectorization overhead 0.158
         remark #15448: unmasked aligned unit stride loads: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 13 
         remark #15477: vector cost: 15.000 
         remark #15478: estimated potential speedup: 0.860 
         remark #15488: --- end vector cost summary ---
         remark #25439: unrolled with remainder by 2  
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(522,28)
      <Remainder>
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(523,41)
         remark #25084: Preprocess Loopnests: Moving Out Store    [ comm_diffuse_comp_mod.f90(523,41) ]
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15388: vectorization support: reference W(:) has aligned access
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 4
         remark #15309: vectorization support: normalized vectorization overhead 0.085
         remark #15300: LOOP WAS VECTORIZED
         remark #15448: unmasked aligned unit stride loads: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 39 
         remark #15477: vector cost: 31.000 
         remark #15478: estimated potential speedup: 1.250 
         remark #15486: divides: 1 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(523,41)
      <Remainder loop for vectorization>
         remark #15388: vectorization support: reference W(:) has aligned access
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 0.484
         remark #15301: REMAINDER LOOP WAS VECTORIZED
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(523,41)
      <Remainder loop for vectorization>
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(524,42)
         remark #25084: Preprocess Loopnests: Moving Out Store    [ comm_diffuse_comp_mod.f90(524,42) ]
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15388: vectorization support: reference W(:) has aligned access
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 4
         remark #15309: vectorization support: normalized vectorization overhead 0.279
         remark #15300: LOOP WAS VECTORIZED
         remark #15448: unmasked aligned unit stride loads: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 9 
         remark #15477: vector cost: 8.500 
         remark #15478: estimated potential speedup: 1.050 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(524,42)
      <Remainder loop for vectorization>
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(532,69)
   remark #25084: Preprocess Loopnests: Moving Out Store    [ comm_diffuse_comp_mod.f90(532,69) ]
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15388: vectorization support: reference COND(:) has aligned access
   remark #15305: vectorization support: vector length 2
   remark #15399: vectorization support: unroll factor set to 4
   remark #15309: vectorization support: normalized vectorization overhead 0.279
   remark #15300: LOOP WAS VECTORIZED
   remark #15448: unmasked aligned unit stride loads: 1 
   remark #15475: --- begin vector cost summary ---
   remark #15476: scalar cost: 9 
   remark #15477: vector cost: 8.500 
   remark #15478: estimated potential speedup: 1.050 
   remark #15488: --- end vector cost summary ---
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(532,69)
<Remainder loop for vectorization>
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(533,69)
   remark #25084: Preprocess Loopnests: Moving Out Store    [ comm_diffuse_comp_mod.f90(533,69) ]
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15388: vectorization support: reference W_MIN(:) has aligned access
   remark #15305: vectorization support: vector length 2
   remark #15399: vectorization support: unroll factor set to 4
   remark #15309: vectorization support: normalized vectorization overhead 0.279
   remark #15300: LOOP WAS VECTORIZED
   remark #15448: unmasked aligned unit stride loads: 1 
   remark #15475: --- begin vector cost summary ---
   remark #15476: scalar cost: 9 
   remark #15477: vector cost: 8.500 
   remark #15478: estimated potential speedup: 1.050 
   remark #15488: --- end vector cost summary ---
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(533,69)
<Remainder loop for vectorization>
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(536,11)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between at (537:14) and at (537:14)
   remark #17106: parallel dependence: assumed OUTPUT dependence between at (537:14) and at (537:14)
   remark #15382: vectorization support: call to function for_write_seq_fmt cannot be vectorized   [ comm_diffuse_comp_mod.f90(537,14) ]
   remark #15382: vectorization support: call to function for_write_seq_fmt_xmit cannot be vectorized   [ comm_diffuse_comp_mod.f90(537,14) ]
   remark #15382: vectorization support: call to function for_write_seq_fmt_xmit cannot be vectorized   [ comm_diffuse_comp_mod.f90(537,14) ]
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed OUTPUT dependence between at (537:14) and at (537:14)
   remark #15346: vector dependence: assumed OUTPUT dependence between at (537:14) and at (537:14)
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(549,5)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between i (550:8) and i (550:8)
   remark #17106: parallel dependence: assumed OUTPUT dependence between i (550:8) and i (550:8)
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed OUTPUT dependence between i (550:8) and i (550:8)
   remark #15346: vector dependence: assumed OUTPUT dependence between i (550:8) and i (550:8)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(550,8)
   <Distributed chunk1>
      remark #25426: Loop Distributed (2 way) 
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between k1 (552:11) and k1 (552:11)
      remark #17106: parallel dependence: assumed OUTPUT dependence between k1 (552:11) and k1 (552:11)
      remark #15344: loop was not vectorized: vector dependence prevents vectorization
      remark #15346: vector dependence: assumed OUTPUT dependence between k1 (552:11) and k1 (552:11)
      remark #15346: vector dependence: assumed OUTPUT dependence between k1 (552:11) and k1 (552:11)

      LOOP BEGIN at comm_diffuse_comp_mod.f90(552,11)
         remark #25084: Preprocess Loopnests: Moving Out Store    [ comm_diffuse_comp_mod.f90(552,11) ]
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17) and p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17)
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17) and p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17)
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed OUTPUT dependence between p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17) and p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17)
         remark #15346: vector dependence: assumed OUTPUT dependence between p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17) and p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17)

         LOOP BEGIN at comm_diffuse_comp_mod.f90(554,14)
            remark #17104: loop was not parallelized: existence of parallel dependence
            remark #17106: parallel dependence: assumed FLOW dependence between p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17) and diffcomps(k2) (555:51)
            remark #17106: parallel dependence: assumed ANTI dependence between diffcomps(k2) (555:51) and p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17)
            remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17) and p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17)
            remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17) and p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17)
            remark #15344: loop was not vectorized: vector dependence prevents vectorization
            remark #15346: vector dependence: assumed FLOW dependence between p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17) and diffcomps(k2) (555:51)
            remark #15346: vector dependence: assumed ANTI dependence between diffcomps(k2) (555:51) and p_cr(i,j,p_cr(i,j,k1),p_cr(i,j,k2)) (558:17)
         LOOP END
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(566,5)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between i (567:8) and i (567:8)
   remark #17106: parallel dependence: assumed OUTPUT dependence between call:INVERT_MATRIX_WITH_MASK_DP (574:54) and call:INVERT_MATRIX_WITH_MASK_DP (574:54)
   remark #17106: parallel dependence: assumed OUTPUT dependence between call:INVERT_MATRIX_WITH_MASK_DP (574:54) and call:INVERT_MATRIX_WITH_MASK_DP (574:54)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(567,8)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(574,18)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed ANTI dependence between at (574:44) and at (574:18)
         remark #17106: parallel dependence: assumed FLOW dependence between at (574:18) and at (574:44)
         remark #17106: parallel dependence: assumed ANTI dependence between at (574:44) and at (574:18)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(574,18)
         <Peeled loop for vectorization>
            remark #25015: Estimate of max trip count of loop=3
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(574,18)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15305: vectorization support: vector length 4
            remark #15399: vectorization support: unroll factor set to 2
            remark #15309: vectorization support: normalized vectorization overhead 0.808
            remark #15300: LOOP WAS VECTORIZED
            remark #15442: entire loop may be executed in remainder
            remark #15448: unmasked aligned unit stride loads: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 20 
            remark #15477: vector cost: 6.500 
            remark #15478: estimated potential speedup: 2.930 
            remark #15488: --- end vector cost summary ---
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(574,18)
         <Remainder loop for vectorization>
            remark #15305: vectorization support: vector length 2
            remark #15309: vectorization support: normalized vectorization overhead 1.261
            remark #15301: REMAINDER LOOP WAS VECTORIZED
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(574,18)
         <Remainder loop for vectorization>
         LOOP END
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(383,14)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15541: outer loop was not auto-vectorized: consider using SIMD directive

   LOOP BEGIN at comm_diffuse_comp_mod.f90(384,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between p_cr(i,j) (388:17) and info_pre_ (398:16)
      remark #17106: parallel dependence: assumed FLOW dependence between info_pre_ (398:16) and p_cr(i,j) (388:17)
      remark #15541: outer loop was not auto-vectorized: consider using SIMD directive

      LOOP BEGIN at comm_diffuse_comp_mod.f90(386,14)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(i,j)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(387,21) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(388,17) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(388,21) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,k1)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(388,17) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(389,17) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(389,21) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,k2)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(389,17) ]
         remark #15329: vectorization support: non-unit strided store was emulated for the variable <ALM(i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(391,20) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(391,20) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(1,i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(391,31) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(1,i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(391,31) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(391,31) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,q,p)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(391,20) ]
         remark #15329: vectorization support: non-unit strided store was emulated for the variable <ALM(i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(393,20) ]
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 0.014
         remark #15452: unmasked strided loads: 1 
         remark #15458: masked indexed (or gather) loads: 3 
         remark #15460: masked strided loads: 8 
         remark #15462: unmasked indexed (or gather) loads: 2 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 36 
         remark #15477: vector cost: 292.000 
         remark #15478: estimated potential speedup: 0.120 
         remark #15487: type converts: 2 
         remark #15488: --- end vector cost summary ---

         LOOP BEGIN at comm_diffuse_comp_mod.f90(386,14)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15389: vectorization support: reference ALM(i,j) has unaligned access   [ comm_diffuse_comp_mod.f90(391,20) ]
            remark #15389: vectorization support: reference ALM(i,j) has unaligned access   [ comm_diffuse_comp_mod.f90(393,20) ]
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(i,j)>, stride is 86   [ comm_diffuse_comp_mod.f90(387,21) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(388,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(388,21) ]
            remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,k1)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(388,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(389,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(389,21) ]
            remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,k2)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(389,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(391,20) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(1,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(391,31) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(1,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(391,31) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(391,31) ]
            remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,q,p)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(391,20) ]
            remark #15305: vectorization support: vector length 2
            remark #15309: vectorization support: normalized vectorization overhead 0.007
            remark #15452: unmasked strided loads: 1 
            remark #15457: masked unaligned unit stride stores: 2 
            remark #15458: masked indexed (or gather) loads: 3 
            remark #15460: masked strided loads: 8 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 36 
            remark #15477: vector cost: 288.000 
            remark #15478: estimated potential speedup: 0.120 
            remark #15487: type converts: 2 
            remark #15488: --- end vector cost summary ---
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(402,11)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,q,p) (407:44) and p_cr(i,j,q,p) (407:44)
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,q,p) (407:44) and p_cr(i,j,q,p) (407:44)
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed OUTPUT dependence between p_cr(i,j,q,p) (407:44) and p_cr(i,j,q,p) (407:44)
         remark #15346: vector dependence: assumed OUTPUT dependence between p_cr(i,j,q,p) (407:44) and p_cr(i,j,q,p) (407:44)

         LOOP BEGIN at comm_diffuse_comp_mod.f90(403,14)
            remark #17104: loop was not parallelized: existence of parallel dependence
            remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,q,p) (407:44) and p_cr(i,j,q,p) (407:44)
            remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,q,p) (407:44) and p_cr(i,j,q,p) (407:44)
            remark #15389: vectorization support: reference ALM(i,j) has unaligned access   [ comm_diffuse_comp_mod.f90(407,44) ]
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(i,j)>, stride is 86   [ comm_diffuse_comp_mod.f90(404,21) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(405,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(405,21) ]
            remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,k1)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(405,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(406,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(406,21) ]
            remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,k2)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(406,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(407,44) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(1,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(407,44) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(1,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(407,44) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(407,44) ]
            remark #15329: vectorization support: indirect store was emulated for the variable <p_cr(i,j,q,p)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(407,44) ]
            remark #15305: vectorization support: vector length 2
            remark #15309: vectorization support: normalized vectorization overhead 0.007
            remark #15452: unmasked strided loads: 1 
            remark #15456: masked unaligned unit stride loads: 1 
            remark #15458: masked indexed (or gather) loads: 2 
            remark #15459: masked indexed (or scatter) stores: 1 
            remark #15460: masked strided loads: 8 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 36 
            remark #15477: vector cost: 269.000 
            remark #15478: estimated potential speedup: 0.130 
            remark #15487: type converts: 2 
            remark #15488: --- end vector cost summary ---
         LOOP END
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(421,14)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15541: outer loop was not auto-vectorized: consider using SIMD directive

   LOOP BEGIN at comm_diffuse_comp_mod.f90(422,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between p_cr(i,j) (426:17) and info_pre_ (436:16)
      remark #17106: parallel dependence: assumed FLOW dependence between info_pre_ (436:16) and p_cr(i,j) (426:17)
      remark #15541: outer loop was not auto-vectorized: consider using SIMD directive

      LOOP BEGIN at comm_diffuse_comp_mod.f90(424,14)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(i,j)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(425,21) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(426,17) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(426,21) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,k1)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(426,17) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(427,17) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(427,21) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,k2)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(427,17) ]
         remark #15329: vectorization support: non-unit strided store was emulated for the variable <ALM(i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(429,20) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(429,20) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(1,i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(429,31) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(1,i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(429,31) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(429,31) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,p,q)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(429,20) ]
         remark #15329: vectorization support: non-unit strided store was emulated for the variable <ALM(i,j)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(431,20) ]
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 0.014
         remark #15452: unmasked strided loads: 1 
         remark #15458: masked indexed (or gather) loads: 3 
         remark #15460: masked strided loads: 8 
         remark #15462: unmasked indexed (or gather) loads: 2 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 36 
         remark #15477: vector cost: 292.000 
         remark #15478: estimated potential speedup: 0.120 
         remark #15487: type converts: 2 
         remark #15488: --- end vector cost summary ---

         LOOP BEGIN at comm_diffuse_comp_mod.f90(424,14)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15389: vectorization support: reference ALM(i,j) has unaligned access   [ comm_diffuse_comp_mod.f90(429,20) ]
            remark #15389: vectorization support: reference ALM(i,j) has unaligned access   [ comm_diffuse_comp_mod.f90(431,20) ]
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(i,j)>, stride is 86   [ comm_diffuse_comp_mod.f90(425,21) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(426,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(426,21) ]
            remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,k1)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(426,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(427,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(427,21) ]
            remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,k2)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(427,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(429,20) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(1,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(429,31) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(1,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(429,31) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(429,31) ]
            remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,p,q)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(429,20) ]
            remark #15305: vectorization support: vector length 2
            remark #15309: vectorization support: normalized vectorization overhead 0.007
            remark #15452: unmasked strided loads: 1 
            remark #15457: masked unaligned unit stride stores: 2 
            remark #15458: masked indexed (or gather) loads: 3 
            remark #15460: masked strided loads: 8 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 36 
            remark #15477: vector cost: 288.000 
            remark #15478: estimated potential speedup: 0.120 
            remark #15487: type converts: 2 
            remark #15488: --- end vector cost summary ---
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(438,11)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,p,q) (443:44) and p_cr(i,j,p,q) (443:44)
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,p,q) (443:44) and p_cr(i,j,p,q) (443:44)
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed OUTPUT dependence between p_cr(i,j,p,q) (443:44) and p_cr(i,j,p,q) (443:44)
         remark #15346: vector dependence: assumed OUTPUT dependence between p_cr(i,j,p,q) (443:44) and p_cr(i,j,p,q) (443:44)

         LOOP BEGIN at comm_diffuse_comp_mod.f90(439,14)
            remark #17104: loop was not parallelized: existence of parallel dependence
            remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,p,q) (443:44) and p_cr(i,j,p,q) (443:44)
            remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i,j,p,q) (443:44) and p_cr(i,j,p,q) (443:44)
            remark #15389: vectorization support: reference ALM(i,j) has unaligned access   [ comm_diffuse_comp_mod.f90(443,44) ]
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(i,j)>, stride is 86   [ comm_diffuse_comp_mod.f90(440,21) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(441,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(441,21) ]
            remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,k1)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(441,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(442,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(442,21) ]
            remark #15328: vectorization support: indirect load was emulated for the variable <p_cr(i,j,k2)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(442,17) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(443,44) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(1,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(443,44) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(1,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(443,44) ]
            remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(0,i,j)>, masked, stride is 43   [ comm_diffuse_comp_mod.f90(443,44) ]
            remark #15329: vectorization support: indirect store was emulated for the variable <p_cr(i,j,p,q)>, masked, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(443,44) ]
            remark #15305: vectorization support: vector length 2
            remark #15309: vectorization support: normalized vectorization overhead 0.007
            remark #15452: unmasked strided loads: 1 
            remark #15456: masked unaligned unit stride loads: 1 
            remark #15458: masked indexed (or gather) loads: 2 
            remark #15459: masked indexed (or scatter) stores: 1 
            remark #15460: masked strided loads: 8 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 36 
            remark #15477: vector cost: 269.000 
            remark #15478: estimated potential speedup: 0.130 
            remark #15487: type converts: 2 
            remark #15488: --- end vector cost summary ---
         LOOP END
      LOOP END
   LOOP END
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(350,54):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(350,48):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(349,48):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(348,48):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(351,50):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(360,42):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(360,42):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(360,42):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(458,11):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(458,11):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(513,8):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(513,8):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(340,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_updatediffprecond_diagonal_] comm_diffuse_comp_mod.f90:340

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   27[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm12]
        
    Routine temporaries
        Total         :    1888
            Global    :     425
            Local     :    1463
        Regenerable   :     350
        Spilled       :      94
        
    Routine stack
        Variables     :    3144 bytes*
            Reads     :      95 [8.99e+01 ~ 0.3%]
            Writes    :     186 [2.14e+02 ~ 0.7%]
        Spills        :     800 bytes*
            Reads     :     164 [1.41e+03 ~ 4.5%]
            Writes    :     141 [9.45e+02 ~ 3.1%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::UPDATEDIFFPRECOND_PSEUDOINV

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::UPDATEDIFFPRECOND_PSEUDOINV) [4/20=20.0%] comm_diffuse_comp_mod.f90(587,14)
  -> EXTERN: (600,10) wall_time_
  -> EXTERN: (601,5) for_alloc_allocatable
  -> EXTERN: (601,5) for_check_mult_overflow64
  -> EXTERN: (616,21) for_trim
  -> EXTERN: (616,50) for_cpstr
  -> INDIRECT-: (617,45)  (*((P64*) *((P64*) (&((COMM_DIFFUSE_COMP_MOD$.btDIFF_PTR$P$50_V$10be *)comm_diffuse_comp_mod_mp_diffcomps_$56_V$39b4)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_updatediffprecond_pseudoinv_$K_V$4a74.0.8)), l:comm_diffuse_comp_mod_mp_diffcomps_$56_V$39bd[0(SI32), l:0(SI64)]]->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$CL$47_V$f84 + 56(SI64)))))[13(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (624,18) for_trim
  -> EXTERN: (624,47) for_cpstr
  -> EXTERN: (630,16) COMPUTE_PSEUDO_INVERSE
  -> EXTERN: (647,5) for_dealloc_allocatable
  -> EXTERN: (648,10) wall_time_
  -> EXTERN: (651,29) for_dealloc_allocatable
  -> EXTERN: (657,8) for_check_mult_overflow64
  -> EXTERN: (657,8) for_alloc_allocatable
  -> EXTERN: (673,3) for_dealloc_allocatable


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(660,15)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(603,8)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(606,11)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between MAT(:,:) (606:11) and MAT(:,:) (606:11)
         remark #17106: parallel dependence: assumed OUTPUT dependence between MAT(:,:) (606:11) and MAT(:,:) (606:11)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(606,11)
            remark #25408: memset generated
            remark #17104: loop was not parallelized: existence of parallel dependence
            remark #17106: parallel dependence: assumed OUTPUT dependence between MAT(:,:) (606:11) and MAT(:,:) (606:11)
            remark #17106: parallel dependence: assumed OUTPUT dependence between MAT(:,:) (606:11) and MAT(:,:) (606:11)
            remark #15542: loop was not vectorized: inner loop was already vectorized

            LOOP BEGIN at comm_diffuse_comp_mod.f90(606,11)
               remark #17108: loop was not parallelized: insufficient computational work
               remark #15389: vectorization support: reference MAT(:,:) has unaligned access
               remark #15381: vectorization support: unaligned access used inside loop body
               remark #15305: vectorization support: vector length 2
               remark #15399: vectorization support: unroll factor set to 2
               remark #15309: vectorization support: normalized vectorization overhead 0.300
               remark #15300: LOOP WAS VECTORIZED
               remark #15451: unmasked unaligned unit stride stores: 1 
               remark #15475: --- begin vector cost summary ---
               remark #15476: scalar cost: 4 
               remark #15477: vector cost: 2.500 
               remark #15478: estimated potential speedup: 1.450 
               remark #15488: --- end vector cost summary ---
               remark #25015: Estimate of max trip count of loop=3
            LOOP END

            LOOP BEGIN at comm_diffuse_comp_mod.f90(606,11)
            <Remainder loop for vectorization>
               remark #25015: Estimate of max trip count of loop=12
            LOOP END
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(607,11)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed ANTI dependence between l (610:23) and l (617:45)
         remark #17106: parallel dependence: assumed FLOW dependence between l (617:45) and l (610:23)
         remark #15382: vectorization support: call to function for_trim cannot be vectorized   [ comm_diffuse_comp_mod.f90(616,21) ]
         remark #15382: vectorization support: call to function (Indirect call) cannot be vectorized   [ comm_diffuse_comp_mod.f90(617,45) ]
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed ANTI dependence between l (610:23) and l (617:45)
         remark #15346: vector dependence: assumed FLOW dependence between l (617:45) and l (610:23)

         LOOP BEGIN at comm_diffuse_comp_mod.f90(609,14)
            remark #17104: loop was not parallelized: existence of parallel dependence
            remark #17106: parallel dependence: assumed ANTI dependence between l (610:23) and l (617:45)
            remark #17106: parallel dependence: assumed FLOW dependence between l (617:45) and l (610:23)
            remark #15382: vectorization support: call to function for_trim cannot be vectorized   [ comm_diffuse_comp_mod.f90(616,21) ]
            remark #15382: vectorization support: call to function (Indirect call) cannot be vectorized   [ comm_diffuse_comp_mod.f90(617,45) ]
            remark #15344: loop was not vectorized: vector dependence prevents vectorization
            remark #15346: vector dependence: assumed ANTI dependence between l (610:23) and l (617:45)
            remark #15346: vector dependence: assumed FLOW dependence between l (617:45) and l (610:23)
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(623,11)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed FLOW dependence between at (624:18) and at (624:47)
         remark #17106: parallel dependence: assumed ANTI dependence between at (624:47) and at (624:18)
         remark #15382: vectorization support: call to function for_trim cannot be vectorized   [ comm_diffuse_comp_mod.f90(624,18) ]
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed FLOW dependence between at (624:18) and at (624:47)
         remark #15346: vector dependence: assumed ANTI dependence between at (624:47) and at (624:18)
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(653,5)
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
   remark #15328: vectorization support: non-unit strided load was emulated for the variable <diffcomps(k)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(654,12) ]
   remark #15328: vectorization support: indirect load was emulated for the variable <diffcomps(k)%COMM>, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(654,12) ]
   remark #15305: vectorization support: vector length 4
   remark #15309: vectorization support: normalized vectorization overhead 0.405
   remark #15452: unmasked strided loads: 1 
   remark #15462: unmasked indexed (or gather) loads: 1 
   remark #15475: --- begin vector cost summary ---
   remark #15476: scalar cost: 20 
   remark #15477: vector cost: 21.000 
   remark #15478: estimated potential speedup: 0.940 
   remark #15488: --- end vector cost summary ---
   remark #25439: unrolled with remainder by 2  
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(653,5)
<Remainder>
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(659,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between i (661:14) and i (661:14)
   remark #17106: parallel dependence: assumed FLOW dependence between i (661:14) and i (661:14)
   remark #17106: parallel dependence: assumed OUTPUT dependence between ind_pre_(i) (662:14) and ind_pre_(i) (662:14)
   remark #17106: parallel dependence: assumed OUTPUT dependence between ind_pre_(i) (662:14) and ind_pre_(i) (662:14)
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between i (661:14) and i (661:14)
   remark #15346: vector dependence: assumed FLOW dependence between i (661:14) and i (661:14)
   remark #25439: unrolled with remainder by 2  
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(659,8)
<Remainder>
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(594,50):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(606,11):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(606,11):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(587,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_updatediffprecond_pseudoinv_] comm_diffuse_comp_mod.f90:587

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   20[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm5]
        
    Routine temporaries
        Total         :     479
            Global    :     102
            Local     :     377
        Regenerable   :      63
        Spilled       :      14
        
    Routine stack
        Variables     :    1160 bytes*
            Reads     :      38 [3.12e+02 ~ 5.7%]
            Writes    :      27 [1.33e+01 ~ 0.2%]
        Spills        :      80 bytes*
            Reads     :      18 [8.59e+01 ~ 1.6%]
            Writes    :      14 [6.85e+01 ~ 1.3%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::UPDATEDIFFUSEMIXMAT

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::UPDATEDIFFUSEMIXMAT) [5/20=25.0%] comm_diffuse_comp_mod.f90(677,14)
  -> EXTERN: (692,9) for_trim
  -> EXTERN: (692,25) for_cpstr
  -> EXTERN: (694,10) __resetsp_inlined
  -> EXTERN: (694,10) UPDATE_STATUS
  -> EXTERN: (694,10) __getsp_inlined
  -> EXTERN: (694,46) for_concat
  -> EXTERN: (694,46) _alloca
  -> EXTERN: (694,49) for_trim
  -> EXTERN: (699,11) for_dealloc_allocatable
  -> EXTERN: (699,11) for_realloc_lhs
  -> EXTERN: (704,5) for_alloc_allocatable
  -> EXTERN: (704,5) for_check_mult_overflow64
  -> EXTERN: (717,11) for_check_mult_overflow64
  -> EXTERN: (717,11) for_alloc_allocatable
  -> EXTERN: (729,22) COMM_MAP_MOD^CONSTRUCTOR_MAPINFO
  -> EXTERN: (730,22) COMM_MAP_MOD^CONSTRUCTOR_MAP
  -> INDIRECT-: (733,22)  (*((P64*) comm_diffuse_comp_mod_mp_updatediffusemixmat_$T$73_V$4afa.0.9))[6(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (735,22) wall_time_
  -> INDIRECT-: (736,22)  (*((P64*) *((P64*) (&((MAP_PTR$P$19_V$930 *)(comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$THETA$48_V$fc9)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_updatediffusemixmat_$J_V$4b0c.0.9)), l:((comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$THETA$48_V$fd2)[0(SI32), l:0(SI64)]] + 56(SI64)))))[16(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (737,22) wall_time_
  -> INDIRECT-: (741,47)  (*((P64*) *((P64*) (&((MAP_PTR$P$19_V$930 *)comm_diffuse_comp_mod_mp_updatediffusemixmat_$THETA_PREV$75_V$4b23.0.9)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_updatediffusemixmat_$J_V$4b0c.0.9)), l:comm_diffuse_comp_mod_mp_updatediffusemixmat_$THETA_PREV$75_V$4b2c.0.9[0(SI32), l:0(SI64)]] + 56(SI64)))))[12(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (742,33) COMM_MAP_MOD^CONSTRUCTOR_CLONE
  -> INDIRECT-: (743,19)  (*((P64*) comm_diffuse_comp_mod_mp_updatediffusemixmat_$T$73_V$4afa.0.9))[12(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (750,13) wall_time_
  -> EXTERN: (753,19) PIX2ANG_RING
  -> EXTERN: (785,33) for_cpstr
  -> EXTERN: (791,36) for_cpstr
  -> INDIRECT-: (801,40)  (*((P64*) *((P64*) (&((F_INT_PTR$P$29_V$1070 *)(comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$1094)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_updatediffusemixmat_$I_V$4b20.0.9)), l:((comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$109d)[0(SI32), l:0(SI64)]] + 56(SI64)))))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (807,40)  (*((P64*) *((P64*) (&((F_INT_PTR$P$29_V$1070 *)(comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$1094)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_updatediffusemixmat_$I_V$4b20.0.9)), l:((comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$109d)[0(SI32), l:0(SI64)]] + 56(SI64)))))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (820,43)  (*((P64*) *((P64*) (&((F_INT_PTR$P$29_V$1070 *)(comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$1094)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_updatediffusemixmat_$I_V$4b20.0.9)), l:((comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$109d)[0(SI32), l:0(SI64)]] + 56(SI64)))))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (822,43)  (*((P64*) *((P64*) (&((F_INT_PTR$P$29_V$1070 *)(comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$1094)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_updatediffusemixmat_$I_V$4b20.0.9)), l:((comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$109d)[0(SI32), l:0(SI64)]] + 56(SI64)))))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (833,43)  (*((P64*) *((P64*) (&((F_INT_PTR$P$29_V$1070 *)(comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$1094)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_updatediffusemixmat_$I_V$4b20.0.9)), l:((comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$109d)[0(SI32), l:0(SI64)]] + 56(SI64)))))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (835,43)  (*((P64*) *((P64*) (&((F_INT_PTR$P$29_V$1070 *)(comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$1094)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_updatediffusemixmat_$I_V$4b20.0.9)), l:((comm_diffuse_comp_mod_mp_updatediffusemixmat_$SELF_V$4abd.0.9->QNCAtemplate.dim31_dv_template.addr_a0_V$4c1e.0.9)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$109d)[0(SI32), l:0(SI64)]] + 56(SI64)))))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (842,32) for_dealloc_allocatable
  -> EXTERN: (843,13) wall_time_
  -> EXTERN: (847,8) for_check_mult_overflow64
  -> EXTERN: (847,8) for_alloc_allocatable
  -> EXTERN: (851,13) __getsp_inlined
  -> EXTERN: (851,13) mpi_allreduce_
  -> EXTERN: (851,13) __resetsp_inlined
  -> EXTERN: (851,27) _alloca
  -> EXTERN: (853,8) __getsp_inlined
  -> EXTERN: (853,8) _alloca
  -> EXTERN: (853,8) __resetsp_inlined
  -> EXTERN: (854,8) for_dealloc_allocatable
  -> INDIRECT-: (865,41)  (*((P64*) *((P64*) (&((MAP_PTR$P$19_V$930 *)comm_diffuse_comp_mod_mp_updatediffusemixmat_$THETA_PREV$75_V$4b23.0.9)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_updatediffusemixmat_$J_V$4b0c.0.9)), l:comm_diffuse_comp_mod_mp_updatediffusemixmat_$THETA_PREV$75_V$4b2c.0.9[0(SI32), l:0(SI64)]] + 56(SI64)))))[12(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (867,5) for_deallocate_all
  -> EXTERN: (867,5) for_finalize
  -> EXTERN: (869,10) __resetsp_inlined
  -> EXTERN: (869,10) UPDATE_STATUS
  -> EXTERN: (869,10) __getsp_inlined
  -> EXTERN: (869,46) for_concat
  -> EXTERN: (869,46) _alloca
  -> EXTERN: (869,49) for_trim
  -> EXTERN: (874,3) for_dealloc_all_nocheck
  -> EXTERN: (874,3) for_finalize
  -> EXTERN: (874,3) for_dealloc_allocatable
  -> EXTERN: (874,3) for_dealloc_allocatable
  -> EXTERN: (874,3) for_dealloc_allocatable
  -> EXTERN: (874,3) for_dealloc_allocatable


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(698,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between self(i) (699:11) and self(i) (699:11)
   remark #17106: parallel dependence: assumed FLOW dependence between self(i) (699:11) and self(i) (699:11)
   remark #17106: parallel dependence: assumed OUTPUT dependence between theta(i) (699:11) and theta(i) (699:11)
   remark #17106: parallel dependence: assumed OUTPUT dependence between theta(i) (699:11) and theta(i) (699:11)
   remark #15382: vectorization support: call to function for_realloc_lhs cannot be vectorized   [ comm_diffuse_comp_mod.f90(699,11) ]
   remark #15382: vectorization support: call to function for_dealloc_allocatable cannot be vectorized   [ comm_diffuse_comp_mod.f90(699,11) ]
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between self(i) (699:11) and self(i) (699:11)
   remark #15346: vector dependence: assumed FLOW dependence between self(i) (699:11) and self(i) (699:11)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(699,11)
   <Distributed chunk1>
      remark #25426: Loop Distributed (2 way) 
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
      remark #15305: vectorization support: vector length 2
      remark #15399: vectorization support: unroll factor set to 4
      remark #15309: vectorization support: normalized vectorization overhead 0.950
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 2 
      remark #15477: vector cost: 5.000 
      remark #15478: estimated potential speedup: 0.390 
      remark #15488: --- end vector cost summary ---
      remark #25439: unrolled with remainder by 8  
      remark #25456: Number of Array Refs Scalar Replaced In Loop: 2
      remark #25457: Number of partial sums replaced: 2
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(699,11)
   <Distributed chunk2>
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between at (699:11) and at (699:11)
      remark #17106: parallel dependence: assumed FLOW dependence between at (699:11) and at (699:11)
      remark #15344: loop was not vectorized: vector dependence prevents vectorization
      remark #15346: vector dependence: assumed ANTI dependence between at (699:11) and at (699:11)
      remark #15346: vector dependence: assumed FLOW dependence between at (699:11) and at (699:11)

      LOOP BEGIN at comm_diffuse_comp_mod.f90(699,11)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed FLOW dependence between at (699:11) and at (699:11)
         remark #17106: parallel dependence: assumed ANTI dependence between at (699:11) and at (699:11)
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed FLOW dependence between at (699:11) and at (699:11)
         remark #15346: vector dependence: assumed ANTI dependence between at (699:11) and at (699:11)
         remark #25439: unrolled with remainder by 2  
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(699,11)
      <Remainder>
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(699,11)
   <Remainder, Distributed chunk1>
      remark #25436: completely unrolled by 7  
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(705,5)
<Peeled loop for vectorization, Multiversioned v1>
   remark #25015: Estimate of max trip count of loop=1
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(705,5)
<Multiversioned v1>
   remark #25233: Loop multiversioned for stride tests on Assumed shape arrays
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15388: vectorization support: reference THETA(j) has aligned access   [ comm_diffuse_comp_mod.f90(706,8) ]
   remark #15388: vectorization support: reference THETA(j) has aligned access   [ comm_diffuse_comp_mod.f90(706,8) ]
   remark #15305: vectorization support: vector length 2
   remark #15399: vectorization support: unroll factor set to 4
   remark #15309: vectorization support: normalized vectorization overhead 0.583
   remark #15300: LOOP WAS VECTORIZED
   remark #15449: unmasked aligned unit stride stores: 2 
   remark #15475: --- begin vector cost summary ---
   remark #15476: scalar cost: 4 
   remark #15477: vector cost: 3.000 
   remark #15478: estimated potential speedup: 1.300 
   remark #15488: --- end vector cost summary ---
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(705,5)
<Alternate Alignment Vectorized Loop, Multiversioned v1>
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(705,5)
<Remainder loop for vectorization, Multiversioned v1>
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(705,5)
<Multiversioned v2>
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
   remark #15329: vectorization support: non-unit strided store was emulated for the variable <THETA(j)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(706,8) ]
   remark #15329: vectorization support: non-unit strided store was emulated for the variable <THETA(j)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(706,8) ]
   remark #15305: vectorization support: vector length 2
   remark #15399: vectorization support: unroll factor set to 4
   remark #15453: unmasked strided stores: 2 
   remark #15475: --- begin vector cost summary ---
   remark #15476: scalar cost: 4 
   remark #15477: vector cost: 6.000 
   remark #15478: estimated potential speedup: 0.660 
   remark #15488: --- end vector cost summary ---
   remark #25439: unrolled with remainder by 2  
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(705,5)
<Remainder, Multiversioned v2>
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(709,5)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between self%COMM (715:22) and BUFFER (847:8)
   remark #17106: parallel dependence: assumed FLOW dependence between BUFFER (847:8) and self%COMM (715:22)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(719,11)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between self (729:79) and T (743:19)
      remark #17106: parallel dependence: assumed FLOW dependence between T (743:19) and self (729:79)
      remark #15541: outer loop was not auto-vectorized: consider using SIMD directive

      LOOP BEGIN at comm_diffuse_comp_mod.f90(732,17)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed ANTI dependence between self(j) (732:17) and T(:,:) (732:17)
         remark #17106: parallel dependence: assumed FLOW dependence between T(:,:) (732:17) and self(j) (732:17)
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed ANTI dependence between self(j) (732:17) and T(:,:) (732:17)
         remark #15346: vector dependence: assumed FLOW dependence between T(:,:) (732:17) and self(j) (732:17)

         LOOP BEGIN at comm_diffuse_comp_mod.f90(732,17)
            remark #17104: loop was not parallelized: existence of parallel dependence
            remark #17106: parallel dependence: assumed FLOW dependence between T(:,:) (732:17) and self(j,:,:) (732:17)
            remark #17106: parallel dependence: assumed ANTI dependence between self(j,:,:) (732:17) and T(:,:) (732:17)
            remark #15344: loop was not vectorized: vector dependence prevents vectorization
            remark #15346: vector dependence: assumed FLOW dependence between T(:,:) (732:17) and self(j,:,:) (732:17)
            remark #15346: vector dependence: assumed ANTI dependence between self(j,:,:) (732:17) and T(:,:) (732:17)
            remark #25439: unrolled with remainder by 2  
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(732,17)
         <Remainder>
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(740,14)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed ANTI dependence between T(:,:) (740:14) and THETA_P(:,:,j) (740:14)
         remark #17106: parallel dependence: assumed FLOW dependence between THETA_P(:,:,j) (740:14) and T(:,:) (740:14)
         remark #15382: vectorization support: call to function ?1memcpy cannot be vectorized
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed ANTI dependence between T(:,:) (740:14) and THETA_P(:,:,j) (740:14)
         remark #15346: vector dependence: assumed FLOW dependence between THETA_P(:,:,j) (740:14) and T(:,:) (740:14)

         LOOP BEGIN at comm_diffuse_comp_mod.f90(740,14)
            remark #25401: memcopy(with guard) generated
            remark #17104: loop was not parallelized: existence of parallel dependence
            remark #17106: parallel dependence: assumed ANTI dependence between T(:,:) (740:14) and THETA_P(:,:,j) (740:14)
            remark #17106: parallel dependence: assumed FLOW dependence between THETA_P(:,:,j) (740:14) and T(:,:) (740:14)
            remark #15398: loop was not vectorized: loop was transformed to memset or memcpy

            LOOP BEGIN at comm_diffuse_comp_mod.f90(740,14)
            <Multiversioned v2>
               remark #17104: loop was not parallelized: existence of parallel dependence
               remark #17106: parallel dependence: assumed FLOW dependence between THETA_P(:,:,j) (740:14) and T(:,:) (740:14)
               remark #17106: parallel dependence: assumed ANTI dependence between T(:,:) (740:14) and THETA_P(:,:,j) (740:14)
               remark #15304: loop was not vectorized: non-vectorizable loop instance from multiversioning
               remark #25439: unrolled with remainder by 2  
            LOOP END

            LOOP BEGIN at comm_diffuse_comp_mod.f90(740,14)
            <Remainder, Multiversioned v2>
            LOOP END
         LOOP END
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(751,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(756,17)
         remark #17102: loop was not parallelized: not a parallelization candidate
         remark #15523: loop was not vectorized: loop control variable ? was found, but loop iteration count cannot be computed before executing the loop
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(816,23)
      <Peeled loop for vectorization>
         remark #25015: Estimate of max trip count of loop=3
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(816,23)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15305: vectorization support: vector length 4
         remark #15309: vectorization support: normalized vectorization overhead 2.733
         remark #15300: LOOP WAS VECTORIZED
         remark #15442: entire loop may be executed in remainder
         remark #15448: unmasked aligned unit stride loads: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 19 
         remark #15477: vector cost: 3.750 
         remark #15478: estimated potential speedup: 4.640 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(816,23)
      <Remainder loop for vectorization>
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(829,23)
      <Peeled loop for vectorization>
         remark #25015: Estimate of max trip count of loop=3
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(829,23)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15305: vectorization support: vector length 4
         remark #15309: vectorization support: normalized vectorization overhead 2.733
         remark #15300: LOOP WAS VECTORIZED
         remark #15448: unmasked aligned unit stride loads: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 19 
         remark #15477: vector cost: 3.750 
         remark #15478: estimated potential speedup: 4.640 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(829,23)
      <Remainder loop for vectorization>
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(849,11)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between at (849:30) and at (849:11)
      remark #17106: parallel dependence: assumed FLOW dependence between at (849:11) and at (849:30)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(849,30)
      <Peeled loop for vectorization>
         remark #25015: Estimate of max trip count of loop=1
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(849,30)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15388: vectorization support: reference at (849:30) has aligned access
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 4
         remark #15309: vectorization support: normalized vectorization overhead 1.550
         remark #15300: LOOP WAS VECTORIZED
         remark #15442: entire loop may be executed in remainder
         remark #15448: unmasked aligned unit stride loads: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 6 
         remark #15477: vector cost: 2.500 
         remark #15478: estimated potential speedup: 2.260 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(849,30)
      <Remainder loop for vectorization>
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(851,27)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15388: vectorization support: reference at (851:27) has aligned access
      remark #15328: vectorization support: non-unit strided load was emulated for the variable <at (851:27)>, stride is unknown to compiler
      remark #15305: vectorization support: vector length 2
      remark #15300: LOOP WAS VECTORIZED
      remark #15449: unmasked aligned unit stride stores: 1 
      remark #15452: unmasked strided loads: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 4 
      remark #15477: vector cost: 2.500 
      remark #15478: estimated potential speedup: 1.600 
      remark #15488: --- end vector cost summary ---
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(851,27)
   <Remainder loop for vectorization>
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(851,27)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15388: vectorization support: reference at (851:27) has aligned access
      remark #15329: vectorization support: non-unit strided store was emulated for the variable <at (851:27)>, stride is unknown to compiler
      remark #15305: vectorization support: vector length 2
      remark #15300: LOOP WAS VECTORIZED
      remark #15448: unmasked aligned unit stride loads: 1 
      remark #15453: unmasked strided stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 4 
      remark #15477: vector cost: 3.000 
      remark #15478: estimated potential speedup: 1.330 
      remark #15488: --- end vector cost summary ---
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(851,27)
   <Remainder loop for vectorization>
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(853,8)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15388: vectorization support: reference at (853:8) has aligned access
      remark #15388: vectorization support: reference BUFFER(:) has aligned access   [ comm_diffuse_comp_mod.f90(853,36) ]
      remark #15305: vectorization support: vector length 2
      remark #15399: vectorization support: unroll factor set to 4
      remark #15309: vectorization support: normalized vectorization overhead 0.014
      remark #15300: LOOP WAS VECTORIZED
      remark #15448: unmasked aligned unit stride loads: 1 
      remark #15449: unmasked aligned unit stride stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 31 
      remark #15477: vector cost: 17.500 
      remark #15478: estimated potential speedup: 1.750 
      remark #15486: divides: 1 
      remark #15488: --- end vector cost summary ---
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(853,8)
   <Remainder loop for vectorization>
      remark #15388: vectorization support: reference at (853:8) has aligned access
      remark #15388: vectorization support: reference BUFFER(:) has aligned access   [ comm_diffuse_comp_mod.f90(853,36) ]
      remark #15305: vectorization support: vector length 2
      remark #15309: vectorization support: normalized vectorization overhead 0.308
      remark #15301: REMAINDER LOOP WAS VECTORIZED
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(853,8)
   <Remainder loop for vectorization>
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(853,8)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15388: vectorization support: reference at (853:8) has aligned access
      remark #15329: vectorization support: non-unit strided store was emulated for the variable <at (853:8)>, stride is unknown to compiler
      remark #15305: vectorization support: vector length 2
      remark #15300: LOOP WAS VECTORIZED
      remark #15448: unmasked aligned unit stride loads: 1 
      remark #15453: unmasked strided stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 4 
      remark #15477: vector cost: 3.000 
      remark #15478: estimated potential speedup: 1.330 
      remark #15488: --- end vector cost summary ---
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(853,8)
   <Remainder loop for vectorization>
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(865,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #15382: vectorization support: call to function (Indirect call) cannot be vectorized   [ comm_diffuse_comp_mod.f90(865,41) ]
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #25439: unrolled with remainder by 2  
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(865,8)
<Remainder>
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(690,55):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(687,62):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(687,59):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(687,55):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(686,55):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(740,14):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(740,14):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(740,14):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(677,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_updatediffusemixmat_] comm_diffuse_comp_mod.f90:677

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   21[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm6]
        
    Routine temporaries
        Total         :    1618
            Global    :     310
            Local     :    1308
        Regenerable   :     164
        Spilled       :      54
        
    Routine stack
        Variables     :    3248 bytes*
            Reads     :     106 [1.91e+02 ~ 3.2%]
            Writes    :     202 [2.71e+02 ~ 4.6%]
        Spills        :     392 bytes*
            Reads     :     117 [2.68e+02 ~ 4.5%]
            Writes    :      67 [9.16e+01 ~ 1.5%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::EVALDIFFUSEBAND

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::EVALDIFFUSEBAND) [6/20=30.0%] comm_diffuse_comp_mod.f90(877,12)
  -> EXTERN: (877,12) for_deallocate
  -> EXTERN: (877,48) for_deallocate
  -> EXTERN: (896,49) for_alloc_allocatable
  -> EXTERN: (896,49) for_check_mult_overflow64
  -> EXTERN: (898,49) for_alloc_allocatable
  -> EXTERN: (898,49) for_check_mult_overflow64
  -> EXTERN: (906,14) COMM_MAP_MOD^CONSTRUCTOR_MAPINFO
  -> EXTERN: (907,14) COMM_MAP_MOD^CONSTRUCTOR_MAP
  -> INDIRECT-: (912,13)  (*((P64*) *((P64*) (&(comm_diffuse_comp_mod_mp_evaldiffuseband_$SELF_V$4f4d.0.10->QNCAtemplate.dim31_dv_template.addr_a0_V$4fcc.0.10)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$X$43_V$ec4 + 56(SI64)))))[13(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (929,16)  (*((P64*) comm_diffuse_comp_mod_mp_evaldiffuseband_$M$78_V$4f9c.0.10))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (930,11) for_realloc_lhs
  -> INDIRECT-: (931,16)  (*((P64*) comm_diffuse_comp_mod_mp_evaldiffuseband_$M$78_V$4f9c.0.10))[3(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (936,10)  (*((P64*) *((P64*) (&((COMM_DATA_SET$B$28_V$1169 *)comm_data_mod_mp_data__V$395e)[(EXPR_CONV.SI32.SI64(*((SI32*) comm_diffuse_comp_mod_mp_evaldiffuseband_$BAND_V$4f4e.0.10))), l:comm_data_mod_mp_data__V$3967[0(SI32), l:0(SI64)]] + 56(SI64)))))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (937,25)  (*((P64*) comm_diffuse_comp_mod_mp_evaldiffuseband_$M$78_V$4f9c.0.10))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (942,46) for_alloc_allocatable
  -> EXTERN: (942,46) for_check_mult_overflow64
  -> EXTERN: (946,46) for_check_mult_overflow64
  -> EXTERN: (946,46) for_alloc_allocatable
  -> INDIRECT-: (953,10)  (*((P64*) comm_diffuse_comp_mod_mp_evaldiffuseband_$M$78_V$4f9c.0.10))[12(SI64)]
     [[ Unable to inline indirect callsite  <2>]]


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(900,8)
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(900,8)
      remark #25408: memset generated
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between evaldiffuseband(:,:) (900:8) and evaldiffuseband(:,:) (900:8)
      remark #17106: parallel dependence: assumed OUTPUT dependence between evaldiffuseband(:,:) (900:8) and evaldiffuseband(:,:) (900:8)
      remark #15542: loop was not vectorized: inner loop was already vectorized
      remark #25015: Estimate of max trip count of loop=1

      LOOP BEGIN at comm_diffuse_comp_mod.f90(900,8)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15389: vectorization support: reference evaldiffuseband(:,:) has unaligned access
         remark #15381: vectorization support: unaligned access used inside loop body
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 2
         remark #15309: vectorization support: normalized vectorization overhead 0.300
         remark #15300: LOOP WAS VECTORIZED
         remark #15451: unmasked unaligned unit stride stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 2.500 
         remark #15478: estimated potential speedup: 1.450 
         remark #15488: --- end vector cost summary ---
         remark #25015: Estimate of max trip count of loop=3
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(900,8)
      <Remainder loop for vectorization>
         remark #25015: Estimate of max trip count of loop=12
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(909,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed FLOW dependence between M(:,:) (909:8) and amp_in(:,:) (909:8)
   remark #17106: parallel dependence: assumed ANTI dependence between amp_in(:,:) (909:8) and M(:,:) (909:8)
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed FLOW dependence between M(:,:) (909:8) and amp_in(:,:) (909:8)
   remark #15346: vector dependence: assumed ANTI dependence between amp_in(:,:) (909:8) and M(:,:) (909:8)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(909,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed FLOW dependence between M(:,:) (909:8) and amp_in(:,:) (909:8)
      remark #17106: parallel dependence: assumed ANTI dependence between amp_in(:,:) (909:8) and M(:,:) (909:8)
      remark #15344: loop was not vectorized: vector dependence prevents vectorization
      remark #15346: vector dependence: assumed FLOW dependence between M(:,:) (909:8) and amp_in(:,:) (909:8)
      remark #15346: vector dependence: assumed ANTI dependence between amp_in(:,:) (909:8) and M(:,:) (909:8)
      remark #25439: unrolled with remainder by 2  
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(909,8)
   <Remainder>
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(926,14)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between self(band,i) (926:40) and M(:,i) (926:14)
   remark #17106: parallel dependence: assumed FLOW dependence between M(:,i) (926:14) and self(band,i) (926:40)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(926,14)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15389: vectorization support: reference M(:,i) has unaligned access
      remark #15389: vectorization support: reference M(:,i) has unaligned access   [ comm_diffuse_comp_mod.f90(926,40) ]
      remark #15381: vectorization support: unaligned access used inside loop body
      remark #15305: vectorization support: vector length 2
      remark #15399: vectorization support: unroll factor set to 4
      remark #15309: vectorization support: normalized vectorization overhead 0.231
      remark #15300: LOOP WAS VECTORIZED
      remark #15450: unmasked unaligned unit stride loads: 1 
      remark #15451: unmasked unaligned unit stride stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 10 
      remark #15477: vector cost: 6.500 
      remark #15478: estimated potential speedup: 1.510 
      remark #15488: --- end vector cost summary ---
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(926,14)
   <Remainder loop for vectorization>
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(930,11)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between at (930:11) and M(:,:) (930:11)
   remark #17106: parallel dependence: assumed FLOW dependence between M(:,:) (930:11) and at (930:11)
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between at (930:11) and M(:,:) (930:11)
   remark #15346: vector dependence: assumed FLOW dependence between M(:,:) (930:11) and at (930:11)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(930,11)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed FLOW dependence between M(:,:) (930:11) and at (930:11)
      remark #17106: parallel dependence: assumed ANTI dependence between at (930:11) and M(:,:) (930:11)
      remark #15344: loop was not vectorized: vector dependence prevents vectorization
      remark #15346: vector dependence: assumed FLOW dependence between M(:,:) (930:11) and at (930:11)
      remark #15346: vector dependence: assumed ANTI dependence between at (930:11) and M(:,:) (930:11)
      remark #25439: unrolled with remainder by 2  
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(930,11)
   <Remainder>
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(943,44)
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(943,44)
      remark #25408: memset generated
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between evaldiffuseband(:,:) (943:44) and evaldiffuseband(:,:) (943:44)
      remark #17106: parallel dependence: assumed OUTPUT dependence between evaldiffuseband(:,:) (943:44) and evaldiffuseband(:,:) (943:44)
      remark #15542: loop was not vectorized: inner loop was already vectorized
      remark #25015: Estimate of max trip count of loop=1

      LOOP BEGIN at comm_diffuse_comp_mod.f90(943,44)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15389: vectorization support: reference evaldiffuseband(:,:) has unaligned access
         remark #15381: vectorization support: unaligned access used inside loop body
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 2
         remark #15309: vectorization support: normalized vectorization overhead 0.300
         remark #15300: LOOP WAS VECTORIZED
         remark #15451: unmasked unaligned unit stride stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 2.500 
         remark #15478: estimated potential speedup: 1.450 
         remark #15488: --- end vector cost summary ---
         remark #25015: Estimate of max trip count of loop=3
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(943,44)
      <Remainder loop for vectorization>
         remark #25015: Estimate of max trip count of loop=12
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(944,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between M(:,:) (944:8) and evaldiffuseband(:,:) (944:8)
   remark #17106: parallel dependence: assumed FLOW dependence between evaldiffuseband(:,:) (944:8) and M(:,:) (944:8)
   remark #15382: vectorization support: call to function ?1memcpy cannot be vectorized
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between M(:,:) (944:8) and evaldiffuseband(:,:) (944:8)
   remark #15346: vector dependence: assumed FLOW dependence between evaldiffuseband(:,:) (944:8) and M(:,:) (944:8)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(944,8)
      remark #25401: memcopy(with guard) generated
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between M(:,:) (944:8) and evaldiffuseband(:,:) (944:8)
      remark #17106: parallel dependence: assumed FLOW dependence between evaldiffuseband(:,:) (944:8) and M(:,:) (944:8)
      remark #15398: loop was not vectorized: loop was transformed to memset or memcpy

      LOOP BEGIN at comm_diffuse_comp_mod.f90(944,8)
      <Multiversioned v2>
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed FLOW dependence between evaldiffuseband(:,:) (944:8) and M(:,:) (944:8)
         remark #17106: parallel dependence: assumed ANTI dependence between M(:,:) (944:8) and evaldiffuseband(:,:) (944:8)
         remark #15304: loop was not vectorized: non-vectorizable loop instance from multiversioning
         remark #25439: unrolled with remainder by 2  
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(944,8)
      <Remainder, Multiversioned v2>
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(947,44)
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(947,44)
      remark #25408: memset generated
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between evaldiffuseband(:,:) (947:44) and evaldiffuseband(:,:) (947:44)
      remark #17106: parallel dependence: assumed OUTPUT dependence between evaldiffuseband(:,:) (947:44) and evaldiffuseband(:,:) (947:44)
      remark #15542: loop was not vectorized: inner loop was already vectorized
      remark #25015: Estimate of max trip count of loop=1

      LOOP BEGIN at comm_diffuse_comp_mod.f90(947,44)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15389: vectorization support: reference evaldiffuseband(:,:) has unaligned access
         remark #15381: vectorization support: unaligned access used inside loop body
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 2
         remark #15309: vectorization support: normalized vectorization overhead 0.300
         remark #15300: LOOP WAS VECTORIZED
         remark #15451: unmasked unaligned unit stride stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 2.500 
         remark #15478: estimated potential speedup: 1.450 
         remark #15488: --- end vector cost summary ---
         remark #25015: Estimate of max trip count of loop=3
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(947,44)
      <Remainder loop for vectorization>
         remark #25015: Estimate of max trip count of loop=12
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(948,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between M(:,:) (948:8) and evaldiffuseband(:,:) (948:8)
   remark #17106: parallel dependence: assumed FLOW dependence between evaldiffuseband(:,:) (948:8) and M(:,:) (948:8)
   remark #15382: vectorization support: call to function ?1memcpy cannot be vectorized
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between M(:,:) (948:8) and evaldiffuseband(:,:) (948:8)
   remark #15346: vector dependence: assumed FLOW dependence between evaldiffuseband(:,:) (948:8) and M(:,:) (948:8)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(948,8)
      remark #25401: memcopy(with guard) generated
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between M(:,:) (948:8) and evaldiffuseband(:,:) (948:8)
      remark #17106: parallel dependence: assumed FLOW dependence between evaldiffuseband(:,:) (948:8) and M(:,:) (948:8)
      remark #15398: loop was not vectorized: loop was transformed to memset or memcpy

      LOOP BEGIN at comm_diffuse_comp_mod.f90(948,8)
      <Multiversioned v2>
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed FLOW dependence between evaldiffuseband(:,:) (948:8) and M(:,:) (948:8)
         remark #17106: parallel dependence: assumed ANTI dependence between M(:,:) (948:8) and evaldiffuseband(:,:) (948:8)
         remark #15304: loop was not vectorized: non-vectorizable loop instance from multiversioning
         remark #25439: unrolled with remainder by 2  
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(948,8)
      <Remainder, Multiversioned v2>
      LOOP END
   LOOP END
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(900,8):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(900,8):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(947,44):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(947,44):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(948,8):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(948,8):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(948,8):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(943,44):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(943,44):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(944,8):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(944,8):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(944,8):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(877,12):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_evaldiffuseband_] comm_diffuse_comp_mod.f90:877

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   20[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm5]
        
    Routine temporaries
        Total         :     765
            Global    :     200
            Local     :     565
        Regenerable   :      80
        Spilled       :      76
        
    Routine stack
        Variables     :     648 bytes*
            Reads     :      40 [1.76e+01 ~ 1.7%]
            Writes    :      64 [3.32e+01 ~ 3.2%]
        Spills        :     552 bytes*
            Reads     :     139 [5.55e+01 ~ 5.3%]
            Writes    :      92 [2.48e+01 ~ 2.4%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::PROJECTDIFFUSEBAND

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::PROJECTDIFFUSEBAND) [7/20=35.0%] comm_diffuse_comp_mod.f90(959,12)
  -> EXTERN: (959,12) for_deallocate
  -> EXTERN: (973,49) for_alloc_allocatable
  -> EXTERN: (973,49) for_check_mult_overflow64
  -> EXTERN: (979,18) COMM_MAP_MOD^CONSTRUCTOR_MAPINFO
  -> EXTERN: (980,18) COMM_MAP_MOD^CONSTRUCTOR_MAPINFO
  -> EXTERN: (981,18) COMM_MAP_MOD^CONSTRUCTOR_MAP
  -> EXTERN: (982,18) COMM_MAP_MOD^CONSTRUCTOR_MAP
  -> EXTERN: (987,8) _alloca
  -> EXTERN: (987,8) for_realloc_lhs
  -> EXTERN: (987,8) __resetsp_inlined
  -> EXTERN: (987,8) __getsp_inlined
  -> EXTERN: (989,8) __resetsp_inlined
  -> EXTERN: (989,8) for_realloc_lhs
  -> EXTERN: (989,8) _alloca
  -> EXTERN: (989,8) __getsp_inlined
  -> INDIRECT-: (990,13)  (*((P64*) comm_diffuse_comp_mod_mp_projectdiffuseband_$M$83_V$5240.0.11))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (992,10)  (*((P64*) *((P64*) (&((COMM_DATA_SET$B$28_V$1169 *)comm_data_mod_mp_data__V$395e)[(EXPR_CONV.SI32.SI64(*((SI32*) comm_diffuse_comp_mod_mp_projectdiffuseband_$BAND_V$51fd.0.11))), l:comm_data_mod_mp_data__V$3967[0(SI32), l:0(SI64)]] + 56(SI64)))))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (999,13)  (*((P64*) comm_diffuse_comp_mod_mp_projectdiffuseband_$M$83_V$5240.0.11))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1001,13)  (*((P64*) comm_diffuse_comp_mod_mp_projectdiffuseband_$M$83_V$5240.0.11))[3(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1003,10)  (*((P64*) comm_diffuse_comp_mod_mp_projectdiffuseband_$M$83_V$5240.0.11))[13(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1005,46) for_check_mult_overflow64
  -> EXTERN: (1005,46) for_alloc_allocatable
  -> EXTERN: (1006,5) for_realloc_lhs
  -> EXTERN: (1006,5) for_dealloc_allocatable
  -> INDIRECT-: (1008,10)  (*((P64*) comm_diffuse_comp_mod_mp_projectdiffuseband_$M$83_V$5240.0.11))[12(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1009,10)  (*((P64*) comm_diffuse_comp_mod_mp_projectdiffuseband_$M_OUT$80_V$5209.0.11))[12(SI64)]
     [[ Unable to inline indirect callsite  <2>]]


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(974,8)
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(974,8)
      remark #25408: memset generated
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between projectdiffuseband(:,:) (974:8) and projectdiffuseband(:,:) (974:8)
      remark #17106: parallel dependence: assumed OUTPUT dependence between projectdiffuseband(:,:) (974:8) and projectdiffuseband(:,:) (974:8)
      remark #15542: loop was not vectorized: inner loop was already vectorized
      remark #25015: Estimate of max trip count of loop=1

      LOOP BEGIN at comm_diffuse_comp_mod.f90(974,8)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15389: vectorization support: reference projectdiffuseband(:,:) has unaligned access
         remark #15381: vectorization support: unaligned access used inside loop body
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 2
         remark #15309: vectorization support: normalized vectorization overhead 0.300
         remark #15300: LOOP WAS VECTORIZED
         remark #15451: unmasked unaligned unit stride stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 2.500 
         remark #15478: estimated potential speedup: 1.450 
         remark #15488: --- end vector cost summary ---
         remark #25015: Estimate of max trip count of loop=3
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(974,8)
      <Remainder loop for vectorization>
         remark #25015: Estimate of max trip count of loop=12
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(987,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between at (987:8) and at (987:8)
   remark #17106: parallel dependence: assumed OUTPUT dependence between at (987:8) and at (987:8)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(987,8)
      remark #25399: memcopy generated
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between at (987:8) and at (987:8)
      remark #17106: parallel dependence: assumed OUTPUT dependence between at (987:8) and at (987:8)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(987,8)
      <Peeled loop for vectorization>
         remark #25015: Estimate of max trip count of loop=1
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(987,8)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15388: vectorization support: reference at (987:8) has aligned access
         remark #15388: vectorization support: reference nmaps(:,:) has aligned access
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 4.667
         remark #15300: LOOP WAS VECTORIZED
         remark #15448: unmasked aligned unit stride loads: 1 
         remark #15449: unmasked aligned unit stride stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 1.500 
         remark #15478: estimated potential speedup: 1.290 
         remark #15488: --- end vector cost summary ---
         remark #25015: Estimate of max trip count of loop=6
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(987,8)
      <Alternate Alignment Vectorized Loop>
         remark #25015: Estimate of max trip count of loop=6
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(987,8)
      <Remainder loop for vectorization>
         remark #25015: Estimate of max trip count of loop=12
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(987,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between M(:,:) (987:8) and M(:,:) (987:8)
   remark #17106: parallel dependence: assumed OUTPUT dependence between M(:,:) (987:8) and M(:,:) (987:8)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(987,8)
      remark #25399: memcopy generated
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between M(:,:) (987:8) and M(:,:) (987:8)
      remark #17106: parallel dependence: assumed OUTPUT dependence between M(:,:) (987:8) and M(:,:) (987:8)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(987,8)
      <Peeled loop for vectorization>
         remark #25015: Estimate of max trip count of loop=1
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(987,8)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15388: vectorization support: reference M(:,:) has aligned access
         remark #15388: vectorization support: reference at (987:8) has aligned access
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 4.667
         remark #15300: LOOP WAS VECTORIZED
         remark #15442: entire loop may be executed in remainder
         remark #15448: unmasked aligned unit stride loads: 1 
         remark #15449: unmasked aligned unit stride stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 1.500 
         remark #15478: estimated potential speedup: 1.290 
         remark #15488: --- end vector cost summary ---
         remark #25015: Estimate of max trip count of loop=6
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(987,8)
      <Alternate Alignment Vectorized Loop>
         remark #25015: Estimate of max trip count of loop=6
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(987,8)
      <Remainder loop for vectorization>
         remark #25015: Estimate of max trip count of loop=12
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(989,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between at (989:8) and at (989:8)
   remark #17106: parallel dependence: assumed OUTPUT dependence between at (989:8) and at (989:8)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(989,8)
      remark #25399: memcopy generated
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between at (989:8) and at (989:8)
      remark #17106: parallel dependence: assumed OUTPUT dependence between at (989:8) and at (989:8)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(989,8)
      <Peeled loop for vectorization>
         remark #25015: Estimate of max trip count of loop=1
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(989,8)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15388: vectorization support: reference at (989:8) has aligned access
         remark #15388: vectorization support: reference nmaps(:,:) has aligned access
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 4.667
         remark #15300: LOOP WAS VECTORIZED
         remark #15448: unmasked aligned unit stride loads: 1 
         remark #15449: unmasked aligned unit stride stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 1.500 
         remark #15478: estimated potential speedup: 1.290 
         remark #15488: --- end vector cost summary ---
         remark #25015: Estimate of max trip count of loop=6
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(989,8)
      <Alternate Alignment Vectorized Loop>
         remark #25015: Estimate of max trip count of loop=6
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(989,8)
      <Remainder loop for vectorization>
         remark #25015: Estimate of max trip count of loop=12
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(989,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between M(:,:) (989:8) and M(:,:) (989:8)
   remark #17106: parallel dependence: assumed OUTPUT dependence between M(:,:) (989:8) and M(:,:) (989:8)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(989,8)
      remark #25399: memcopy generated
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between M(:,:) (989:8) and M(:,:) (989:8)
      remark #17106: parallel dependence: assumed OUTPUT dependence between M(:,:) (989:8) and M(:,:) (989:8)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(989,8)
      <Peeled loop for vectorization>
         remark #25015: Estimate of max trip count of loop=1
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(989,8)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15388: vectorization support: reference M(:,:) has aligned access
         remark #15388: vectorization support: reference at (989:8) has aligned access
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 4.667
         remark #15300: LOOP WAS VECTORIZED
         remark #15442: entire loop may be executed in remainder
         remark #15448: unmasked aligned unit stride loads: 1 
         remark #15449: unmasked aligned unit stride stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 1.500 
         remark #15478: estimated potential speedup: 1.290 
         remark #15488: --- end vector cost summary ---
         remark #25015: Estimate of max trip count of loop=6
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(989,8)
      <Alternate Alignment Vectorized Loop>
         remark #25015: Estimate of max trip count of loop=6
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(989,8)
      <Remainder loop for vectorization>
         remark #25015: Estimate of max trip count of loop=12
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(996,11)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between self(band,i) (996:37) and M(:,i) (996:11)
   remark #17106: parallel dependence: assumed FLOW dependence between M(:,i) (996:11) and self(band,i) (996:37)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(996,11)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15389: vectorization support: reference M(:,i) has unaligned access
      remark #15389: vectorization support: reference M(:,i) has unaligned access   [ comm_diffuse_comp_mod.f90(996,37) ]
      remark #15381: vectorization support: unaligned access used inside loop body
      remark #15305: vectorization support: vector length 2
      remark #15399: vectorization support: unroll factor set to 4
      remark #15309: vectorization support: normalized vectorization overhead 0.231
      remark #15300: LOOP WAS VECTORIZED
      remark #15450: unmasked unaligned unit stride loads: 1 
      remark #15451: unmasked unaligned unit stride stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 10 
      remark #15477: vector cost: 6.500 
      remark #15478: estimated potential speedup: 1.510 
      remark #15488: --- end vector cost summary ---
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(996,11)
   <Remainder loop for vectorization>
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1000,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between self(band) (1000:8) and M(:,:) (1000:8)
   remark #17106: parallel dependence: assumed FLOW dependence between M(:,:) (1000:8) and self(band) (1000:8)
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between self(band) (1000:8) and M(:,:) (1000:8)
   remark #15346: vector dependence: assumed FLOW dependence between M(:,:) (1000:8) and self(band) (1000:8)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1000,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed FLOW dependence between M(:,:) (1000:8) and self(band,:,:) (1000:8)
      remark #17106: parallel dependence: assumed ANTI dependence between self(band,:,:) (1000:8) and M(:,:) (1000:8)
      remark #15344: loop was not vectorized: vector dependence prevents vectorization
      remark #15346: vector dependence: assumed FLOW dependence between M(:,:) (1000:8) and self(band,:,:) (1000:8)
      remark #15346: vector dependence: assumed ANTI dependence between self(band,:,:) (1000:8) and M(:,:) (1000:8)
      remark #25439: unrolled with remainder by 2  
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1000,8)
   <Remainder>
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1006,5)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between M(:,:) (1006:5) and projectdiffuseband(:,:) (1006:5)
   remark #17106: parallel dependence: assumed FLOW dependence between projectdiffuseband(:,:) (1006:5) and M(:,:) (1006:5)
   remark #15382: vectorization support: call to function ?1memcpy cannot be vectorized
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between M(:,:) (1006:5) and projectdiffuseband(:,:) (1006:5)
   remark #15346: vector dependence: assumed FLOW dependence between projectdiffuseband(:,:) (1006:5) and M(:,:) (1006:5)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1006,5)
      remark #25401: memcopy(with guard) generated
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between M(:,:) (1006:5) and projectdiffuseband(:,:) (1006:5)
      remark #17106: parallel dependence: assumed FLOW dependence between projectdiffuseband(:,:) (1006:5) and M(:,:) (1006:5)
      remark #15398: loop was not vectorized: loop was transformed to memset or memcpy

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1006,5)
      <Multiversioned v2>
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed FLOW dependence between projectdiffuseband(:,:) (1006:5) and M(:,:) (1006:5)
         remark #17106: parallel dependence: assumed ANTI dependence between M(:,:) (1006:5) and projectdiffuseband(:,:) (1006:5)
         remark #15304: loop was not vectorized: non-vectorizable loop instance from multiversioning
         remark #25439: unrolled with remainder by 2  
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1006,5)
      <Remainder, Multiversioned v2>
      LOOP END
   LOOP END
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(974,8):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(974,8):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(987,8):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(987,8):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(987,8):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(987,8):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(987,8):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(987,8):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(989,8):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(989,8):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(989,8):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(989,8):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(989,8):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(989,8):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1006,5):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1006,5):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1006,5):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(959,12):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_projectdiffuseband_] comm_diffuse_comp_mod.f90:959

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   20[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm5]
        
    Routine temporaries
        Total         :     718
            Global    :     170
            Local     :     548
        Regenerable   :      74
        Spilled       :      47
        
    Routine stack
        Variables     :    1244 bytes*
            Reads     :      69 [3.62e+01 ~ 1.8%]
            Writes    :     123 [6.60e+01 ~ 3.3%]
        Spills        :     336 bytes*
            Reads     :     122 [6.94e+01 ~ 3.5%]
            Writes    :      73 [2.85e+01 ~ 1.4%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::APPLYDIFFPRECOND

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::APPLYDIFFPRECOND) [8/20=40.0%] comm_diffuse_comp_mod.f90(1013,14)
  -> EXTERN: (1017,18) memmove
  -> EXTERN: (1017,18) _alloca
  -> EXTERN: (1017,18) for_trim
  -> EXTERN: (1018,11) for_cpstr
  -> (1019,13) COMM_DIFFUSE_COMP_MOD::APPLYDIFFPRECOND_DIAGONAL (isz = 1438) (sz = 1443)
     [[ Inlining would exceed -inline-max-size value (1443>253) <1>]]
  -> EXTERN: (1020,11) for_cpstr
  -> (1021,13) COMM_DIFFUSE_COMP_MOD::APPLYDIFFPRECOND_PSEUDOINV (isz = 2733) (sz = 2738)
     [[ Inlining would exceed -inline-max-size value (2738>253) <1>]]
  -> EXTERN: (1023,13) __resetsp_inlined
  -> EXTERN: (1023,13) REPORT_ERROR
  -> EXTERN: (1023,13) __getsp_inlined


    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(1017,18):remark #34014: optimization advice for memmove: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1017,18):remark #34026: call to memmove implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1013,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_applydiffprecond_] comm_diffuse_comp_mod.f90:1013

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   10[ rax rdx rcx rbx rsi rdi r8 r13-r15]
        
    Routine temporaries
        Total         :      48
            Global    :      16
            Local     :      32
        Regenerable   :      18
        Spilled       :       4
        
    Routine stack
        Variables     :     512 bytes*
            Reads     :       0 [0.00e+00 ~ 0.0%]
            Writes    :       0 [0.00e+00 ~ 0.0%]
        Spills        :       0 bytes*
            Reads     :       0 [0.00e+00 ~ 0.0%]
            Writes    :       0 [0.00e+00 ~ 0.0%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::APPLYDIFFPRECOND_DIAGONAL

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::APPLYDIFFPRECOND_DIAGONAL) [9/20=45.0%] comm_diffuse_comp_mod.f90(1028,14)
  -> EXTERN: (1039,5) for_alloc_allocatable
  -> EXTERN: (1039,5) for_check_mult_overflow64
  -> EXTERN: (1043,13) CR_EXTRACT_COMP_2D
  -> INDIRECT-: (1045,16)  (*((P64*) *((P64*) (&(((COMM_DIFFUSE_COMP_MOD$.btDIFF_PTR$P$50_V$10be *)comm_diffuse_comp_mod_mp_diffcomps_$56_V$39b4)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_applydiffprecond_diagonal_$I_V$5511.0.13)), l:comm_diffuse_comp_mod_mp_diffcomps_$56_V$39bd[0(SI32), l:0(SI64)]]->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$X$43_V$ec6)->COMM_MAP$INFO$13_V$6d1 + 56(SI64)))))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1046,16)  (*((P64*) comm_diffuse_comp_mod_mp_info_pre_$55_V$398a))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1049,8) for_dealloc_allocatable
  -> EXTERN: (1056,11) _alloca
  -> EXTERN: (1056,11) __getsp_inlined
  -> EXTERN: (1056,11) __resetsp_inlined
  -> EXTERN: (1057,18) _alloca
  -> EXTERN: (1064,8) for_check_mult_overflow64
  -> EXTERN: (1064,8) for_alloc_allocatable
  -> INDIRECT-: (1067,16)  (*((P64*) *((P64*) (&(((COMM_DIFFUSE_COMP_MOD$.btDIFF_PTR$P$50_V$10be *)comm_diffuse_comp_mod_mp_diffcomps_$56_V$39b4)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_applydiffprecond_diagonal_$I_V$5511.0.13)), l:comm_diffuse_comp_mod_mp_diffcomps_$56_V$39bd[0(SI32), l:0(SI64)]]->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$X$43_V$ec6)->COMM_MAP$INFO$13_V$6d1 + 56(SI64)))))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1068,16)  (*((P64*) comm_diffuse_comp_mod_mp_info_pre_$55_V$398a))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1071,13) CR_INSERT_COMP_2D
  -> EXTERN: (1072,8) for_dealloc_allocatable
  -> EXTERN: (1075,5) for_dealloc_allocatable
  -> EXTERN: (1077,3) for_dealloc_allocatable
  -> EXTERN: (1077,3) for_dealloc_allocatable


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(1040,5)
   remark #25096: Loop Interchange not done due to: Imperfect Loop Nest (Either at Source or due to other Compiler Transformations)
   remark #25452: Original Order found to be proper, but by a close margin
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1040,5)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1040,5)
         remark #25408: memset generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between Y(:,:,:) (1040:5) and Y(:,:,:) (1040:5)
         remark #17106: parallel dependence: assumed OUTPUT dependence between Y(:,:,:) (1040:5) and Y(:,:,:) (1040:5)
         remark #15542: loop was not vectorized: inner loop was already vectorized
         remark #25015: Estimate of max trip count of loop=1

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1040,5)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15389: vectorization support: reference Y(:,:,:) has unaligned access
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 2
            remark #15309: vectorization support: normalized vectorization overhead 0.300
            remark #15300: LOOP WAS VECTORIZED
            remark #15451: unmasked unaligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 2.500 
            remark #15478: estimated potential speedup: 1.450 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=3
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1040,5)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=12
         LOOP END
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1041,5)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between diffcomps (1042:8) and info_pre_ (1046:16)
   remark #15382: vectorization support: call to function CR_EXTRACT_COMP_2D cannot be vectorized   [ comm_diffuse_comp_mod.f90(1043,13) ]
   remark #15382: vectorization support: call to function (Indirect call) cannot be vectorized   [ comm_diffuse_comp_mod.f90(1045,16) ]
   remark #15382: vectorization support: call to function (Indirect call) cannot be vectorized   [ comm_diffuse_comp_mod.f90(1046,16) ]
   remark #15382: vectorization support: call to function for_dealloc_allocatable cannot be vectorized   [ comm_diffuse_comp_mod.f90(1049,8) ]
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between diffcomps (1042:8) and info_pre_ (1046:16)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1047,11)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1047,11)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15329: vectorization support: non-unit strided store was emulated for the variable <Y(i,k,:)>, stride is unknown to compiler
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <ALM(j,:)>, stride is unknown to compiler
         remark #15305: vectorization support: vector length 2
         remark #15452: unmasked strided loads: 1 
         remark #15453: unmasked strided stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 4.000 
         remark #15478: estimated potential speedup: 1.000 
         remark #15488: --- end vector cost summary ---
         remark #25439: unrolled with remainder by 2  
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1047,11)
      <Remainder>
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1053,5)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between at (1056:11) and at (1056:11)
   remark #17106: parallel dependence: assumed OUTPUT dependence between at (1056:11) and at (1056:11)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1054,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between at (1056:11) and at (1056:11)
      remark #17106: parallel dependence: assumed OUTPUT dependence between at (1056:11) and at (1056:11)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1056,11)
         remark #25399: memcopy generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between at (1056:11) and at (1056:11)
         remark #17106: parallel dependence: assumed OUTPUT dependence between at (1056:11) and at (1056:11)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1056,11)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference at (1056:13) has aligned access   [ comm_diffuse_comp_mod.f90(1056,13) ]
            remark #15389: vectorization support: reference p_cr(i,j,:) has unaligned access   [ comm_diffuse_comp_mod.f90(1056,13) ]
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15305: vectorization support: vector length 4
            remark #15309: vectorization support: normalized vectorization overhead 0.600
            remark #15300: LOOP WAS VECTORIZED
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15450: unmasked unaligned unit stride loads: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 1.250 
            remark #15478: estimated potential speedup: 2.900 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=6
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1056,11)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=24
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1057,18)
         remark #25408: memset generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between at (1057:18) and at (1057:18)
         remark #17106: parallel dependence: assumed OUTPUT dependence between at (1057:18) and at (1057:18)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1057,18)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference at (1057:18) has aligned access
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 2
            remark #15300: LOOP WAS VECTORIZED
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 1.500 
            remark #15478: estimated potential speedup: 2.660 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=3
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1057,18)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=12
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1057,18)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between at (1057:18) and at (1057:18)
         remark #17106: parallel dependence: assumed OUTPUT dependence between at (1057:18) and at (1057:18)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1057,18)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference at (1057:18) has aligned access
            remark #15388: vectorization support: reference at (1057:18) has aligned access
            remark #15389: vectorization support: reference p_cr(i,j,:,:) has unaligned access
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 4
            remark #15309: vectorization support: normalized vectorization overhead 0.139
            remark #15300: LOOP WAS VECTORIZED
            remark #15448: unmasked aligned unit stride loads: 1 
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15450: unmasked unaligned unit stride loads: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 12 
            remark #15477: vector cost: 4.500 
            remark #15478: estimated potential speedup: 2.590 
            remark #15488: --- end vector cost summary ---
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1057,18)
         <Remainder loop for vectorization>
            remark #15388: vectorization support: reference at (1057:18) has aligned access
            remark #15388: vectorization support: reference at (1057:18) has aligned access
            remark #15389: vectorization support: reference p_cr(i,j,:,:) has unaligned access
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15305: vectorization support: vector length 2
            remark #15309: vectorization support: normalized vectorization overhead 1.000
            remark #15301: REMAINDER LOOP WAS VECTORIZED
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1057,18)
         <Remainder loop for vectorization>
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1056,11)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between Y(p_cr(i,j,:),i,j) (1056:11) and Y(p_cr(i,j,:),i,j) (1056:11)
         remark #17106: parallel dependence: assumed OUTPUT dependence between Y(p_cr(i,j,:),i,j) (1056:11) and Y(p_cr(i,j,:),i,j) (1056:11)
         remark #15389: vectorization support: reference p_cr(i,j,:) has unaligned access
         remark #15388: vectorization support: reference at (1056:11) has aligned access
         remark #15381: vectorization support: unaligned access used inside loop body
         remark #15329: vectorization support: irregularly indexed store was emulated for the variable <Y(p_cr(i,j,:),i,j)>, part of index is read from memory
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 0.300
         remark #15300: LOOP WAS VECTORIZED
         remark #15448: unmasked aligned unit stride loads: 1 
         remark #15450: unmasked unaligned unit stride loads: 1 
         remark #15463: unmasked indexed (or scatter) stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 7 
         remark #15477: vector cost: 5.000 
         remark #15478: estimated potential speedup: 1.390 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1056,11)
      <Remainder loop for vectorization>
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1062,5)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between diffcomps (1063:8) and alm (1071:13)
   remark #17106: parallel dependence: assumed FLOW dependence between alm (1071:13) and diffcomps (1063:8)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1065,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between ALM(:,:) (1065:8) and ALM(:,:) (1065:8)
      remark #17106: parallel dependence: assumed OUTPUT dependence between ALM(:,:) (1065:8) and ALM(:,:) (1065:8)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1065,8)
         remark #25408: memset generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between ALM(:,:) (1065:8) and ALM(:,:) (1065:8)
         remark #17106: parallel dependence: assumed OUTPUT dependence between ALM(:,:) (1065:8) and ALM(:,:) (1065:8)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1065,8)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15389: vectorization support: reference ALM(:,:) has unaligned access
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 2
            remark #15309: vectorization support: normalized vectorization overhead 0.300
            remark #15300: LOOP WAS VECTORIZED
            remark #15451: unmasked unaligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 2.500 
            remark #15478: estimated potential speedup: 1.450 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=3
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1065,8)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=12
         LOOP END
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1069,11)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1069,11)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15329: vectorization support: non-unit strided store was emulated for the variable <ALM(j,:)>, stride is unknown to compiler
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <Y(i,k,:)>, stride is unknown to compiler
         remark #15305: vectorization support: vector length 2
         remark #15452: unmasked strided loads: 1 
         remark #15453: unmasked strided stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 4.000 
         remark #15478: estimated potential speedup: 1.000 
         remark #15488: --- end vector cost summary ---
         remark #25439: unrolled with remainder by 2  
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1069,11)
      <Remainder>
      LOOP END
   LOOP END
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(1034,48):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1033,48):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1040,5):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1040,5):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1056,11):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1056,11):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1056,11):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1057,18):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1057,18):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1065,8):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1065,8):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1028,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_applydiffprecond_diagonal_] comm_diffuse_comp_mod.f90:1028

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   21[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm6]
        
    Routine temporaries
        Total         :     729
            Global    :     178
            Local     :     551
        Regenerable   :      72
        Spilled       :      56
        
    Routine stack
        Variables     :     248 bytes*
            Reads     :      43 [2.57e+02 ~ 2.3%]
            Writes    :      57 [9.89e+01 ~ 0.9%]
        Spills        :     400 bytes*
            Reads     :      84 [6.46e+02 ~ 5.9%]
            Writes    :      63 [2.51e+02 ~ 2.3%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::APPLYDIFFPRECOND_PSEUDOINV

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::APPLYDIFFPRECOND_PSEUDOINV) [10/20=50.0%] comm_diffuse_comp_mod.f90(1080,14)
  -> EXTERN: (1097,10) UPDATE_STATUS
  -> EXTERN: (1098,5) for_alloc_allocatable
  -> EXTERN: (1098,5) for_check_mult_overflow64
  -> EXTERN: (1099,5) for_alloc_allocatable
  -> EXTERN: (1099,5) for_check_mult_overflow64
  -> EXTERN: (1104,13) CR_EXTRACT_COMP_2D
  -> INDIRECT-: (1106,16)  (*((P64*) *((P64*) (&(((COMM_DIFFUSE_COMP_MOD$.btDIFF_PTR$P$50_V$10be *)comm_diffuse_comp_mod_mp_diffcomps_$56_V$39b4)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_applydiffprecond_pseudoinv_$II_V$55c2.0.14)), l:comm_diffuse_comp_mod_mp_diffcomps_$56_V$39bd[0(SI32), l:0(SI64)]]->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$X$43_V$ec6)->COMM_MAP$INFO$13_V$6d1 + 56(SI64)))))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1107,16)  (*((P64*) comm_diffuse_comp_mod_mp_info_pre_$55_V$398a))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1110,8) for_dealloc_allocatable
  -> EXTERN: (1116,13) UPDATE_STATUS
  -> EXTERN: (1117,18) COMM_MAP_MOD^CONSTRUCTOR_MAP
  -> INDIRECT-: (1124,16)  (*((P64*) *((P64*) (&((COMM_DATA_SET$INFO$20_V$10d9 *)comm_data_mod_mp_data__V$395e)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_applydiffprecond_pseudoinv_$K_V$55c3.0.14)), l:comm_data_mod_mp_data__V$3967[0(SI32), l:0(SI64)]] + 56(SI64)))))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1126,16)  (*((P64*) comm_diffuse_comp_mod_mp_info_pre_$55_V$398a))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1136,13) UPDATE_STATUS
  -> INDIRECT-: (1139,13)  (*((P64*) comm_diffuse_comp_mod_mp_applydiffprecond_pseudoinv_$INVN_X$84_V$55b7.0.14))[4(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1140,13) UPDATE_STATUS
  -> INDIRECT-: (1141,13)  (*((P64*) *((P64*) (&((COMM_DATA_SET$N$26_V$1145 *)comm_data_mod_mp_data__V$395e)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_applydiffprecond_pseudoinv_$K_V$55c3.0.14)), l:comm_data_mod_mp_data__V$3967[0(SI32), l:0(SI64)]] + 56(SI64)))))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1142,13) UPDATE_STATUS
  -> INDIRECT-: (1143,13)  (*((P64*) comm_diffuse_comp_mod_mp_applydiffprecond_pseudoinv_$INVN_X$84_V$55b7.0.14))[3(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1144,13) UPDATE_STATUS
  -> INDIRECT-: (1153,16)  (*((P64*) *((P64*) (&((COMM_DATA_SET$INFO$20_V$10d9 *)comm_data_mod_mp_data__V$395e)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_applydiffprecond_pseudoinv_$K_V$55c3.0.14)), l:comm_data_mod_mp_data__V$3967[0(SI32), l:0(SI64)]] + 56(SI64)))))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1155,16)  (*((P64*) comm_diffuse_comp_mod_mp_info_pre_$55_V$398a))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1165,13) UPDATE_STATUS
  -> INDIRECT-: (1167,13)  (*((P64*) comm_diffuse_comp_mod_mp_applydiffprecond_pseudoinv_$INVN_X$84_V$55b7.0.14))[12(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1171,10) UPDATE_STATUS
  -> EXTERN: (1172,10) wall_time_
  -> EXTERN: (1174,5) for_alloc_allocatable
  -> EXTERN: (1174,5) for_check_mult_overflow64
  -> EXTERN: (1174,5) for_alloc_allocatable
  -> EXTERN: (1174,5) for_check_mult_overflow64
  -> EXTERN: (1181,11) for_realloc_lhs
  -> EXTERN: (1197,5) for_dealloc_allocatable
  -> EXTERN: (1197,5) for_dealloc_allocatable
  -> EXTERN: (1200,10) wall_time_
  -> EXTERN: (1202,10) UPDATE_STATUS
  -> EXTERN: (1208,8) for_check_mult_overflow64
  -> EXTERN: (1208,8) for_alloc_allocatable
  -> INDIRECT-: (1211,16)  (*((P64*) *((P64*) (&(((COMM_DIFFUSE_COMP_MOD$.btDIFF_PTR$P$50_V$10be *)comm_diffuse_comp_mod_mp_diffcomps_$56_V$39b4)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_applydiffprecond_pseudoinv_$II_V$55c2.0.14)), l:comm_diffuse_comp_mod_mp_diffcomps_$56_V$39bd[0(SI32), l:0(SI64)]]->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$X$43_V$ec6)->COMM_MAP$INFO$13_V$6d1 + 56(SI64)))))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1212,16)  (*((P64*) comm_diffuse_comp_mod_mp_info_pre_$55_V$398a))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1215,13) CR_INSERT_COMP_2D
  -> EXTERN: (1216,8) for_dealloc_allocatable
  -> EXTERN: (1218,10) UPDATE_STATUS
  -> EXTERN: (1220,5) for_dealloc_allocatable
  -> EXTERN: (1220,5) for_dealloc_allocatable
  -> EXTERN: (1222,3) for_dealloc_allocatable
  -> EXTERN: (1222,3) for_dealloc_allocatable
  -> EXTERN: (1222,3) for_dealloc_allocatable
  -> EXTERN: (1222,3) for_dealloc_allocatable
  -> EXTERN: (1222,3) for_dealloc_allocatable


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(1100,5)
   remark #25096: Loop Interchange not done due to: Imperfect Loop Nest (Either at Source or due to other Compiler Transformations)
   remark #25452: Original Order found to be proper, but by a close margin
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1100,5)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1100,5)
         remark #25408: memset generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between Y(:,:,:) (1100:5) and Y(:,:,:) (1100:5)
         remark #17106: parallel dependence: assumed OUTPUT dependence between Y(:,:,:) (1100:5) and Y(:,:,:) (1100:5)
         remark #15542: loop was not vectorized: inner loop was already vectorized
         remark #25015: Estimate of max trip count of loop=1

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1100,5)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15389: vectorization support: reference Y(:,:,:) has unaligned access
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 2
            remark #15309: vectorization support: normalized vectorization overhead 0.300
            remark #15300: LOOP WAS VECTORIZED
            remark #15451: unmasked unaligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 2.500 
            remark #15478: estimated potential speedup: 1.450 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=3
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1100,5)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=12
         LOOP END
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1101,5)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1108,11)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1108,11)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15329: vectorization support: non-unit strided store was emulated for the variable <Y(i,k,:)>, stride is unknown to compiler
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <ALM(j,:)>, stride is unknown to compiler
         remark #15305: vectorization support: vector length 2
         remark #15452: unmasked strided loads: 1 
         remark #15453: unmasked strided stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 4.000 
         remark #15478: estimated potential speedup: 1.000 
         remark #15488: --- end vector cost summary ---
         remark #25439: unrolled with remainder by 2  
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1108,11)
      <Remainder>
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1114,5)
   remark #25096: Loop Interchange not done due to: Imperfect Loop Nest (Either at Source or due to other Compiler Transformations)
   remark #25452: Original Order found to be proper, but by a close margin
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1114,5)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1114,5)
         remark #25408: memset generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between Z(:,:,:) (1114:5) and Z(:,:,:) (1114:5)
         remark #17106: parallel dependence: assumed OUTPUT dependence between Z(:,:,:) (1114:5) and Z(:,:,:) (1114:5)
         remark #15542: loop was not vectorized: inner loop was already vectorized
         remark #25015: Estimate of max trip count of loop=1

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1114,5)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15389: vectorization support: reference Z(:,:,:) has unaligned access
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 2
            remark #15309: vectorization support: normalized vectorization overhead 0.300
            remark #15300: LOOP WAS VECTORIZED
            remark #15451: unmasked unaligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 2.500 
            remark #15478: estimated potential speedup: 1.450 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=3
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1114,5)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=12
         LOOP END
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1115,5)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1123,8)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1130,53)
         remark #25096: Loop Interchange not done due to: Imperfect Loop Nest (Either at Source or due to other Compiler Transformations)
         remark #25451: Advice: Loop Interchange, if possible, might help loopnest. Suggested Permutation : ( 1 2 ) --> ( 2 1 ) 
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between INVN(i,p) (1130:17) and INVN(i,p) (1130:17)
         remark #17106: parallel dependence: assumed OUTPUT dependence between INVN(i,p) (1130:17) and INVN(i,p) (1130:17)
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed OUTPUT dependence between INVN(i,p) (1130:17) and INVN(i,p) (1130:17)
         remark #15346: vector dependence: assumed OUTPUT dependence between INVN(i,p) (1130:17) and INVN(i,p) (1130:17)

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1129,14)
            remark #17104: loop was not parallelized: existence of parallel dependence
            remark #17106: parallel dependence: assumed FLOW dependence between INVN(i,p) (1130:17) and p_cr(l,p,qq,k) (1130:17)
            remark #17106: parallel dependence: assumed ANTI dependence between p_cr(l,p,qq,k) (1130:17) and INVN(i,p) (1130:17)
            remark #15344: loop was not vectorized: vector dependence prevents vectorization
            remark #15346: vector dependence: assumed FLOW dependence between INVN(i,p) (1130:17) and p_cr(l,p,qq,k) (1130:17)
            remark #15346: vector dependence: assumed ANTI dependence between p_cr(l,p,qq,k) (1130:17) and INVN(i,p) (1130:17)
         LOOP END
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1146,47)
      remark #25084: Preprocess Loopnests: Moving Out Store    [ comm_diffuse_comp_mod.f90(1146,11) ]
      remark #25084: Preprocess Loopnests: Moving Out Store    [ comm_diffuse_comp_mod.f90(1147,8) ]
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between data_(k,i) (1146:47) and INVN(:,i) (1146:11)
      remark #17106: parallel dependence: assumed FLOW dependence between INVN(:,i) (1146:11) and data_(k,i) (1146:47)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1146,11)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15389: vectorization support: reference INVN(:,i) has unaligned access
         remark #15389: vectorization support: reference INVN(:,i) has unaligned access
         remark #15381: vectorization support: unaligned access used inside loop body
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 4
         remark #15309: vectorization support: normalized vectorization overhead 0.231
         remark #15300: LOOP WAS VECTORIZED
         remark #15450: unmasked unaligned unit stride loads: 1 
         remark #15451: unmasked unaligned unit stride stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 12 
         remark #15477: vector cost: 6.500 
         remark #15478: estimated potential speedup: 1.810 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1146,11)
      <Remainder loop for vectorization>
         remark #15389: vectorization support: reference INVN(:,i) has unaligned access
         remark #15389: vectorization support: reference INVN(:,i) has unaligned access
         remark #15381: vectorization support: unaligned access used inside loop body
         remark #15335: remainder loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 1.294
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1152,8)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1159,39)
         remark #25096: Loop Interchange not done due to: Imperfect Loop Nest (Either at Source or due to other Compiler Transformations)
         remark #25451: Advice: Loop Interchange, if possible, might help loopnest. Suggested Permutation : ( 1 2 ) --> ( 2 1 ) 
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed FLOW dependence between Z(q,j,p) (1159:17) and INVN(i,p) (1159:17)
         remark #17106: parallel dependence: assumed ANTI dependence between INVN(i,p) (1159:17) and Z(q,j,p) (1159:17)
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed FLOW dependence between Z(q,j,p) (1159:17) and INVN(i,p) (1159:17)
         remark #15346: vector dependence: assumed ANTI dependence between INVN(i,p) (1159:17) and Z(q,j,p) (1159:17)

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1158,14)
            remark #17104: loop was not parallelized: existence of parallel dependence
            remark #17106: parallel dependence: assumed FLOW dependence between Z(q,j,p) (1159:17) and INVN(i,p) (1159:17)
            remark #17106: parallel dependence: assumed ANTI dependence between INVN(i,p) (1159:17) and Z(q,j,p) (1159:17)
            remark #15344: loop was not vectorized: vector dependence prevents vectorization
            remark #15346: vector dependence: assumed FLOW dependence between Z(q,j,p) (1159:17) and INVN(i,p) (1159:17)
            remark #15346: vector dependence: assumed ANTI dependence between INVN(i,p) (1159:17) and Z(q,j,p) (1159:17)
         LOOP END
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1176,5)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1177,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between info_pre (1179:11) and at (1181:11)
      remark #17106: parallel dependence: assumed FLOW dependence between at (1181:11) and info_pre (1179:11)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1181,11)
         remark #25399: memcopy generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between W(:) (1181:11) and W(:) (1181:11)
         remark #17106: parallel dependence: assumed OUTPUT dependence between W(:) (1181:11) and W(:) (1181:11)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1181,11)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference W(:) has aligned access
            remark #15389: vectorization support: reference Y(:,i,p) has unaligned access
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15305: vectorization support: vector length 2
            remark #15309: vectorization support: normalized vectorization overhead 0.750
            remark #15300: LOOP WAS VECTORIZED
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15450: unmasked unaligned unit stride loads: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 2.000 
            remark #15478: estimated potential speedup: 1.770 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=6
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1181,11)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=12
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1182,11)
         remark #25408: memset generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between W2(:) (1182:11) and W2(:) (1182:11)
         remark #17106: parallel dependence: assumed OUTPUT dependence between W2(:) (1182:11) and W2(:) (1182:11)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1182,11)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference W2(:) has aligned access
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 2
            remark #15300: LOOP WAS VECTORIZED
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 1.500 
            remark #15478: estimated potential speedup: 2.660 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=3
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1182,11)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=12
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1183,11)
      <Distributed chunk1, Predicate Optimized v1>
         remark #25426: Loop Distributed (3 way) 
         remark #25096: Loop Interchange not done due to: Imperfect Loop Nest (Either at Source or due to other Compiler Transformations)
         remark #25452: Original Order found to be proper, but by a close margin
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between k (1184:14) and k (1186:14)
         remark #17106: parallel dependence: assumed OUTPUT dependence between k (1186:14) and k (1184:14)
         remark #25422: Invariant Condition at line 1184 hoisted out of this loop
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed OUTPUT dependence between k (1184:14) and k (1186:14)
         remark #15346: vector dependence: assumed OUTPUT dependence between k (1186:14) and k (1184:14)
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1183,11)
      <Distributed chunk1, Predicate Optimized v4>
         remark #15305: vectorization support: vector length 4
         remark #15309: vectorization support: normalized vectorization overhead 0.333
         remark #15301: PARTIAL LOOP WAS VECTORIZED
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 2 
         remark #15477: vector cost: 1.500 
         remark #15478: estimated potential speedup: 1.320 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1183,11)
      <Remainder loop for vectorization, Distributed chunk1, Predicate Optimized v4>
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1183,11)
      <Distributed chunk2>
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1184,14)
         <Peeled loop for vectorization, Distributed chunk1>
            remark #25015: Estimate of max trip count of loop=1
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1184,14)
         <Distributed chunk1>
            remark #25426: Loop Distributed (2 way) 
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15389: vectorization support: reference ind_pre_(k) has unaligned access   [ comm_diffuse_comp_mod.f90(1185,33) ]
            remark #15388: vectorization support: reference W(k) has aligned access   [ comm_diffuse_comp_mod.f90(1185,86) ]
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15328: vectorization support: irregularly indexed load was emulated for the variable <p_cr(l,p,ind_pre_(k),numband+ind_pre_(j))>, part of index is read from memory   [ comm_diffuse_comp_mod.f90(1185,33) ]
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 4
            remark #15309: vectorization support: normalized vectorization overhead 0.250
            remark #15301: PARTIAL LOOP WAS VECTORIZED
            remark #15448: unmasked aligned unit stride loads: 1 
            remark #15450: unmasked unaligned unit stride loads: 1 
            remark #15462: unmasked indexed (or gather) loads: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 16 
            remark #15477: vector cost: 6.500 
            remark #15478: estimated potential speedup: 2.390 
            remark #15488: --- end vector cost summary ---
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1184,14)
         <Remainder loop for vectorization, Distributed chunk1>
            remark #15389: vectorization support: reference ind_pre_(k) has unaligned access   [ comm_diffuse_comp_mod.f90(1185,33) ]
            remark #15388: vectorization support: reference W(k) has aligned access   [ comm_diffuse_comp_mod.f90(1185,86) ]
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15305: vectorization support: vector length 2
            remark #15309: vectorization support: normalized vectorization overhead 1.882
            remark #15301: REMAINDER LOOP WAS VECTORIZED
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1184,14)
         <Remainder loop for vectorization, Distributed chunk1>
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1188,11)
         remark #25408: memset generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between W(:) (1188:11) and W(:) (1188:11)
         remark #17106: parallel dependence: assumed OUTPUT dependence between W(:) (1188:11) and W(:) (1188:11)
         remark #15542: loop was not vectorized: inner loop was already vectorized
         remark #25015: Estimate of max trip count of loop=1

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1188,11)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference W(:) has aligned access
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 2
            remark #15300: LOOP WAS VECTORIZED
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 1.500 
            remark #15478: estimated potential speedup: 2.660 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=3
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1188,11)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=12
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1189,11)
      <Distributed chunk1>
         remark #25426: Loop Distributed (2 way) 
         remark #25096: Loop Interchange not done due to: Imperfect Loop Nest (Either at Source or due to other Compiler Transformations)
         remark #25451: Advice: Loop Interchange, if possible, might help loopnest. Suggested Permutation : ( 1 2 ) --> ( 2 1 ) 
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between k (1190:14) and k (1192:14)
         remark #17106: parallel dependence: assumed OUTPUT dependence between k (1192:14) and k (1190:14)
         remark #25084: Preprocess Loopnests: Moving Out Store    [ comm_diffuse_comp_mod.f90(1190,14) ]
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed OUTPUT dependence between k (1190:14) and k (1192:14)
         remark #15346: vector dependence: assumed OUTPUT dependence between k (1192:14) and k (1190:14)

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1190,14)
            remark #25084: Preprocess Loopnests: Moving Out Store    [ comm_diffuse_comp_mod.f90(1192,14) ]
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15389: vectorization support: reference ind_pre_(k) has unaligned access   [ comm_diffuse_comp_mod.f90(1191,31) ]
            remark #15388: vectorization support: reference W2(k) has aligned access   [ comm_diffuse_comp_mod.f90(1191,84) ]
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
            remark #15328: vectorization support: irregularly indexed load was emulated for the variable <p_cr(l,p,ind_pre_(j),numband+ind_pre_(k))>, 64-bit indexed, part of index is read from memory   [ comm_diffuse_comp_mod.f90(1191,31) ]
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 4
            remark #15309: vectorization support: normalized vectorization overhead 0.203
            remark #15448: unmasked aligned unit stride loads: 1 
            remark #15450: unmasked unaligned unit stride loads: 1 
            remark #15462: unmasked indexed (or gather) loads: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 16 
            remark #15477: vector cost: 16.000 
            remark #15478: estimated potential speedup: 0.990 
            remark #15487: type converts: 1 
            remark #15488: --- end vector cost summary ---
            remark #25439: unrolled with remainder by 2  
            remark #25456: Number of Array Refs Scalar Replaced In Loop: 4
            remark #25457: Number of partial sums replaced: 1
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1190,14)
         <Remainder>
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1194,11)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15389: vectorization support: reference Z(:,i,p) has unaligned access
         remark #15388: vectorization support: reference W(:) has aligned access
         remark #15389: vectorization support: reference Z(:,i,p) has unaligned access
         remark #15381: vectorization support: unaligned access used inside loop body
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 4
         remark #15309: vectorization support: normalized vectorization overhead 0.188
         remark #15300: LOOP WAS VECTORIZED
         remark #15448: unmasked aligned unit stride loads: 1 
         remark #15450: unmasked unaligned unit stride loads: 1 
         remark #15451: unmasked unaligned unit stride stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 8 
         remark #15477: vector cost: 4.000 
         remark #15478: estimated potential speedup: 1.960 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1194,11)
      <Remainder loop for vectorization>
         remark #15389: vectorization support: reference Z(:,i,p) has unaligned access
         remark #15388: vectorization support: reference W(:) has aligned access
         remark #15389: vectorization support: reference Z(:,i,p) has unaligned access
         remark #15381: vectorization support: unaligned access used inside loop body
         remark #15335: remainder loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 1.143
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1208,8)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1209,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between ALM(:,:) (1209:8) and ALM(:,:) (1209:8)
      remark #17106: parallel dependence: assumed OUTPUT dependence between ALM(:,:) (1209:8) and ALM(:,:) (1209:8)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1209,8)
         remark #25408: memset generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between ALM(:,:) (1209:8) and ALM(:,:) (1209:8)
         remark #17106: parallel dependence: assumed OUTPUT dependence between ALM(:,:) (1209:8) and ALM(:,:) (1209:8)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1209,8)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15389: vectorization support: reference ALM(:,:) has unaligned access
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 2
            remark #15309: vectorization support: normalized vectorization overhead 0.300
            remark #15300: LOOP WAS VECTORIZED
            remark #15451: unmasked unaligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 2.500 
            remark #15478: estimated potential speedup: 1.450 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=3
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1209,8)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=12
         LOOP END
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1213,11)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1213,11)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15329: vectorization support: non-unit strided store was emulated for the variable <ALM(j,:)>, stride is unknown to compiler
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <Z(i,k,:)>, stride is unknown to compiler
         remark #15305: vectorization support: vector length 2
         remark #15452: unmasked strided loads: 1 
         remark #15453: unmasked strided stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 4.000 
         remark #15478: estimated potential speedup: 1.000 
         remark #15488: --- end vector cost summary ---
         remark #25439: unrolled with remainder by 2  
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1213,11)
      <Remainder>
      LOOP END
   LOOP END
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(1088,51):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1086,51):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1088,48):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1086,48):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1087,48):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1100,5):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1100,5):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1114,5):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1114,5):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1181,11):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1181,11):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1181,11):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1182,11):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1182,11):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1188,11):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1188,11):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1209,8):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1209,8):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1080,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_applydiffprecond_pseudoinv_] comm_diffuse_comp_mod.f90:1080

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   25[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm10]
        
    Routine temporaries
        Total         :    1515
            Global    :     333
            Local     :    1182
        Regenerable   :     193
        Spilled       :      90
        
    Routine stack
        Variables     :     884 bytes*
            Reads     :     137 [3.81e+02 ~ 1.2%]
            Writes    :     164 [2.33e+02 ~ 0.7%]
        Spills        :     672 bytes*
            Reads     :     155 [2.10e+03 ~ 6.6%]
            Writes    :     106 [5.43e+02 ~ 1.7%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::ADD_TO_NPRE

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::ADD_TO_NPRE) [11/20=55.0%] comm_diffuse_comp_mod.f90(1420,14)


    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(1420,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_add_to_npre_] comm_diffuse_comp_mod.f90:1420

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :    9[ rax rdx rcx rsi rdi r8-r11]
        
    Routine temporaries
        Total         :      24
            Global    :       0
            Local     :      24
        Regenerable   :       0
        Spilled       :       0
        
    Routine stack
        Variables     :       0 bytes*
            Reads     :       0 [0.00e+00 ~ 0.0%]
            Writes    :       0 [0.00e+00 ~ 0.0%]
        Spills        :       0 bytes*
            Reads     :       0 [0.00e+00 ~ 0.0%]
            Writes    :       0 [0.00e+00 ~ 0.0%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::SAMPLEDIFFUSESPECIND

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::SAMPLEDIFFUSESPECIND) [12/20=60.0%] comm_diffuse_comp_mod.f90(1430,14)
  -> INDIRECT-: (1454,13)  (*((P64*) comm_diffuse_comp_mod_mp_samplediffusespecind_$C$86_V$5b02.0.18))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1456,18) strcmp
  -> EXTERN: (1467,9) for_trim
  -> EXTERN: (1467,25) for_cpstr
  -> EXTERN: (1468,8) for_check_mult_overflow64
  -> EXTERN: (1468,8) for_alloc_allocatable
  -> EXTERN: (1469,8) for_realloc_lhs
  -> EXTERN: (1475,11) for_alloc_allocatable
  -> EXTERN: (1475,11) for_check_mult_overflow64
  -> EXTERN: (1475,11) for_alloc_allocatable
  -> EXTERN: (1475,11) for_check_mult_overflow64
  -> EXTERN: (1482,14) for_realloc_lhs
  -> EXTERN: (1487,19) POWELL
  -> EXTERN: (1505,11) for_dealloc_allocatable
  -> EXTERN: (1505,11) for_dealloc_allocatable
  -> EXTERN: (1508,8) for_dealloc_allocatable
  -> EXTERN: (1508,8) for_realloc_lhs
  -> INDIRECT-: (1511,13)  (*((P64*) *((P64*) (comm_diffuse_comp_mod_mp_samplediffusespecind_$SELF_V$5ad4.0.18 + 56(SI64)))))[15(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1516,8) for_dealloc_allocatable
  -> EXTERN: (1521,3) for_dealloc_allocatable
  -> EXTERN: (1521,3) for_dealloc_allocatable
  -> EXTERN: (1521,3) for_dealloc_allocatable
  -> EXTERN: (1521,3) for_dealloc_allocatable
  -> EXTERN: (1521,3) for_dealloc_allocatable
  -> EXTERN: (1521,3) for_dealloc_allocatable
  -> EXTERN: (1521,3) for_dealloc_allocatable


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(1453,5)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1456,18)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1469,8)
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed FLOW dependence between BUFFER (1469:8) and BUFFER (1469:8)
   remark #15346: vector dependence: assumed ANTI dependence between BUFFER (1469:8) and BUFFER (1469:8)
   remark #15346: vector dependence: assumed ANTI dependence between BUFFER (1469:8) and BUFFER (1469:8)
   remark #15346: vector dependence: assumed FLOW dependence between BUFFER (1469:8) and BUFFER (1469:8)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1469,8)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15389: vectorization support: reference BUFFER(:,:) has unaligned access
      remark #15389: vectorization support: reference at (1469:8) has unaligned access
      remark #15381: vectorization support: unaligned access used inside loop body
      remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
      remark #15305: vectorization support: vector length 2
      remark #15399: vectorization support: unroll factor set to 4
      remark #15309: vectorization support: normalized vectorization overhead 0.074
      remark #15450: unmasked unaligned unit stride loads: 1 
      remark #15451: unmasked unaligned unit stride stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 15 
      remark #15477: vector cost: 17.000 
      remark #15478: estimated potential speedup: 0.880 
      remark #15488: --- end vector cost summary ---
      remark #25439: unrolled with remainder by 8  
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1469,8)
   <Remainder>
      remark #25436: completely unrolled by 7  
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1470,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between self (1471:51) and x (1487:19)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1482,14)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed FLOW dependence between x(1) (1481:14) and x(1) (1499:20)
      remark #17106: parallel dependence: assumed ANTI dependence between x(1) (1499:20) and x(1) (1481:14)
      remark #17106: parallel dependence: assumed OUTPUT dependence between k_lnl (1480:14) and k_lnl (1480:14)
      remark #17106: parallel dependence: assumed OUTPUT dependence between k_lnl (1480:14) and k_lnl (1480:14)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1482,14)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15388: vectorization support: reference a_lnl_(:) has aligned access
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <at (1482:14)>, stride is unknown to compiler
         remark #15305: vectorization support: vector length 2
         remark #15300: LOOP WAS VECTORIZED
         remark #15449: unmasked aligned unit stride stores: 1 
         remark #15452: unmasked strided loads: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 2.500 
         remark #15478: estimated potential speedup: 1.600 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1482,14)
      <Remainder loop for vectorization>
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1483,14)
      <Predicate Optimized v1>
         remark #17108: loop was not parallelized: insufficient computational work
         remark #25423: Condition at line 1484 hoisted out of this loop
         remark #15389: vectorization support: reference theta_lnl_(i) has unaligned access   [ comm_diffuse_comp_mod.f90(1485,17) ]
         remark #15381: vectorization support: unaligned access used inside loop body
         remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <at (1485:17)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(1485,17) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <at (1485:32)>, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(1485,32) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <at (1485:17)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(1485,17) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <at (1485:32)>, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(1485,32) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <at (1485:17)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(1485,17) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <at (1485:32)>, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(1485,32) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <at (1485:17)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(1485,17) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <at (1485:32)>, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(1485,32) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <at (1485:17)>, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(1485,17) ]
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 0.091
         remark #15451: unmasked unaligned unit stride stores: 1 
         remark #15452: unmasked strided loads: 4 
         remark #15462: unmasked indexed (or gather) loads: 5 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 8 
         remark #15477: vector cost: 71.500 
         remark #15478: estimated potential speedup: 0.110 
         remark #15488: --- end vector cost summary ---
         remark #25439: unrolled with remainder by 2  
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1483,14)
      <Remainder, Predicate Optimized v1>
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1483,14)
      <Predicate Optimized v2>
         remark #15389: vectorization support: reference theta_lnl_(i) has unaligned access   [ comm_diffuse_comp_mod.f90(1485,17) ]
         remark #15381: vectorization support: unaligned access used inside loop body
         remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <at (1485:17)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(1485,17) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <at (1485:32)>, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(1485,32) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <at (1485:17)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(1485,17) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <at (1485:32)>, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(1485,32) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <at (1485:17)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(1485,17) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <at (1485:32)>, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(1485,32) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <at (1485:17)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(1485,17) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <at (1485:32)>, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(1485,32) ]
         remark #15328: vectorization support: indirect load was emulated for the variable <at (1485:17)>, 64-bit indexed, part of address is read from memory   [ comm_diffuse_comp_mod.f90(1485,17) ]
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 0.091
         remark #15451: unmasked unaligned unit stride stores: 1 
         remark #15452: unmasked strided loads: 4 
         remark #15462: unmasked indexed (or gather) loads: 5 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 8 
         remark #15477: vector cost: 71.500 
         remark #15478: estimated potential speedup: 0.110 
         remark #15488: --- end vector cost summary ---
         remark #25439: unrolled with remainder by 2  
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1483,14)
      <Remainder, Predicate Optimized v2>
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1491,20)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15329: vectorization support: non-unit strided store was emulated for the variable <BUFFER(k,:)>, stride is unknown to compiler
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 4
         remark #15309: vectorization support: normalized vectorization overhead 0.083
         remark #15300: LOOP WAS VECTORIZED
         remark #15453: unmasked strided stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 3.000 
         remark #15478: estimated potential speedup: 1.320 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1491,20)
      <Remainder loop for vectorization>
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1496,23)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15329: vectorization support: non-unit strided store was emulated for the variable <BUFFER(k,:)>, stride is unknown to compiler
         remark #15305: vectorization support: vector length 2
         remark #15427: loop was completely unrolled
         remark #15309: vectorization support: normalized vectorization overhead 0.333
         remark #15453: unmasked strided stores: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 4 
         remark #15477: vector cost: 3.000 
         remark #15478: estimated potential speedup: 1.000 
         remark #15488: --- end vector cost summary ---
         remark #25436: completely unrolled by 2  
      LOOP END
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1508,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between at (1508:8) and at (1508:8)
   remark #17106: parallel dependence: assumed FLOW dependence between at (1508:8) and at (1508:8)
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between at (1508:8) and at (1508:8)
   remark #15346: vector dependence: assumed FLOW dependence between at (1508:8) and at (1508:8)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1508,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed FLOW dependence between at (1508:8) and at (1508:8)
      remark #17106: parallel dependence: assumed ANTI dependence between at (1508:8) and at (1508:8)
      remark #15344: loop was not vectorized: vector dependence prevents vectorization
      remark #15346: vector dependence: assumed FLOW dependence between at (1508:8) and at (1508:8)
      remark #15346: vector dependence: assumed ANTI dependence between at (1508:8) and at (1508:8)
      remark #25439: unrolled with remainder by 2  
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1508,8)
   <Remainder>
   LOOP END
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(1443,50):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1442,72):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1442,55):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1442,50):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1443,55):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1442,62):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1442,65):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1452,5):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to increase the width of loads
comm_diffuse_comp_mod.f90(1452,5):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (8, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1456,18):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (16, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1458,8):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (16, 0), and destination (alignment, offset): (32, 0)
comm_diffuse_comp_mod.f90(1430,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_samplediffusespecind_] comm_diffuse_comp_mod.f90:1430

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   24[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm9]
        
    Routine temporaries
        Total         :    1163
            Global    :     196
            Local     :     967
        Regenerable   :      87
        Spilled       :      58
        
    Routine stack
        Variables     :    1108 bytes*
            Reads     :      90 [7.47e+01 ~ 13.5%]
            Writes    :     128 [1.72e+02 ~ 31.0%]
        Spills        :     392 bytes*
            Reads     :     103 [3.77e+00 ~ 0.7%]
            Writes    :      73 [5.15e+00 ~ 0.9%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::LNL_DIFFUSE_MULTI

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::LNL_DIFFUSE_MULTI) [13/20=65.0%] comm_diffuse_comp_mod.f90(1524,12)
  -> EXTERN: (1534,5) for_alloc_allocatable
  -> EXTERN: (1534,5) for_check_mult_overflow64
  -> EXTERN: (1535,5) for_realloc_lhs
  -> EXTERN: (1535,5) for_dealloc_allocatable
  -> EXTERN: (1542,11) for_dealloc_allocatable
  -> INDIRECT-: (1579,26)  (*((P64*) *((P64*) (&((F_INT_PTR$P$29_V$1070 *)comm_diffuse_comp_mod_mp_c_lnl_$54_V$3970->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$1094)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_lnl_diffuse_multi_$L_V$5e04.0.19)), l:(comm_diffuse_comp_mod_mp_c_lnl_$54_V$3970->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_INT_V$109d)[0(SI32), l:0(SI64)]] + 56(SI64)))))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1623,5) for_dealloc_allocatable
  -> EXTERN: (1625,3) for_dealloc_allocatable


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(1535,5)
   remark #25399: memcopy generated
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between THETA(:) (1535:5) and THETA(:) (1535:5)
   remark #17106: parallel dependence: assumed OUTPUT dependence between THETA(:) (1535:5) and THETA(:) (1535:5)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1535,5)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15388: vectorization support: reference THETA(:) has aligned access
      remark #15388: vectorization support: reference theta_lnl_(:) has aligned access
      remark #15305: vectorization support: vector length 2
      remark #15300: LOOP WAS VECTORIZED
      remark #15448: unmasked aligned unit stride loads: 1 
      remark #15449: unmasked aligned unit stride stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 4 
      remark #15477: vector cost: 1.500 
      remark #15478: estimated potential speedup: 2.660 
      remark #15488: --- end vector cost summary ---
      remark #25015: Estimate of max trip count of loop=6
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1535,5)
   <Remainder loop for vectorization>
      remark #25015: Estimate of max trip count of loop=12
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1540,40)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15523: loop was not vectorized: loop control variable l was found, but loop iteration count cannot be computed before executing the loop
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1571,5)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between rms_smooth_(l) (1575:21) and theta (1579:26)
   remark #17106: parallel dependence: assumed FLOW dependence between theta (1579:26) and rms_smooth_(l) (1575:21)
   remark #15382: vectorization support: call to function (Indirect call) cannot be vectorized   [ comm_diffuse_comp_mod.f90(1579,26) ]
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between rms_smooth_(l) (1575:21) and theta (1579:26)
   remark #15346: vector dependence: assumed FLOW dependence between theta (1579:26) and rms_smooth_(l) (1575:21)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1573,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between rms_smooth_(l) (1575:21) and theta (1579:26)
      remark #17106: parallel dependence: assumed FLOW dependence between theta (1579:26) and rms_smooth_(l) (1575:21)
      remark #15382: vectorization support: call to function (Indirect call) cannot be vectorized   [ comm_diffuse_comp_mod.f90(1579,26) ]
      remark #15344: loop was not vectorized: vector dependence prevents vectorization
      remark #15346: vector dependence: assumed ANTI dependence between rms_smooth_(l) (1575:21) and theta (1579:26)
      remark #15346: vector dependence: assumed FLOW dependence between theta (1579:26) and rms_smooth_(l) (1575:21)
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1600,5)
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15389: vectorization support: reference THETA(l) has unaligned access   [ comm_diffuse_comp_mod.f90(1602,32) ]
   remark #15381: vectorization support: unaligned access used inside loop body
   remark #15335: loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
   remark #15328: vectorization support: non-unit strided load was emulated for the variable <c_lnl(2,l)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(1601,12) ]
   remark #15328: vectorization support: non-unit strided load was emulated for the variable <c_lnl(1,l)>, masked, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(1602,41) ]
   remark #15328: vectorization support: non-unit strided load was emulated for the variable <c_lnl(2,l)>, stride is unknown to compiler   [ comm_diffuse_comp_mod.f90(1602,84) ]
   remark #15305: vectorization support: vector length 2
   remark #15309: vectorization support: normalized vectorization overhead 0.183
   remark #15452: unmasked strided loads: 2 
   remark #15456: masked unaligned unit stride loads: 1 
   remark #15460: masked strided loads: 1 
   remark #15475: --- begin vector cost summary ---
   remark #15476: scalar cost: 52 
   remark #15477: vector cost: 63.000 
   remark #15478: estimated potential speedup: 0.820 
   remark #15486: divides: 1 
   remark #15488: --- end vector cost summary ---
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(1532,44):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1535,5):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1535,5):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1535,5):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1524,12):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_lnl_diffuse_multi_] comm_diffuse_comp_mod.f90:1524

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   19[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm4]
        
    Routine temporaries
        Total         :     321
            Global    :      63
            Local     :     258
        Regenerable   :      30
        Spilled       :       8
        
    Routine stack
        Variables     :      80 bytes*
            Reads     :      14 [7.61e+00 ~ 0.8%]
            Writes    :      15 [1.30e+01 ~ 1.3%]
        Spills        :      24 bytes*
            Reads     :       6 [3.39e+01 ~ 3.4%]
            Writes    :       6 [1.48e+01 ~ 1.5%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::PRINT_PRECOND_MAT

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::PRINT_PRECOND_MAT) [14/20=70.0%] comm_diffuse_comp_mod.f90(1791,14)
  -> EXTERN: (1801,5) __resetsp_inlined
  -> EXTERN: (1801,5) for_open
  -> EXTERN: (1801,5) __getsp_inlined
  -> EXTERN: (1801,18) for_trim
  -> EXTERN: (1801,30) for_concat
  -> EXTERN: (1801,30) _alloca
  -> EXTERN: (1802,9) for_trim
  -> EXTERN: (1802,28) for_cpstr
  -> INDIRECT-: (1804,16)  (*((P64*) comm_diffuse_comp_mod_mp_info_pre_$55_V$398a))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1805,11) for_write_seq_lis
  -> EXTERN: (1806,11) for_write_seq_lis
  -> EXTERN: (1808,14) for_write_seq_lis
  -> EXTERN: (1808,14) _alloca
  -> EXTERN: (1808,14) __getsp_inlined
  -> EXTERN: (1808,14) __resetsp_inlined
  -> EXTERN: (1810,11) for_check_mult_overflow64
  -> EXTERN: (1810,11) for_alloc_allocatable
  -> EXTERN: (1811,16) GET_EIGENVALUES
  -> EXTERN: (1812,11) __getsp_inlined
  -> EXTERN: (1812,11) for_write_seq_lis
  -> EXTERN: (1812,11) _alloca
  -> EXTERN: (1812,11) for_write_seq_lis_xmit
  -> EXTERN: (1812,11) __resetsp_inlined
  -> EXTERN: (1813,11) for_dealloc_allocatable
  -> EXTERN: (1816,8) for_alloc_allocatable
  -> EXTERN: (1816,8) for_check_mult_overflow64
  -> EXTERN: (1818,11) for_realloc_lhs
  -> EXTERN: (1818,11) __resetsp_inlined
  -> EXTERN: (1818,11) __getsp_inlined
  -> EXTERN: (1818,17) _alloca
  -> EXTERN: (1819,11) for_write_seq_lis
  -> EXTERN: (1820,11) for_write_seq_lis
  -> EXTERN: (1822,14) for_write_seq_lis
  -> EXTERN: (1822,14) _alloca
  -> EXTERN: (1822,14) __getsp_inlined
  -> EXTERN: (1822,14) __resetsp_inlined
  -> EXTERN: (1824,11) for_check_mult_overflow64
  -> EXTERN: (1824,11) for_alloc_allocatable
  -> EXTERN: (1825,16) GET_EIGENVALUES
  -> EXTERN: (1826,11) __getsp_inlined
  -> EXTERN: (1826,11) for_write_seq_lis
  -> EXTERN: (1826,11) _alloca
  -> EXTERN: (1826,11) for_write_seq_lis_xmit
  -> EXTERN: (1826,11) __resetsp_inlined
  -> EXTERN: (1827,11) for_dealloc_allocatable
  -> EXTERN: (1829,8) for_dealloc_allocatable
  -> EXTERN: (1831,11) for_close
  -> EXTERN: (1834,3) for_dealloc_allocatable
  -> EXTERN: (1834,3) for_dealloc_allocatable


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(1803,8)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1807,11)
      remark #25096: Loop Interchange not done due to: Imperfect Loop Nest (Either at Source or due to other Compiler Transformations)
      remark #25451: Advice: Loop Interchange, if possible, might help loopnest. Suggested Permutation : ( 1 2 ) --> ( 2 1 ) 
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between at (1808:14) and at (1808:14)
      remark #17106: parallel dependence: assumed OUTPUT dependence between at (1808:14) and at (1808:14)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1808,14)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15388: vectorization support: reference at (1808:25) has aligned access   [ comm_diffuse_comp_mod.f90(1808,25) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <p_cr(i,2,j,:)>, stride is unknown to compiler
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 4
         remark #15418: vectorization support: number of FP down converts: double precision to single precision 1   [ comm_diffuse_comp_mod.f90(1808,25) ]
         remark #15300: LOOP WAS VECTORIZED
         remark #15451: unmasked unaligned unit stride stores: 1 
         remark #15452: unmasked strided loads: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 5 
         remark #15477: vector cost: 3.000 
         remark #15478: estimated potential speedup: 1.650 
         remark #15487: type converts: 1 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1808,14)
      <Remainder loop for vectorization>
         remark #15388: vectorization support: reference at (1808:25) has aligned access   [ comm_diffuse_comp_mod.f90(1808,25) ]
         remark #15335: remainder loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 1.300
         remark #15418: vectorization support: number of FP down converts: double precision to single precision 1   [ comm_diffuse_comp_mod.f90(1808,25) ]
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1812,11)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15388: vectorization support: reference at (1812:26) has aligned access   [ comm_diffuse_comp_mod.f90(1812,26) ]
      remark #15388: vectorization support: reference W(:) has aligned access
      remark #15305: vectorization support: vector length 4
      remark #15399: vectorization support: unroll factor set to 2
      remark #15418: vectorization support: number of FP down converts: double precision to single precision 1   [ comm_diffuse_comp_mod.f90(1812,26) ]
      remark #15300: LOOP WAS VECTORIZED
      remark #15448: unmasked aligned unit stride loads: 1 
      remark #15449: unmasked aligned unit stride stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 5 
      remark #15477: vector cost: 1.500 
      remark #15478: estimated potential speedup: 3.230 
      remark #15487: type converts: 1 
      remark #15488: --- end vector cost summary ---
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1812,11)
   <Remainder loop for vectorization>
      remark #15388: vectorization support: reference at (1812:26) has aligned access   [ comm_diffuse_comp_mod.f90(1812,26) ]
      remark #15388: vectorization support: reference W(:) has aligned access
      remark #15335: remainder loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
      remark #15305: vectorization support: vector length 2
      remark #15309: vectorization support: normalized vectorization overhead 1.250
      remark #15418: vectorization support: number of FP down converts: double precision to single precision 1   [ comm_diffuse_comp_mod.f90(1812,26) ]
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1817,8)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,17)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between at (1818:17) and at (1818:17)
      remark #17106: parallel dependence: assumed OUTPUT dependence between at (1818:17) and at (1818:17)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,17)
         remark #25408: memset generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between at (1818:17) and at (1818:17)
         remark #17106: parallel dependence: assumed OUTPUT dependence between at (1818:17) and at (1818:17)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,17)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15389: vectorization support: reference at (1818:17) has unaligned access
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 2
            remark #15309: vectorization support: normalized vectorization overhead 0.300
            remark #15300: LOOP WAS VECTORIZED
            remark #15451: unmasked unaligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 2.500 
            remark #15478: estimated potential speedup: 1.450 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=3
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,17)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=12
         LOOP END
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,17)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between at (1818:17) and at (1818:17)
      remark #17106: parallel dependence: assumed OUTPUT dependence between at (1818:17) and at (1818:17)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,17)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,17)
         <Peeled loop for vectorization>
            remark #25015: Estimate of max trip count of loop=1
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,17)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference at (1818:17) has aligned access
            remark #15388: vectorization support: reference at (1818:17) has aligned access
            remark #15388: vectorization support: reference p_cr(l,2,:,:) has aligned access
            remark #15305: vectorization support: vector length 2
            remark #15399: vectorization support: unroll factor set to 4
            remark #15309: vectorization support: normalized vectorization overhead 0.531
            remark #15300: LOOP WAS VECTORIZED
            remark #15448: unmasked aligned unit stride loads: 2 
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 9 
            remark #15477: vector cost: 4.000 
            remark #15478: estimated potential speedup: 2.180 
            remark #15488: --- end vector cost summary ---
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,17)
         <Alternate Alignment Vectorized Loop>
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,17)
         <Remainder loop for vectorization>
            remark #15388: vectorization support: reference at (1818:17) has aligned access
            remark #15388: vectorization support: reference at (1818:17) has aligned access
            remark #15389: vectorization support: reference p_cr(l,2,:,:) has unaligned access
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15335: remainder loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
            remark #15305: vectorization support: vector length 2
            remark #15309: vectorization support: normalized vectorization overhead 1.000
         LOOP END
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,11)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed OUTPUT dependence between MAT(:,:) (1818:11) and MAT(:,:) (1818:11)
      remark #17106: parallel dependence: assumed OUTPUT dependence between MAT(:,:) (1818:11) and MAT(:,:) (1818:11)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,11)
         remark #25399: memcopy generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between MAT(:,:) (1818:11) and MAT(:,:) (1818:11)
         remark #17106: parallel dependence: assumed OUTPUT dependence between MAT(:,:) (1818:11) and MAT(:,:) (1818:11)
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,11)
         <Peeled loop for vectorization>
            remark #25015: Estimate of max trip count of loop=1
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,11)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference MAT(:,:) has aligned access
            remark #15388: vectorization support: reference at (1818:11) has aligned access
            remark #15305: vectorization support: vector length 2
            remark #15309: vectorization support: normalized vectorization overhead 4.667
            remark #15300: LOOP WAS VECTORIZED
            remark #15442: entire loop may be executed in remainder
            remark #15448: unmasked aligned unit stride loads: 1 
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 1.500 
            remark #15478: estimated potential speedup: 1.290 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=6
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,11)
         <Alternate Alignment Vectorized Loop>
            remark #25015: Estimate of max trip count of loop=6
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(1818,11)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=12
         LOOP END
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1821,11)
      remark #25096: Loop Interchange not done due to: Imperfect Loop Nest (Either at Source or due to other Compiler Transformations)
      remark #25451: Advice: Loop Interchange, if possible, might help loopnest. Suggested Permutation : ( 1 2 ) --> ( 2 1 ) 
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between MAT(j,:) (1822:14) and at (1822:14)
      remark #17106: parallel dependence: assumed FLOW dependence between at (1822:14) and MAT(j,:) (1822:14)
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1822,14)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15388: vectorization support: reference at (1822:25) has aligned access   [ comm_diffuse_comp_mod.f90(1822,25) ]
         remark #15328: vectorization support: non-unit strided load was emulated for the variable <MAT(j,:)>, stride is unknown to compiler
         remark #15305: vectorization support: vector length 2
         remark #15399: vectorization support: unroll factor set to 4
         remark #15418: vectorization support: number of FP down converts: double precision to single precision 1   [ comm_diffuse_comp_mod.f90(1822,25) ]
         remark #15300: LOOP WAS VECTORIZED
         remark #15451: unmasked unaligned unit stride stores: 1 
         remark #15452: unmasked strided loads: 1 
         remark #15475: --- begin vector cost summary ---
         remark #15476: scalar cost: 5 
         remark #15477: vector cost: 3.000 
         remark #15478: estimated potential speedup: 1.650 
         remark #15487: type converts: 1 
         remark #15488: --- end vector cost summary ---
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(1822,14)
      <Remainder loop for vectorization>
         remark #15388: vectorization support: reference at (1822:25) has aligned access   [ comm_diffuse_comp_mod.f90(1822,25) ]
         remark #15335: remainder loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
         remark #15305: vectorization support: vector length 2
         remark #15309: vectorization support: normalized vectorization overhead 1.300
         remark #15418: vectorization support: number of FP down converts: double precision to single precision 1   [ comm_diffuse_comp_mod.f90(1822,25) ]
      LOOP END
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1826,11)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15388: vectorization support: reference at (1826:26) has aligned access   [ comm_diffuse_comp_mod.f90(1826,26) ]
      remark #15388: vectorization support: reference W(:) has aligned access
      remark #15305: vectorization support: vector length 4
      remark #15399: vectorization support: unroll factor set to 2
      remark #15418: vectorization support: number of FP down converts: double precision to single precision 1   [ comm_diffuse_comp_mod.f90(1826,26) ]
      remark #15300: LOOP WAS VECTORIZED
      remark #15448: unmasked aligned unit stride loads: 1 
      remark #15449: unmasked aligned unit stride stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 5 
      remark #15477: vector cost: 1.500 
      remark #15478: estimated potential speedup: 3.230 
      remark #15487: type converts: 1 
      remark #15488: --- end vector cost summary ---
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1826,11)
   <Remainder loop for vectorization>
      remark #15388: vectorization support: reference at (1826:26) has aligned access   [ comm_diffuse_comp_mod.f90(1826,26) ]
      remark #15388: vectorization support: reference W(:) has aligned access
      remark #15335: remainder loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
      remark #15305: vectorization support: vector length 2
      remark #15309: vectorization support: normalized vectorization overhead 1.250
      remark #15418: vectorization support: number of FP down converts: double precision to single precision 1   [ comm_diffuse_comp_mod.f90(1826,26) ]
   LOOP END
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(1795,46):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1796,46):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1818,17):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1818,17):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1818,11):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1818,11):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(1818,11):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(1791,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_print_precond_mat_] comm_diffuse_comp_mod.f90:1791

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   22[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm7]
        
    Routine temporaries
        Total         :     825
            Global    :     196
            Local     :     629
        Regenerable   :     173
        Spilled       :      32
        
    Routine stack
        Variables     :    1544 bytes*
            Reads     :      36 [1.45e+01 ~ 0.6%]
            Writes    :      93 [5.54e+01 ~ 2.3%]
        Spills        :     224 bytes*
            Reads     :      46 [5.95e+01 ~ 2.4%]
            Writes    :      36 [1.91e+01 ~ 0.8%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::INITDIFFUSEHDF

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::INITDIFFUSEHDF) [15/20=75.0%] comm_diffuse_comp_mod.f90(1380,14)
  -> EXTERN: (1391,9) for_trim
  -> EXTERN: (1391,25) for_cpstr
  -> EXTERN: (1392,13) READ_HDF_1D_DP
  -> EXTERN: (1392,13) __resetsp_inlined
  -> EXTERN: (1392,13) __getsp_inlined
  -> EXTERN: (1392,31) for_trim
  -> EXTERN: (1392,36) for_adjustl
  -> EXTERN: (1392,36) _alloca
  -> EXTERN: (1392,44) _alloca
  -> EXTERN: (1392,60) _alloca
  -> EXTERN: (1392,60) for_concat
  -> EXTERN: (1392,62) for_trim
  -> EXTERN: (1392,67) for_adjustl
  -> INDIRECT-: (1394,16)  (*((P64*) *((P64*) (&((comm_diffuse_comp_mod_mp_initdiffusehdf_$SELF_V$5a2f.0.16->QNCAtemplate.dim31_dv_template.addr_a0_V$5a59.0.16)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$X$43_V$ec6)->COMM_MAP$INFO$13_V$6d1 + 56(SI64)))))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1406,8) __resetsp_inlined
  -> EXTERN: (1406,8) for_concat
  -> EXTERN: (1406,8) __getsp_inlined
  -> EXTERN: (1406,15) for_trim
  -> EXTERN: (1406,20) _alloca
  -> EXTERN: (1406,20) for_adjustl
  -> EXTERN: (1406,28) _alloca
  -> EXTERN: (1406,39) for_trim
  -> EXTERN: (1406,44) for_adjustl
  -> EXTERN: (1407,13) __resetsp_inlined
  -> INDIRECT-: (1407,13)  (*((P64*) *((P64*) (&(comm_diffuse_comp_mod_mp_initdiffusehdf_$SELF_V$5a2f.0.16->QNCAtemplate.dim31_dv_template.addr_a0_V$5a59.0.16)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$X$43_V$ec4 + 56(SI64)))))[11(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1407,13) __getsp_inlined
  -> EXTERN: (1407,37) for_trim
  -> EXTERN: (1407,42) for_adjustl
  -> EXTERN: (1407,56) for_concat
  -> EXTERN: (1407,56) _alloca
  -> EXTERN: (1408,8) for_realloc_lhs
  -> INDIRECT-: (1410,16)  (*((P64*) *((P64*) (&((MAP_PTR$P$19_V$930 *)(comm_diffuse_comp_mod_mp_initdiffusehdf_$SELF_V$5a2f.0.16->QNCAtemplate.dim31_dv_template.addr_a0_V$5a59.0.16)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$THETA$48_V$fc9)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_initdiffusehdf_$I_V$5a46.0.16)), l:((comm_diffuse_comp_mod_mp_initdiffusehdf_$SELF_V$5a2f.0.16->QNCAtemplate.dim31_dv_template.addr_a0_V$5a59.0.16)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$THETA$48_V$fd2)[0(SI32), l:0(SI64)]] + 56(SI64)))))[11(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1410,16) __resetsp_inlined
  -> EXTERN: (1410,16) __getsp_inlined
  -> EXTERN: (1410,49) for_trim
  -> EXTERN: (1410,66) for_trim
  -> EXTERN: (1410,71) for_adjustl
  -> EXTERN: (1410,97) _alloca
  -> EXTERN: (1410,97) for_concat
  -> INDIRECT-: (1412,35)  (*((P64*) *((P64*) (&((MAP_PTR$P$19_V$930 *)(comm_diffuse_comp_mod_mp_initdiffusehdf_$SELF_V$5a2f.0.16->QNCAtemplate.dim31_dv_template.addr_a0_V$5a59.0.16)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$THETA$48_V$fc9)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_initdiffusehdf_$I_V$5a46.0.16)), l:((comm_diffuse_comp_mod_mp_initdiffusehdf_$SELF_V$5a2f.0.16->QNCAtemplate.dim31_dv_template.addr_a0_V$5a59.0.16)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$THETA$48_V$fd2)[0(SI32), l:0(SI64)]] + 56(SI64)))))[8(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1416,10)  (*((P64*) *((P64*) (comm_diffuse_comp_mod_mp_initdiffusehdf_$SELF_V$5a2f.0.16 + 56(SI64)))))[15(SI64)]
     [[ Unable to inline indirect callsite  <2>]]


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(1393,8)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1408,8)
   remark #25084: Preprocess Loopnests: Moving Out Store    [ comm_diffuse_comp_mod.f90(1408,8) ]
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between at (1408:8) and self(:,:) (1408:8)
   remark #17106: parallel dependence: assumed FLOW dependence between self(:,:) (1408:8) and at (1408:8)
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between at (1408:8) and self(:,:) (1408:8)
   remark #15346: vector dependence: assumed FLOW dependence between self(:,:) (1408:8) and at (1408:8)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1408,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between at (1408:8) and self(:,:) (1408:8)
      remark #17106: parallel dependence: assumed FLOW dependence between self(:,:) (1408:8) and at (1408:8)
      remark #15344: loop was not vectorized: vector dependence prevents vectorization
      remark #15346: vector dependence: assumed ANTI dependence between at (1408:8) and self(:,:) (1408:8)
      remark #15346: vector dependence: assumed FLOW dependence between self(:,:) (1408:8) and at (1408:8)
      remark #25439: unrolled with remainder by 2  
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1408,8)
   <Remainder>
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1409,8)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(1380,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_initdiffusehdf_] comm_diffuse_comp_mod.f90:1380

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   17[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm2]
        
    Routine temporaries
        Total         :     369
            Global    :      94
            Local     :     275
        Regenerable   :      87
        Spilled       :      10
        
    Routine stack
        Variables     :    6020 bytes*
            Reads     :      14 [1.83e+01 ~ 2.6%]
            Writes    :      45 [4.47e+01 ~ 6.5%]
        Spills        :      40 bytes*
            Reads     :       7 [7.84e+00 ~ 1.1%]
            Writes    :       5 [2.94e+00 ~ 0.4%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::DUMPDIFFUSETOFITS

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::DUMPDIFFUSETOFITS) [16/20=80.0%] comm_diffuse_comp_mod.f90(1227,14)
  -> EXTERN: (1244,9) for_trim
  -> EXTERN: (1244,25) for_cpstr
  -> EXTERN: (1245,8) for_concat
  -> EXTERN: (1245,8) __resetsp_inlined
  -> EXTERN: (1245,8) __getsp_inlined
  -> EXTERN: (1245,19) for_trim
  -> EXTERN: (1245,24) _alloca
  -> EXTERN: (1245,40) for_trim
  -> EXTERN: (1245,45) _alloca
  -> INDIRECT-: (1248,16)  (*((P64*) *((P64*) (&((comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15->QNCAtemplate.dim31_dv_template.addr_a0_V$57d1.0.15)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$X$43_V$ec6)->COMM_MAP$INFO$13_V$6d1 + 56(SI64)))))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1268,13) mpi_allreduce_
  -> EXTERN: (1271,19) INT2STRING
  -> EXTERN: (1272,14) for_concat
  -> EXTERN: (1272,21) for_trim
  -> EXTERN: (1272,26) for_adjustl
  -> EXTERN: (1272,51) for_trim
  -> EXTERN: (1272,56) for_adjustl
  -> EXTERN: (1273,19) WRITE_HDF_1D_DP
  -> EXTERN: (1273,40) for_trim
  -> EXTERN: (1273,45) for_adjustl
  -> EXTERN: (1276,19) for_inquire
  -> EXTERN: (1276,24) for_trim
  -> EXTERN: (1277,18) GETLUN
  -> EXTERN: (1279,14) for_open
  -> EXTERN: (1279,30) for_trim
  -> EXTERN: (1280,14) __getsp_inlined
  -> EXTERN: (1280,14) for_write_seq_lis
  -> EXTERN: (1280,14) __resetsp_inlined
  -> EXTERN: (1281,81) _alloca
  -> EXTERN: (1281,81) for_concat
  -> EXTERN: (1285,14) for_open
  -> EXTERN: (1285,30) for_trim
  -> EXTERN: (1287,11) for_write_seq_fmt_xmit
  -> EXTERN: (1287,11) for_write_seq_fmt
  -> EXTERN: (1287,39) for_trim
  -> EXTERN: (1288,17) for_close
  -> EXTERN: (1292,13) UPDATE_STATUS
  -> EXTERN: (1295,15) COMM_MAP_MOD^CONSTRUCTOR_CLONE
  -> EXTERN: (1296,8) for_realloc_lhs
  -> EXTERN: (1298,13) UPDATE_STATUS
  -> EXTERN: (1301,16) INT2STRING
  -> EXTERN: (1302,11) for_concat
  -> EXTERN: (1302,18) for_trim
  -> EXTERN: (1302,23) for_adjustl
  -> EXTERN: (1302,45) for_trim
  -> EXTERN: (1302,50) for_adjustl
  -> EXTERN: (1303,38) CREATE_HDF_GROUP
  -> EXTERN: (1303,71) for_trim
  -> EXTERN: (1303,76) for_adjustl
  -> EXTERN: (1306,13) UPDATE_STATUS
  -> EXTERN: (1308,8) __resetsp_inlined
  -> EXTERN: (1308,8) for_concat
  -> EXTERN: (1308,8) __getsp_inlined
  -> EXTERN: (1308,19) for_trim
  -> EXTERN: (1308,46) for_trim
  -> EXTERN: (1308,51) _alloca
  -> INDIRECT-: (1309,13)  (*((P64*) *((P64*) (&(comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15->QNCAtemplate.dim31_dv_template.addr_a0_V$57d1.0.15)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$B_OUT$46_V$f54 + 56(SI64)))))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1310,13)  (*((P64*) comm_diffuse_comp_mod_mp_dumpdiffusetofits_$MAP$85_V$5789.0.15))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1311,8) for_realloc_lhs
  -> EXTERN: (1312,13) UPDATE_STATUS
  -> INDIRECT-: (1317,16)  (*((P64*) comm_diffuse_comp_mod_mp_dumpdiffusetofits_$MAP$85_V$5789.0.15))[9(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1317,16) __resetsp_inlined
  -> EXTERN: (1317,16) __getsp_inlined
  -> EXTERN: (1317,30) for_trim
  -> EXTERN: (1317,35) _alloca
  -> EXTERN: (1317,44) _alloca
  -> EXTERN: (1317,44) for_concat
  -> EXTERN: (1317,46) for_trim
  -> EXTERN: (1318,45) for_trim
  -> EXTERN: (1318,55) _alloca
  -> EXTERN: (1318,55) for_concat
  -> EXTERN: (1320,16) __resetsp_inlined
  -> INDIRECT-: (1320,16)  (*((P64*) comm_diffuse_comp_mod_mp_dumpdiffusetofits_$MAP$85_V$5789.0.15))[9(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1320,16) __getsp_inlined
  -> EXTERN: (1320,30) for_trim
  -> EXTERN: (1320,35) _alloca
  -> EXTERN: (1320,44) for_concat
  -> EXTERN: (1320,44) _alloca
  -> EXTERN: (1320,46) for_trim
  -> INDIRECT-: (1322,13)  (*((P64*) comm_diffuse_comp_mod_mp_dumpdiffusetofits_$MAP$85_V$5789.0.15))[12(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1323,13) UPDATE_STATUS
  -> EXTERN: (1326,18) COMM_MAP_MOD^CONSTRUCTOR_CLONE
  -> EXTERN: (1327,11) for_realloc_lhs
  -> EXTERN: (1329,11) __getsp_inlined
  -> EXTERN: (1329,11) for_concat
  -> EXTERN: (1329,11) __resetsp_inlined
  -> EXTERN: (1329,22) for_trim
  -> EXTERN: (1329,49) for_trim
  -> EXTERN: (1329,54) _alloca
  -> INDIRECT-: (1330,16)  (*((P64*) *((P64*) (&(comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15->QNCAtemplate.dim31_dv_template.addr_a0_V$57d1.0.15)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$B_OUT$46_V$f54 + 56(SI64)))))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1331,16)  (*((P64*) comm_diffuse_comp_mod_mp_dumpdiffusetofits_$MAP$85_V$5789.0.15))[5(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1333,16) __getsp_inlined
  -> INDIRECT-: (1333,16)  (*((P64*) comm_diffuse_comp_mod_mp_dumpdiffusetofits_$MAP$85_V$5789.0.15))[9(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1333,16) __resetsp_inlined
  -> EXTERN: (1333,30) for_trim
  -> EXTERN: (1333,35) _alloca
  -> EXTERN: (1333,44) _alloca
  -> EXTERN: (1333,44) for_concat
  -> EXTERN: (1333,46) for_trim
  -> INDIRECT-: (1334,16)  (*((P64*) comm_diffuse_comp_mod_mp_dumpdiffusetofits_$MAP$85_V$5789.0.15))[12(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1336,13) UPDATE_STATUS
  -> EXTERN: (1338,8) for_alloc_allocatable
  -> EXTERN: (1338,8) for_check_mult_overflow64
  -> INDIRECT-: (1339,13)  (*((P64*) *((P64*) (&(comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15->QNCAtemplate.dim31_dv_template.addr_a0_V$57d1.0.15)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$X$43_V$ec4 + 56(SI64)))))[17(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1340,8) for_realloc_lhs
  -> INDIRECT-: (1340,28)  (*((P64*) *((P64*) (comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15 + 56(SI64)))))[12(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1342,11) for_concat
  -> EXTERN: (1342,11) __resetsp_inlined
  -> EXTERN: (1342,11) __getsp_inlined
  -> EXTERN: (1342,22) for_trim
  -> EXTERN: (1342,27) _alloca
  -> EXTERN: (1342,46) for_trim
  -> EXTERN: (1342,73) for_trim
  -> EXTERN: (1342,78) _alloca
  -> EXTERN: (1343,16) WRITE_SIGMA_L
  -> EXTERN: (1344,27) WRITE_HDF_2D_DP
  -> EXTERN: (1344,27) __getsp_inlined
  -> EXTERN: (1344,27) __resetsp_inlined
  -> EXTERN: (1344,53) for_trim
  -> EXTERN: (1344,58) for_adjustl
  -> EXTERN: (1344,72) for_concat
  -> EXTERN: (1344,72) _alloca
  -> EXTERN: (1346,8) for_dealloc_allocatable
  -> EXTERN: (1347,13) UPDATE_STATUS
  -> EXTERN: (1351,11) for_concat
  -> EXTERN: (1351,11) __resetsp_inlined
  -> EXTERN: (1351,11) __getsp_inlined
  -> EXTERN: (1351,22) for_trim
  -> EXTERN: (1351,49) for_trim
  -> EXTERN: (1352,18) for_trim
  -> EXTERN: (1352,23) _alloca
  -> INDIRECT-: (1353,35)  (*((P64*) *((P64*) (&((MAP_PTR$P$19_V$930 *)(comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15->QNCAtemplate.dim31_dv_template.addr_a0_V$57d1.0.15)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$THETA$48_V$fc9)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_dumpdiffusetofits_$I_V$577f.0.15)), l:((comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15->QNCAtemplate.dim31_dv_template.addr_a0_V$57d1.0.15)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$THETA$48_V$fd2)[0(SI32), l:0(SI64)]] + 56(SI64)))))[6(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (1356,19)  (*((P64*) *((P64*) (&((MAP_PTR$P$19_V$930 *)(comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15->QNCAtemplate.dim31_dv_template.addr_a0_V$57d1.0.15)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$THETA$48_V$fc9)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_dumpdiffusetofits_$I_V$577f.0.15)), l:((comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15->QNCAtemplate.dim31_dv_template.addr_a0_V$57d1.0.15)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$THETA$48_V$fd2)[0(SI32), l:0(SI64)]] + 56(SI64)))))[9(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1356,19) __getsp_inlined
  -> EXTERN: (1356,19) __resetsp_inlined
  -> EXTERN: (1356,45) for_trim
  -> EXTERN: (1356,50) _alloca
  -> EXTERN: (1356,59) for_concat
  -> EXTERN: (1356,59) _alloca
  -> EXTERN: (1356,61) for_trim
  -> EXTERN: (1357,48) for_trim
  -> EXTERN: (1357,65) for_trim
  -> EXTERN: (1357,70) for_adjustl
  -> EXTERN: (1357,96) for_concat
  -> EXTERN: (1357,96) _alloca
  -> EXTERN: (1360,19) __getsp_inlined
  -> INDIRECT-: (1360,19)  (*((P64*) *((P64*) (&((MAP_PTR$P$19_V$930 *)(comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15->QNCAtemplate.dim31_dv_template.addr_a0_V$57d1.0.15)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$THETA$48_V$fc9)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_dumpdiffusetofits_$I_V$577f.0.15)), l:((comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15->QNCAtemplate.dim31_dv_template.addr_a0_V$57d1.0.15)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$THETA$48_V$fd2)[0(SI32), l:0(SI64)]] + 56(SI64)))))[9(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1360,19) __resetsp_inlined
  -> EXTERN: (1360,45) for_trim
  -> EXTERN: (1360,50) _alloca
  -> EXTERN: (1360,59) _alloca
  -> EXTERN: (1360,59) for_concat
  -> EXTERN: (1360,61) for_trim
  -> EXTERN: (1363,13) UPDATE_STATUS
  -> EXTERN: (1369,14) __resetsp_inlined
  -> EXTERN: (1369,14) for_concat
  -> EXTERN: (1369,14) __getsp_inlined
  -> EXTERN: (1369,38) for_trim
  -> EXTERN: (1369,65) for_trim
  -> EXTERN: (1370,21) for_trim
  -> EXTERN: (1370,26) _alloca
  -> INDIRECT-: (1371,19)  (*((P64*) *((P64*) (&((MAP_PTR$P$19_V$1025 *)(comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15->QNCAtemplate.dim31_dv_template.addr_a0_V$57d1.0.15)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_V$1049)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_dumpdiffusetofits_$I_V$577f.0.15)), l:((comm_diffuse_comp_mod_mp_dumpdiffusetofits_$SELF_V$5751.0.15->QNCAtemplate.dim31_dv_template.addr_a0_V$57d1.0.15)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$F_V$1052)[0(SI32), l:0(SI64)]] + 56(SI64)))))[9(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (1371,19) __getsp_inlined
  -> EXTERN: (1371,19) __resetsp_inlined
  -> EXTERN: (1371,41) for_trim
  -> EXTERN: (1371,46) _alloca
  -> EXTERN: (1371,55) for_concat
  -> EXTERN: (1371,55) _alloca
  -> EXTERN: (1371,57) for_trim
  -> EXTERN: (1374,13) UPDATE_STATUS
  -> EXTERN: (1377,3) for_dealloc_allocatable


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(1246,8)
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15388: vectorization support: reference vals(:) has aligned access
   remark #15305: vectorization support: vector length 2
   remark #15427: loop was completely unrolled
   remark #15399: vectorization support: unroll factor set to 5
   remark #15300: LOOP WAS VECTORIZED
   remark #15449: unmasked aligned unit stride stores: 1 
   remark #15475: --- begin vector cost summary ---
   remark #15476: scalar cost: 4 
   remark #15477: vector cost: 1.500 
   remark #15478: estimated potential speedup: 2.660 
   remark #15488: --- end vector cost summary ---
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1247,8)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1296,8)
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1296,8)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15389: vectorization support: reference MAP(:,:) has unaligned access
      remark #15389: vectorization support: reference MAP(:,:) has unaligned access   [ comm_diffuse_comp_mod.f90(1296,44) ]
      remark #15381: vectorization support: unaligned access used inside loop body
      remark #15305: vectorization support: vector length 2
      remark #15399: vectorization support: unroll factor set to 4
      remark #15309: vectorization support: normalized vectorization overhead 0.227
      remark #15300: LOOP WAS VECTORIZED
      remark #15450: unmasked unaligned unit stride loads: 1 
      remark #15451: unmasked unaligned unit stride stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 10 
      remark #15477: vector cost: 5.500 
      remark #15478: estimated potential speedup: 1.780 
      remark #15488: --- end vector cost summary ---
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1296,8)
   <Remainder loop for vectorization>
      remark #15389: vectorization support: reference MAP(:,:) has unaligned access
      remark #15389: vectorization support: reference MAP(:,:) has unaligned access   [ comm_diffuse_comp_mod.f90(1296,44) ]
      remark #15381: vectorization support: unaligned access used inside loop body
      remark #15335: remainder loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
      remark #15305: vectorization support: vector length 2
      remark #15309: vectorization support: normalized vectorization overhead 1.333
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1311,8)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between self (1311:8) and MAP(:,:) (1311:8)
   remark #17106: parallel dependence: assumed FLOW dependence between MAP(:,:) (1311:8) and self (1311:8)
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between self (1311:8) and MAP(:,:) (1311:8)
   remark #15346: vector dependence: assumed FLOW dependence between MAP(:,:) (1311:8) and self (1311:8)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1311,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between self (1311:8) and MAP(:,:) (1311:8)
      remark #17106: parallel dependence: assumed FLOW dependence between MAP(:,:) (1311:8) and self (1311:8)
      remark #15344: loop was not vectorized: vector dependence prevents vectorization
      remark #15346: vector dependence: assumed ANTI dependence between self (1311:8) and MAP(:,:) (1311:8)
      remark #15346: vector dependence: assumed FLOW dependence between MAP(:,:) (1311:8) and self (1311:8)
      remark #25439: unrolled with remainder by 2  
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1311,8)
   <Remainder>
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1327,11)
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1327,11)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15389: vectorization support: reference MAP(:,:) has unaligned access
      remark #15389: vectorization support: reference MAP(:,:) has unaligned access   [ comm_diffuse_comp_mod.f90(1327,47) ]
      remark #15381: vectorization support: unaligned access used inside loop body
      remark #15305: vectorization support: vector length 2
      remark #15399: vectorization support: unroll factor set to 4
      remark #15309: vectorization support: normalized vectorization overhead 0.227
      remark #15300: LOOP WAS VECTORIZED
      remark #15450: unmasked unaligned unit stride loads: 1 
      remark #15451: unmasked unaligned unit stride stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 10 
      remark #15477: vector cost: 5.500 
      remark #15478: estimated potential speedup: 1.780 
      remark #15488: --- end vector cost summary ---
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1327,11)
   <Remainder loop for vectorization>
      remark #15389: vectorization support: reference MAP(:,:) has unaligned access
      remark #15389: vectorization support: reference MAP(:,:) has unaligned access   [ comm_diffuse_comp_mod.f90(1327,47) ]
      remark #15381: vectorization support: unaligned access used inside loop body
      remark #15335: remainder loop was not vectorized: vectorization possible but seems inefficient. Use vector always directive or -vec-threshold0 to override 
      remark #15305: vectorization support: vector length 2
      remark #15309: vectorization support: normalized vectorization overhead 1.333
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1340,8)
   remark #17108: loop was not parallelized: insufficient computational work
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1340,8)
   <Peeled loop for vectorization>
      remark #25015: Estimate of max trip count of loop=1
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1340,8)
      remark #17108: loop was not parallelized: insufficient computational work
      remark #15388: vectorization support: reference SIGMA_L(:,:) has aligned access
      remark #15388: vectorization support: reference SIGMA_L(:,:) has aligned access   [ comm_diffuse_comp_mod.f90(1340,28) ]
      remark #15305: vectorization support: vector length 2
      remark #15399: vectorization support: unroll factor set to 4
      remark #15309: vectorization support: normalized vectorization overhead 0.500
      remark #15300: LOOP WAS VECTORIZED
      remark #15442: entire loop may be executed in remainder
      remark #15448: unmasked aligned unit stride loads: 1 
      remark #15449: unmasked aligned unit stride stores: 1 
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 9 
      remark #15477: vector cost: 3.000 
      remark #15478: estimated potential speedup: 2.880 
      remark #15488: --- end vector cost summary ---
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1340,8)
   <Remainder loop for vectorization>
      remark #15388: vectorization support: reference SIGMA_L(:,:) has aligned access
      remark #15388: vectorization support: reference SIGMA_L(:,:) has aligned access   [ comm_diffuse_comp_mod.f90(1340,28) ]
      remark #15305: vectorization support: vector length 2
      remark #15309: vectorization support: normalized vectorization overhead 1.200
      remark #15301: REMAINDER LOOP WAS VECTORIZED
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(1340,8)
   <Remainder loop for vectorization>
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1350,8)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(1367,11)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(1242,46):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(1227,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_dumpdiffusetofits_] comm_diffuse_comp_mod.f90:1227

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   20[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm5]
        
    Routine temporaries
        Total         :    1434
            Global    :     343
            Local     :    1091
        Regenerable   :     497
        Spilled       :      37
        
    Routine stack
        Variables     :   19738 bytes*
            Reads     :      62 [3.88e+01 ~ 1.6%]
            Writes    :     297 [1.56e+02 ~ 6.3%]
        Spills        :     256 bytes*
            Reads     :      72 [4.12e+01 ~ 1.7%]
            Writes    :      41 [1.91e+01 ~ 0.8%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::INITDIFFPRECOND_PSEUDOINV

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::INITDIFFPRECOND_PSEUDOINV) [17/20=85.0%] comm_diffuse_comp_mod.f90(282,14)
  -> EXTERN: (298,8) for_check_mult_overflow64
  -> EXTERN: (298,8) for_alloc_allocatable
  -> EXTERN: (302,24) strcmp
  -> INDIRECT-: (307,16)  (*((P64*) comm_diffuse_comp_mod_mp_initdiffprecond_pseudoinv_$C$68_V$45b9.0.5))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (309,20) COMM_MAP_MOD^CONSTRUCTOR_MAPINFO
  -> EXTERN: (313,10) wall_time_
  -> EXTERN: (314,5) for_alloc_allocatable
  -> EXTERN: (314,5) for_check_mult_overflow64
  -> EXTERN: (317,11) for_check_mult_overflow64
  -> EXTERN: (317,11) for_alloc_allocatable
  -> EXTERN: (321,3) for_dealloc_allocatable
  -> EXTERN: (321,3) for_dealloc_allocatable


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(302,24)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15541: outer loop was not auto-vectorized: consider using SIMD directive

   LOOP BEGIN at comm_diffuse_comp_mod.f90(302,24)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(314,5)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15523: loop was not vectorized: loop control variable ? was found, but loop iteration count cannot be computed before executing the loop
   remark #25456: Number of Array Refs Scalar Replaced In Loop: 2
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(315,5)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed FLOW dependence between p_cr(l,j) (317:11) and p_cr(l,j) (317:11)
   remark #17106: parallel dependence: assumed ANTI dependence between p_cr(l,j) (317:11) and p_cr(l,j) (317:11)
   remark #15382: vectorization support: call to function for_check_mult_overflow64 cannot be vectorized   [ comm_diffuse_comp_mod.f90(317,11) ]
   remark #15382: vectorization support: call to function for_alloc_allocatable cannot be vectorized   [ comm_diffuse_comp_mod.f90(317,11) ]
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed FLOW dependence between p_cr(l,j) (317:11) and p_cr(l,j) (317:11)
   remark #15346: vector dependence: assumed ANTI dependence between p_cr(l,j) (317:11) and p_cr(l,j) (317:11)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(316,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed FLOW dependence between p_cr(l,j) (317:11) and p_cr(l,j) (317:11)
      remark #17106: parallel dependence: assumed ANTI dependence between p_cr(l,j) (317:11) and p_cr(l,j) (317:11)
      remark #15382: vectorization support: call to function for_check_mult_overflow64 cannot be vectorized   [ comm_diffuse_comp_mod.f90(317,11) ]
      remark #15382: vectorization support: call to function for_alloc_allocatable cannot be vectorized   [ comm_diffuse_comp_mod.f90(317,11) ]
      remark #15344: loop was not vectorized: vector dependence prevents vectorization
      remark #15346: vector dependence: assumed FLOW dependence between p_cr(l,j) (317:11) and p_cr(l,j) (317:11)
      remark #15346: vector dependence: assumed ANTI dependence between p_cr(l,j) (317:11) and p_cr(l,j) (317:11)
   LOOP END
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(291,50):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(288,48):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(299,8):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to increase the width of loads
comm_diffuse_comp_mod.f90(299,8):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (8, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(314,5):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to increase the width of stores
comm_diffuse_comp_mod.f90(314,5):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (1, 0)
comm_diffuse_comp_mod.f90(302,24):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (16, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(304,14):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to increase the width of stores
comm_diffuse_comp_mod.f90(304,14):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (16, 0), and destination (alignment, offset): (1, 0)
comm_diffuse_comp_mod.f90(282,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_initdiffprecond_pseudoinv_] comm_diffuse_comp_mod.f90:282

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   21[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm6]
        
    Routine temporaries
        Total         :     371
            Global    :      53
            Local     :     318
        Regenerable   :      67
        Spilled       :      15
        
    Routine stack
        Variables     :     716 bytes*
            Reads     :      41 [1.38e+01 ~ 1.4%]
            Writes    :      75 [1.43e+01 ~ 1.4%]
        Spills        :      80 bytes*
            Reads     :      17 [1.05e+01 ~ 1.1%]
            Writes    :      10 [3.22e+00 ~ 0.3%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::INITDIFFPRECOND_DIAGONAL

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::INITDIFFPRECOND_DIAGONAL) [18/20=90.0%] comm_diffuse_comp_mod.f90(200,14)
  -> EXTERN: (216,8) for_check_mult_overflow64
  -> EXTERN: (216,8) for_alloc_allocatable
  -> EXTERN: (220,24) strcmp
  -> INDIRECT-: (225,16)  (*((P64*) comm_diffuse_comp_mod_mp_initdiffprecond_diagonal_$C$61_V$41cd.0.4))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (227,20) COMM_MAP_MOD^CONSTRUCTOR_MAPINFO
  -> EXTERN: (231,10) wall_time_
  -> EXTERN: (232,5) for_alloc_allocatable
  -> EXTERN: (232,5) for_check_mult_overflow64
  -> EXTERN: (234,5) for_alloc_allocatable
  -> EXTERN: (234,5) for_check_mult_overflow64
  -> EXTERN: (234,5) for_alloc_allocatable
  -> EXTERN: (234,5) for_check_mult_overflow64
  -> INDIRECT-: (238,16)  (*((P64*) comm_diffuse_comp_mod_mp_info_pre_$55_V$398a))[2(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> INDIRECT-: (241,19)  (*((P64*) *((P64*) (&((COMM_DATA_SET$INFO$20_V$10d9 *)comm_data_mod_mp_data__V$395e)[(EXPR_CONV.SI32.SI64(comm_diffuse_comp_mod_mp_initdiffprecond_diagonal_$Q_V$41ed.0.4)), l:comm_data_mod_mp_data__V$3967[0(SI32), l:0(SI64)]] + 56(SI64)))))[1(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (258,11) for_check_mult_overflow64
  -> EXTERN: (258,11) for_alloc_allocatable
  -> EXTERN: (268,11) for_check_mult_overflow64
  -> EXTERN: (268,11) for_alloc_allocatable
  -> EXTERN: (269,11) for_check_mult_overflow64
  -> EXTERN: (269,11) for_alloc_allocatable
  -> EXTERN: (269,11) for_check_mult_overflow64
  -> EXTERN: (269,11) for_alloc_allocatable
  -> EXTERN: (270,11) for_realloc_lhs
  -> EXTERN: (271,11) __getsp_inlined
  -> EXTERN: (271,11) _alloca
  -> EXTERN: (271,11) _alloca
  -> EXTERN: (271,11) for_realloc_lhs
  -> EXTERN: (271,11) __resetsp_inlined
  -> EXTERN: (275,5) for_dealloc_allocatable
  -> EXTERN: (275,5) for_dealloc_allocatable
  -> EXTERN: (277,10) wall_time_
  -> EXTERN: (279,3) for_dealloc_allocatable
  -> EXTERN: (279,3) for_dealloc_allocatable


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(220,24)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15541: outer loop was not auto-vectorized: consider using SIMD directive

   LOOP BEGIN at comm_diffuse_comp_mod.f90(220,24)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15521: loop was not vectorized: loop control variable was not identified. Explicitly compute the iteration count before executing the loop or try using canonical loop form from OpenMP specification
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(232,5)
   remark #17102: loop was not parallelized: not a parallelization candidate
   remark #15523: loop was not vectorized: loop control variable ? was found, but loop iteration count cannot be computed before executing the loop
   remark #25456: Number of Array Refs Scalar Replaced In Loop: 2
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(235,5)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between info_pre (237:8) and at (271:11)
   remark #17106: parallel dependence: assumed FLOW dependence between at (271:11) and info_pre (237:8)
   remark #15542: loop was not vectorized: inner loop was already vectorized

   LOOP BEGIN at comm_diffuse_comp_mod.f90(237,8)
      remark #17102: loop was not parallelized: not a parallelization candidate
      remark #15542: loop was not vectorized: inner loop was already vectorized

      LOOP BEGIN at comm_diffuse_comp_mod.f90(239,11)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(239,11)
            remark #25408: memset generated
            remark #17104: loop was not parallelized: existence of parallel dependence
            remark #17106: parallel dependence: assumed OUTPUT dependence between MAT(:,:) (239:11) and MAT(:,:) (239:11)
            remark #17106: parallel dependence: assumed OUTPUT dependence between MAT(:,:) (239:11) and MAT(:,:) (239:11)
            remark #15542: loop was not vectorized: inner loop was already vectorized
            remark #25015: Estimate of max trip count of loop=1

            LOOP BEGIN at comm_diffuse_comp_mod.f90(239,11)
               remark #17108: loop was not parallelized: insufficient computational work
               remark #15389: vectorization support: reference MAT(:,:) has unaligned access
               remark #15381: vectorization support: unaligned access used inside loop body
               remark #15305: vectorization support: vector length 2
               remark #15399: vectorization support: unroll factor set to 2
               remark #15309: vectorization support: normalized vectorization overhead 0.300
               remark #15300: LOOP WAS VECTORIZED
               remark #15451: unmasked unaligned unit stride stores: 1 
               remark #15475: --- begin vector cost summary ---
               remark #15476: scalar cost: 4 
               remark #15477: vector cost: 2.500 
               remark #15478: estimated potential speedup: 1.450 
               remark #15488: --- end vector cost summary ---
               remark #25015: Estimate of max trip count of loop=3
            LOOP END

            LOOP BEGIN at comm_diffuse_comp_mod.f90(239,11)
            <Remainder loop for vectorization>
               remark #25015: Estimate of max trip count of loop=12
            LOOP END
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(240,11)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #15382: vectorization support: call to function (Indirect call) cannot be vectorized   [ comm_diffuse_comp_mod.f90(241,19) ]
         remark #15382: vectorization support: call to function ?1memcpy cannot be vectorized   [ comm_diffuse_comp_mod.f90(244,17) ]
         remark #15382: vectorization support: call to function ?1memcpy cannot be vectorized   [ comm_diffuse_comp_mod.f90(247,20) ]
         remark #15344: loop was not vectorized: vector dependence prevents vectorization

         LOOP BEGIN at comm_diffuse_comp_mod.f90(243,14)
            remark #17104: loop was not parallelized: existence of parallel dependence
            remark #17106: parallel dependence: assumed ANTI dependence between diffcomps (244:23) and diffcomps(k2) (247:20)
            remark #17106: parallel dependence: assumed FLOW dependence between diffcomps(k2) (247:20) and diffcomps (244:23)
            remark #15382: vectorization support: call to function ?1memcpy cannot be vectorized   [ comm_diffuse_comp_mod.f90(244,17) ]
            remark #15382: vectorization support: call to function ?1memcpy cannot be vectorized   [ comm_diffuse_comp_mod.f90(247,20) ]
            remark #15344: loop was not vectorized: vector dependence prevents vectorization
            remark #15346: vector dependence: assumed ANTI dependence between diffcomps (244:23) and diffcomps(k2) (247:20)
            remark #15346: vector dependence: assumed FLOW dependence between diffcomps(k2) (247:20) and diffcomps (244:23)

            LOOP BEGIN at comm_diffuse_comp_mod.f90(246,17)
               remark #17104: loop was not parallelized: existence of parallel dependence
               remark #17106: parallel dependence: assumed ANTI dependence between diffcomps (247:26) and diffcomps(k2) (247:20)
               remark #17106: parallel dependence: assumed FLOW dependence between diffcomps(k2) (247:20) and diffcomps (247:26)
               remark #15382: vectorization support: call to function ?1memcpy cannot be vectorized   [ comm_diffuse_comp_mod.f90(247,20) ]
               remark #15344: loop was not vectorized: vector dependence prevents vectorization
               remark #15346: vector dependence: assumed ANTI dependence between diffcomps (247:26) and diffcomps(k2) (247:20)
               remark #15346: vector dependence: assumed FLOW dependence between diffcomps(k2) (247:20) and diffcomps (247:26)
            LOOP END
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(259,11)
         remark #25408: memset generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i1,j,:) (259:11) and p_cr(i1,j,:) (259:11)
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i1,j,:) (259:11) and p_cr(i1,j,:) (259:11)
         remark #15542: loop was not vectorized: inner loop was already vectorized
         remark #25015: Estimate of max trip count of loop=1

         LOOP BEGIN at comm_diffuse_comp_mod.f90(259,11)
         <Peeled loop for vectorization>
            remark #25015: Estimate of max trip count of loop=3
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(259,11)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference p_cr(i1,j,:) has aligned access
            remark #15305: vectorization support: vector length 4
            remark #15309: vectorization support: normalized vectorization overhead 3.667
            remark #15300: LOOP WAS VECTORIZED
            remark #15442: entire loop may be executed in remainder
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 2 
            remark #15477: vector cost: 0.750 
            remark #15478: estimated potential speedup: 1.410 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=6
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(259,11)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=24
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(260,11)
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed ANTI dependence between n (262:17) and n (262:17)
         remark #17106: parallel dependence: assumed FLOW dependence between n (262:17) and n (262:17)
         remark #17106: parallel dependence: assumed FLOW dependence between n (262:17) and n (262:17)
         remark #17106: parallel dependence: assumed ANTI dependence between n (262:17) and n (262:17)
         remark #17106: parallel dependence: assumed OUTPUT dependence between IND(n) (263:17) and IND(n) (263:17)
         remark #17106: parallel dependence: assumed OUTPUT dependence between IND(n) (263:17) and IND(n) (263:17)
         remark #15344: loop was not vectorized: vector dependence prevents vectorization
         remark #15346: vector dependence: assumed ANTI dependence between n (262:17) and n (262:17)
         remark #15346: vector dependence: assumed FLOW dependence between n (262:17) and n (262:17)
         remark #15346: vector dependence: assumed FLOW dependence between n (262:17) and n (262:17)
         remark #15346: vector dependence: assumed ANTI dependence between n (262:17) and n (262:17)
         remark #25439: unrolled with remainder by 2  
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(260,11)
      <Remainder>
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(270,11)
         remark #25399: memcopy generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i1,j,:) (270:11) and p_cr(i1,j,:) (270:11)
         remark #17106: parallel dependence: assumed OUTPUT dependence between p_cr(i1,j,:) (270:11) and p_cr(i1,j,:) (270:11)
         remark #15542: loop was not vectorized: inner loop was already vectorized
         remark #25015: Estimate of max trip count of loop=1

         LOOP BEGIN at comm_diffuse_comp_mod.f90(270,11)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference p_cr(i1,j,:) has aligned access
            remark #15388: vectorization support: reference IND(:) has aligned access
            remark #15305: vectorization support: vector length 4
            remark #15309: vectorization support: normalized vectorization overhead 1.667
            remark #15300: LOOP WAS VECTORIZED
            remark #15448: unmasked aligned unit stride loads: 1 
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 0.750 
            remark #15478: estimated potential speedup: 4.170 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=6
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(270,11)
         <Alternate Alignment Vectorized Loop>
            remark #25015: Estimate of max trip count of loop=6
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(270,11)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=24
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(271,11)
         remark #25399: memcopy generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between at (271:11) and at (271:11)
         remark #17106: parallel dependence: assumed OUTPUT dependence between at (271:11) and at (271:11)
         remark #15542: loop was not vectorized: inner loop was already vectorized
         remark #25015: Estimate of max trip count of loop=1

         LOOP BEGIN at comm_diffuse_comp_mod.f90(271,11)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference at (271:43) has aligned access   [ comm_diffuse_comp_mod.f90(271,43) ]
            remark #15388: vectorization support: reference IND(:) has aligned access   [ comm_diffuse_comp_mod.f90(271,43) ]
            remark #15305: vectorization support: vector length 4
            remark #15300: LOOP WAS VECTORIZED
            remark #15448: unmasked aligned unit stride loads: 1 
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 0.750 
            remark #15478: estimated potential speedup: 5.330 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=6
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(271,11)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=24
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(271,11)
         remark #25399: memcopy generated
         remark #17104: loop was not parallelized: existence of parallel dependence
         remark #17106: parallel dependence: assumed OUTPUT dependence between at (271:11) and at (271:11)
         remark #17106: parallel dependence: assumed OUTPUT dependence between at (271:11) and at (271:11)
         remark #15542: loop was not vectorized: inner loop was already vectorized
         remark #25015: Estimate of max trip count of loop=1

         LOOP BEGIN at comm_diffuse_comp_mod.f90(271,11)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference at (271:52) has aligned access   [ comm_diffuse_comp_mod.f90(271,52) ]
            remark #15388: vectorization support: reference IND(:) has aligned access   [ comm_diffuse_comp_mod.f90(271,52) ]
            remark #15305: vectorization support: vector length 4
            remark #15300: LOOP WAS VECTORIZED
            remark #15448: unmasked aligned unit stride loads: 1 
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 4 
            remark #15477: vector cost: 0.750 
            remark #15478: estimated potential speedup: 5.330 
            remark #15488: --- end vector cost summary ---
            remark #25015: Estimate of max trip count of loop=6
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(271,11)
         <Remainder loop for vectorization>
            remark #25015: Estimate of max trip count of loop=24
         LOOP END
      LOOP END

      LOOP BEGIN at comm_diffuse_comp_mod.f90(271,11)
         remark #17108: loop was not parallelized: insufficient computational work
         remark #15542: loop was not vectorized: inner loop was already vectorized

         LOOP BEGIN at comm_diffuse_comp_mod.f90(271,11)
         <Peeled loop for vectorization>
            remark #25015: Estimate of max trip count of loop=1
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(271,11)
            remark #17108: loop was not parallelized: insufficient computational work
            remark #15388: vectorization support: reference p_cr(i1,j,:,:) has aligned access
            remark #15389: vectorization support: reference IND(:) has unaligned access
            remark #15381: vectorization support: unaligned access used inside loop body
            remark #15328: vectorization support: irregularly indexed load was emulated for the variable <MAT(IND(:),IND(:))>, part of index is read from memory
            remark #15305: vectorization support: vector length 2
            remark #15309: vectorization support: normalized vectorization overhead 1.400
            remark #15300: LOOP WAS VECTORIZED
            remark #15442: entire loop may be executed in remainder
            remark #15449: unmasked aligned unit stride stores: 1 
            remark #15450: unmasked unaligned unit stride loads: 1 
            remark #15462: unmasked indexed (or gather) loads: 1 
            remark #15475: --- begin vector cost summary ---
            remark #15476: scalar cost: 8 
            remark #15477: vector cost: 5.000 
            remark #15478: estimated potential speedup: 1.570 
            remark #15488: --- end vector cost summary ---
         LOOP END

         LOOP BEGIN at comm_diffuse_comp_mod.f90(271,11)
         <Remainder loop for vectorization>
         LOOP END
      LOOP END
   LOOP END
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(209,50):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(206,48):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(217,8):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to increase the width of loads
comm_diffuse_comp_mod.f90(217,8):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (8, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(232,5):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to increase the width of stores
comm_diffuse_comp_mod.f90(232,5):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (32, 0), and destination (alignment, offset): (1, 0)
comm_diffuse_comp_mod.f90(239,11):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(239,11):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(244,17):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to increase the width of loads
comm_diffuse_comp_mod.f90(244,17):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (1, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(247,20):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to increase the width of loads
comm_diffuse_comp_mod.f90(247,20):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (1, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(259,11):remark #34014: optimization advice for memset: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(259,11):remark #34026: call to memset implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(270,11):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(270,11):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(270,11):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(271,11):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(271,11):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(271,11):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(271,11):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(271,11):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(271,11):remark #34026: call to memcpy implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(220,24):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (16, 0), and destination (alignment, offset): (16, 0)
comm_diffuse_comp_mod.f90(222,14):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to increase the width of stores
comm_diffuse_comp_mod.f90(222,14):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (16, 0), and destination (alignment, offset): (1, 0)
comm_diffuse_comp_mod.f90(200,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_initdiffprecond_diagonal_] comm_diffuse_comp_mod.f90:200

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   21[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm6]
        
    Routine temporaries
        Total         :    1124
            Global    :     180
            Local     :     944
        Regenerable   :     125
        Spilled       :      33
        
    Routine stack
        Variables     :    1204 bytes*
            Reads     :      90 [2.54e+03 ~ 3.6%]
            Writes    :     146 [1.09e+04 ~ 15.3%]
        Spills        :     216 bytes*
            Reads     :      41 [1.70e+03 ~ 2.4%]
            Writes    :      30 [5.08e+02 ~ 0.7%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::INITDIFFPRECOND

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::INITDIFFPRECOND) [19/20=95.0%] comm_diffuse_comp_mod.f90(185,14)
  -> EXTERN: (189,18) memmove
  -> EXTERN: (189,18) _alloca
  -> EXTERN: (189,18) for_trim
  -> EXTERN: (190,11) for_cpstr
  -> (191,13) COMM_DIFFUSE_COMP_MOD::INITDIFFPRECOND_DIAGONAL (isz = 2620) (sz = 2625)
     [[ Inlining would exceed -inline-max-size value (2625>253) <1>]]
  -> EXTERN: (192,11) for_cpstr
  -> (193,13) COMM_DIFFUSE_COMP_MOD::INITDIFFPRECOND_PSEUDOINV (isz = 952) (sz = 957)
     [[ Inlining would exceed -inline-max-size value (957>253) <1>]]
  -> EXTERN: (195,13) __resetsp_inlined
  -> EXTERN: (195,13) REPORT_ERROR
  -> EXTERN: (195,13) __getsp_inlined


    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(189,18):remark #34014: optimization advice for memmove: increase the destination's alignment to 16 (and use __assume_aligned) to speed up library implementation
comm_diffuse_comp_mod.f90(189,18):remark #34026: call to memmove implemented as a call to optimized library version
comm_diffuse_comp_mod.f90(185,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_initdiffprecond_] comm_diffuse_comp_mod.f90:185

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   10[ rax rdx rcx rbx rsi rdi r8 r13-r15]
        
    Routine temporaries
        Total         :      48
            Global    :      16
            Local     :      32
        Regenerable   :      18
        Spilled       :       4
        
    Routine stack
        Variables     :     512 bytes*
            Reads     :       0 [0.00e+00 ~ 0.0%]
            Writes    :       0 [0.00e+00 ~ 0.0%]
        Spills        :       0 bytes*
            Reads     :       0 [0.00e+00 ~ 0.0%]
            Writes    :       0 [0.00e+00 ~ 0.0%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

Begin optimization report for: COMM_DIFFUSE_COMP_MOD::INITDIFFUSE

    Report from: Interprocedural optimizations [ipo]

INLINE REPORT: (COMM_DIFFUSE_COMP_MOD::INITDIFFUSE) [20/20=100.0%] comm_diffuse_comp_mod.f90(85,14)
  -> INDIRECT-: (94,10)  (*((P64*) *((P64*) (comm_diffuse_comp_mod_mp_initdiffuse_$SELF_V$39f1.0.2 + 56(SI64)))))[5(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (102,5) memcpy
  -> EXTERN: (109,5) memcpy
  -> EXTERN: (110,5) memcpy
  -> EXTERN: (111,27) COMM_MAP_MOD^CONSTRUCTOR_MAPINFO
  -> EXTERN: (117,5) memcpy
  -> EXTERN: (127,9) for_trim
  -> EXTERN: (127,41) for_cpstr
  -> EXTERN: (127,56) for_trim
  -> EXTERN: (127,88) for_cpstr
  -> EXTERN: (128,18) COMM_MAP_MOD^CONSTRUCTOR_MAP
  -> EXTERN: (131,8) __resetsp_inlined
  -> EXTERN: (131,8) __getsp_inlined
  -> EXTERN: (131,18) COMM_MAP_MOD^CONSTRUCTOR_MAP
  -> EXTERN: (131,33) for_trim
  -> EXTERN: (131,56) for_concat
  -> EXTERN: (131,56) _alloca
  -> EXTERN: (131,58) for_trim
  -> EXTERN: (132,8) for_realloc_lhs
  -> INDIRECT-: (133,13)  (*((P64*) *((P64*) (&(comm_diffuse_comp_mod_mp_initdiffuse_$SELF_V$39f1.0.2->QNCAtemplate.dim31_dv_template.addr_a0_V$3a09.0.2)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$X$43_V$ec4 + 56(SI64)))))[3(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (138,9) for_trim
  -> EXTERN: (138,36) for_cpstr
  -> EXTERN: (139,8) __getsp_inlined
  -> EXTERN: (139,8) __resetsp_inlined
  -> EXTERN: (139,21) COMM_MAP_MOD^CONSTRUCTOR_MAP
  -> EXTERN: (139,43) for_trim
  -> EXTERN: (139,66) _alloca
  -> EXTERN: (139,66) for_concat
  -> EXTERN: (139,68) for_trim
  -> EXTERN: (144,9) for_trim
  -> EXTERN: (144,32) for_cpstr
  -> EXTERN: (145,8) __resetsp_inlined
  -> EXTERN: (145,8) __getsp_inlined
  -> EXTERN: (145,25) COMM_MAP_MOD^CONSTRUCTOR_MAP
  -> EXTERN: (145,47) for_trim
  -> EXTERN: (145,70) _alloca
  -> EXTERN: (145,70) for_concat
  -> EXTERN: (145,72) for_trim
  -> EXTERN: (150,9) for_trim
  -> EXTERN: (150,39) for_cpstr
  -> EXTERN: (151,8) __resetsp_inlined
  -> EXTERN: (151,8) __getsp_inlined
  -> EXTERN: (151,24) COMM_MAP_MOD^CONSTRUCTOR_MAP
  -> EXTERN: (151,46) for_trim
  -> EXTERN: (151,69) _alloca
  -> EXTERN: (151,69) for_concat
  -> EXTERN: (151,71) for_trim
  -> EXTERN: (156,9) for_trim
  -> EXTERN: (156,41) for_cpstr
  -> EXTERN: (157,19) COMM_MAP_MOD^CONSTRUCTOR_MAP
  -> INDIRECT-: (158,13)  (*((P64*) *((P64*) (&(comm_diffuse_comp_mod_mp_initdiffuse_$SELF_V$39f1.0.2->QNCAtemplate.dim31_dv_template.addr_a0_V$3a09.0.2)->COMM_DIFFUSE_COMP_MOD$.btCOMM_DIFFUSE_COMP$MU$45_V$f24 + 56(SI64)))))[3(SI64)]
     [[ Unable to inline indirect callsite  <2>]]
  -> EXTERN: (162,10) UPDATE_STATUS
  -> EXTERN: (163,5) for_alloc_allocatable
  -> EXTERN: (163,5) for_check_mult_overflow64
  -> EXTERN: (163,5) for_alloc_allocatable
  -> EXTERN: (163,5) for_check_mult_overflow64
  -> EXTERN: (163,5) for_alloc_allocatable
  -> EXTERN: (163,5) for_check_mult_overflow64
  -> EXTERN: (165,21) COMM_MAP_MOD^CONSTRUCTOR_MAPINFO
  -> EXTERN: (167,26) COMM_MAP_MOD^CONSTRUCTOR_MAP
  -> EXTERN: (170,10) UPDATE_STATUS
  -> EXTERN: (173,19) COMM_B_BL_MOD^CONSTRUCTOR
  -> EXTERN: (177,16) COMM_CL_MOD^CONSTRUCTOR
  -> EXTERN: (180,38) for_check_mult_overflow64
  -> EXTERN: (180,38) for_alloc_allocatable
  -> EXTERN: (181,38) for_check_mult_overflow64
  -> EXTERN: (181,38) for_alloc_allocatable


    Report from: Loop nest, Vector & Auto-parallelization optimizations [loop, vec, par]


LOOP BEGIN at comm_diffuse_comp_mod.f90(132,8)
   remark #25084: Preprocess Loopnests: Moving Out Store    [ comm_diffuse_comp_mod.f90(132,8) ]
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed ANTI dependence between at (132:8) and self(:,:) (132:8)
   remark #17106: parallel dependence: assumed FLOW dependence between self(:,:) (132:8) and at (132:8)
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed ANTI dependence between at (132:8) and self(:,:) (132:8)
   remark #15346: vector dependence: assumed FLOW dependence between self(:,:) (132:8) and at (132:8)

   LOOP BEGIN at comm_diffuse_comp_mod.f90(132,8)
      remark #17104: loop was not parallelized: existence of parallel dependence
      remark #17106: parallel dependence: assumed ANTI dependence between at (132:8) and self(:,:) (132:8)
      remark #17106: parallel dependence: assumed FLOW dependence between self(:,:) (132:8) and at (132:8)
      remark #15344: loop was not vectorized: vector dependence prevents vectorization
      remark #15346: vector dependence: assumed ANTI dependence between at (132:8) and self(:,:) (132:8)
      remark #15346: vector dependence: assumed FLOW dependence between self(:,:) (132:8) and at (132:8)
      remark #25439: unrolled with remainder by 2  
   LOOP END

   LOOP BEGIN at comm_diffuse_comp_mod.f90(132,8)
   <Remainder>
   LOOP END
LOOP END

LOOP BEGIN at comm_diffuse_comp_mod.f90(164,5)
   remark #17104: loop was not parallelized: existence of parallel dependence
   remark #17106: parallel dependence: assumed OUTPUT dependence between at (165:21) and self(i) (167:8)
   remark #17106: parallel dependence: assumed OUTPUT dependence between self(i) (167:8) and at (165:21)
   remark #15382: vectorization support: call to function COMM_MAP_MOD^CONSTRUCTOR_MAPINFO cannot be vectorized   [ comm_diffuse_comp_mod.f90(165,21) ]
   remark #15382: vectorization support: call to function COMM_MAP_MOD^CONSTRUCTOR_MAP cannot be vectorized   [ comm_diffuse_comp_mod.f90(167,26) ]
   remark #15344: loop was not vectorized: vector dependence prevents vectorization
   remark #15346: vector dependence: assumed OUTPUT dependence between at (165:21) and self(i) (167:8)
   remark #15346: vector dependence: assumed OUTPUT dependence between self(i) (167:8) and at (165:21)
LOOP END

    Report from: Code generation optimizations [cg]

comm_diffuse_comp_mod.f90(102,5):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to increase the width of loads
comm_diffuse_comp_mod.f90(102,5):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to increase the width of stores
comm_diffuse_comp_mod.f90(102,5):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (1, 0), and destination (alignment, offset): (1, 0)
comm_diffuse_comp_mod.f90(109,5):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to increase the width of loads
comm_diffuse_comp_mod.f90(109,5):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (1, 0), and destination (alignment, offset): (32, 0)
comm_diffuse_comp_mod.f90(110,5):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to increase the width of loads
comm_diffuse_comp_mod.f90(110,5):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (1, 0), and destination (alignment, offset): (32, 0)
comm_diffuse_comp_mod.f90(117,5):remark #34014: optimization advice for memcpy: increase the source's alignment to 16 (and use __assume_aligned) to increase the width of loads
comm_diffuse_comp_mod.f90(117,5):remark #34014: optimization advice for memcpy: increase the destination's alignment to 16 (and use __assume_aligned) to increase the width of stores
comm_diffuse_comp_mod.f90(117,5):remark #34000: call to memcpy implemented inline with loads and stores with proven source (alignment, offset): (1, 0), and destination (alignment, offset): (8, 0)
comm_diffuse_comp_mod.f90(85,14):remark #34051: REGISTER ALLOCATION : [comm_diffuse_comp_mod_mp_initdiffuse_] comm_diffuse_comp_mod.f90:85

    Hardware registers
        Reserved     :    2[ rsp rip]
        Available    :   39[ rax rdx rcx rbx rbp rsi rdi r8-r15 mm0-mm7 zmm0-zmm15]
        Callee-save  :    6[ rbx rbp r12-r15]
        Assigned     :   18[ rax rdx rcx rbx rsi rdi r8-r15 zmm0-zmm3]
        
    Routine temporaries
        Total         :    1009
            Global    :     143
            Local     :     866
        Regenerable   :     207
        Spilled       :      38
        
    Routine stack
        Variables     :    9416 bytes*
            Reads     :     116 [1.37e+02 ~ 6.3%]
            Writes    :     201 [2.75e+02 ~ 12.5%]
        Spills        :     264 bytes*
            Reads     :      44 [3.02e+01 ~ 1.4%]
            Writes    :      35 [1.96e+01 ~ 0.9%]
    
    Notes
    
        *Non-overlapping variables and spills may share stack space,
         so the total stack size might be less than this.
    

===========================================================================

    Report from: Interprocedural optimizations [ipo]

INLINING FOOTNOTES:

<1> The subprogram is larger than the inliner would normally inline.  Use the
    option -inline-max-size to increase the size of any subprogram that would
    normally be inlined, add "!DIR$ATTRIBUTES FORCEINLINE" to the
    declaration of the called function, or add "!DIR$ FORCEINLINE" before
    the call site.

<2> The indirectly called subprogram must be resolved to its targets before it 
can be inlined.  Consider compiling with -ipo or -prof-gen followed by 
-prof-use.

